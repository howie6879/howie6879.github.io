<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>神经网络与深度学习 on 老胡的储物柜</title>
    <link>https://www.howie6879.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 神经网络与深度学习 on 老胡的储物柜</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 14 Jan 2021 22:35:47 +0800</lastBuildDate><atom:link href="https://www.howie6879.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>梯度下降推导</title>
      <link>https://www.howie6879.cn/post/2021/03_gd_math_note/</link>
      <pubDate>Thu, 14 Jan 2021 22:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2021/03_gd_math_note/</guid>
      <description>以感知器为例，可以梯度下降来学习合适的权重和偏置： 假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为： $$ \bar{y}^i = w_1x_1^i+w_2x_2^i+&amp;hellip;+w_nx_n^i+b $$ 任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示： $$ e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2} $$ 那么所有误差的和可以表示为： $$ \begin{aligned} E &amp;amp;= e^1+e^2+&amp;hellip;+e^n \ &amp;amp;= \sum_{i=1}^ne^i \ &amp;amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 \end{aligned} $$ 想象一下，当你从山顶往下</description>
    </item>
    
    <item>
      <title>nndl_note: 深度神经⽹络为何很难训练</title>
      <link>https://www.howie6879.cn/post/2020/02_why_are_deep_neural_networks_hard_to_train/</link>
      <pubDate>Fri, 18 Dec 2020 21:13:58 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2020/02_why_are_deep_neural_networks_hard_to_train/</guid>
      <description>消失的梯度问题 导致梯度消失的原因 在更加复杂⽹络中的不稳定梯度 其它深度学习的障碍 上一章提到了神经网络的一种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入得到一个近似的输出。 普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许</description>
    </item>
    
    <item>
      <title>nndl_note: 神经⽹络可以计算任何函数的可视化证明</title>
      <link>https://www.howie6879.cn/post/2019/12_a_visual_proof_that_neural_nets_can_compute_any_function/</link>
      <pubDate>Sat, 02 Nov 2019 21:13:58 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/12_a_visual_proof_that_neural_nets_can_compute_any_function/</guid>
      <description>两个预先声明 一个输入和一个输出的普遍性 多个输入变量 S型神经元的延伸 修补阶跃函数 结论 本章其实和前面章节的关联性不大，所以大可将本章作为小短文来阅读，当然基本的深度学习基础还是要有的。 主要介绍了神经⽹络拥有的⼀种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入$x</description>
    </item>
    
    <item>
      <title>nndl_note: 改进神经⽹络的学习⽅法</title>
      <link>https://www.howie6879.cn/post/2019/11_improving_the_way_neural_networks_learn/</link>
      <pubDate>Wed, 30 Oct 2019 19:40:02 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/11_improving_the_way_neural_networks_learn/</guid>
      <description>交叉熵代价函数 引⼊交叉熵代价函数 交叉熵的含义？源⾃哪⾥？ 过度拟合和规范化 规范化 为何规范化可以帮助减轻过度拟合 规范化的其他技术 权重初始化 如何选择神经⽹络的超参数 参考 万丈高楼平地起，反向传播是深度学习这栋大厦的基石，所以在这块花多少时间都是值得的 前面一章，我们深入理解了反向传播算法如</description>
    </item>
    
    <item>
      <title>nndl_note: 反向传播算法如何工作</title>
      <link>https://www.howie6879.cn/post/2019/08_how_does_the_back_propagation_algorithm_work/</link>
      <pubDate>Fri, 10 May 2019 16:36:44 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/08_how_does_the_back_propagation_algorithm_work/</guid>
      <description>热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 关于代价函数的两个假设 反向传播的四个基本方程 输出层误差的⽅程 使用下一层的误差表示当前层的误差 代价函数关于⽹络中任意偏置的改变率 代价函数关于任何⼀个权重的改变率 反向传播算法 反向传播：全局观 参考 前面一章，我们通过了梯度下降算法实现目标函数的最</description>
    </item>
    
    <item>
      <title>nndl_note: 识别手写字</title>
      <link>https://www.howie6879.cn/post/2019/07_use_neural_network_recognize_handwriting/</link>
      <pubDate>Wed, 08 May 2019 16:36:44 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/07_use_neural_network_recognize_handwriting/</guid>
      <description>感知器 S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 随机梯度下降算法 实现数字分类模型 参考 Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。 神经网络是一门重要的机器</description>
    </item>
    
    <item>
      <title>神经网络基础</title>
      <link>https://www.howie6879.cn/post/2019/01_neural_network_foundation/</link>
      <pubDate>Thu, 03 Jan 2019 08:37:56 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/01_neural_network_foundation/</guid>
      <description>要想入门以及往下理解深度学习，其中一些概念可能是无法避免地需要你理解一番，比如，备份地址： 什么是感知器 什么是神经网络 张量以及运算 微分 梯度下降 带着问题出发 在开始之前希望你有一点机器学习方面的知识，解决问题的前提是提出问题，我们提出这样一个问题，对MNIST数据集进行分析，然后在解决</description>
    </item>
    
    <item>
      <title>如何用Python创建一个简单的神经网络</title>
      <link>https://www.howie6879.cn/post/2018/07_how-to-create-asimple-neural-network-in-python/</link>
      <pubDate>Thu, 13 Dec 2018 16:03:05 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2018/07_how-to-create-asimple-neural-network-in-python/</guid>
      <description>如何用Python创建一个简单的神经网络 原文地址：How to Create a Simple Neural Network in Python 作者：Dr. Michael J. Garbade 翻译：howie6879 理解神经网络如何工作的最好方式是自己动手创建一个，这篇文章将会给你演示怎么做到这一点 神经网络(NN)，也称之为人工神经网络(ANN)，它是机器学习领域中学习算法集合中</description>
    </item>
    
  </channel>
</rss>
