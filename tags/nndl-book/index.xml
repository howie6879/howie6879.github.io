<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nndl-book on 老胡的储物柜</title>
    <link>https://www.howie6879.cn/tags/nndl-book/</link>
    <description>Recent content in nndl-book on 老胡的储物柜</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 14 Jan 2021 22:35:47 +0800</lastBuildDate><atom:link href="https://www.howie6879.cn/tags/nndl-book/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>梯度下降推导</title>
      <link>https://www.howie6879.cn/post/2021/03_gd_math_note/</link>
      <pubDate>Thu, 14 Jan 2021 22:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2021/03_gd_math_note/</guid>
      <description>以感知器为例，可以梯度下降来学习合适的权重和偏置： 假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为： \[ \bar{y}^i = w_1x_1^i+w_2x_2^i+...+w_nx_n^i+b \] 任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示： \[ e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2} \] 那么所有误差的和可以表示为： \[ \begin{aligned} E &amp;= e^1+e^2+...+e^n \\ &amp;= \sum_{i=1}^ne^i \\ &amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 \end{aligned} \] 想象一下，当你从山顶往下</description>
    </item>
    
    <item>
      <title>nndl_note: 深度神经⽹络为何很难训练</title>
      <link>https://www.howie6879.cn/post/2020/02_why_are_deep_neural_networks_hard_to_train/</link>
      <pubDate>Fri, 18 Dec 2020 21:13:58 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2020/02_why_are_deep_neural_networks_hard_to_train/</guid>
      <description>消失的梯度问题 导致梯度消失的原因 在更加复杂⽹络中的不稳定梯度 其它深度学习的障碍 上一章提到了神经网络的一种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入得到一个近似的输出。 普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许</description>
    </item>
    
    <item>
      <title>nndl_note: 神经⽹络可以计算任何函数的可视化证明</title>
      <link>https://www.howie6879.cn/post/2019/12_a_visual_proof_that_neural_nets_can_compute_any_function/</link>
      <pubDate>Sat, 02 Nov 2019 21:13:58 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/12_a_visual_proof_that_neural_nets_can_compute_any_function/</guid>
      <description>两个预先声明 一个输入和一个输出的普遍性 多个输入变量 S型神经元的延伸 修补阶跃函数 结论 本章其实和前面章节的关联性不大，所以大可将本章作为小短文来阅读，当然基本的深度学习基础还是要有的。 主要介绍了神经⽹络拥有的⼀种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入$x</description>
    </item>
    
    <item>
      <title>nndl_note: 改进神经⽹络的学习⽅法</title>
      <link>https://www.howie6879.cn/post/2019/11_improving_the_way_neural_networks_learn/</link>
      <pubDate>Wed, 30 Oct 2019 19:40:02 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/11_improving_the_way_neural_networks_learn/</guid>
      <description>交叉熵代价函数 引⼊交叉熵代价函数 交叉熵的含义？源⾃哪⾥？ 过度拟合和规范化 规范化 为何规范化可以帮助减轻过度拟合 规范化的其他技术 权重初始化 如何选择神经⽹络的超参数 参考 万丈高楼平地起，反向传播是深度学习这栋大厦的基石，所以在这块花多少时间都是值得的 前面一章，我们深入理解了反向传播算法如</description>
    </item>
    
    <item>
      <title>nndl_note: 反向传播算法如何工作</title>
      <link>https://www.howie6879.cn/post/2019/08_how_does_the_back_propagation_algorithm_work/</link>
      <pubDate>Fri, 10 May 2019 16:36:44 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/08_how_does_the_back_propagation_algorithm_work/</guid>
      <description>热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 关于代价函数的两个假设 反向传播的四个基本方程 输出层误差的⽅程 使用下一层的误差表示当前层的误差 代价函数关于⽹络中任意偏置的改变率 代价函数关于任何⼀个权重的改变率 反向传播算法 反向传播：全局观 参考 前面一章，我们通过了梯度下降算法实现目标函数的最</description>
    </item>
    
    <item>
      <title>nndl_note: 识别手写字</title>
      <link>https://www.howie6879.cn/post/2019/07_use_neural_network_recognize_handwriting/</link>
      <pubDate>Wed, 08 May 2019 16:36:44 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/07_use_neural_network_recognize_handwriting/</guid>
      <description>感知器 S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 随机梯度下降算法 实现数字分类模型 参考 Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。 神经网络是一门重要的机器</description>
    </item>
    
  </channel>
</rss>
