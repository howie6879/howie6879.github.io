'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href','section']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/k8s/docs/01_basic/01.%E8%B5%B0%E8%BF%9BKubernetes/',title:"01.走进 Kubernetes",section:"第一部分：基础",content:"走进Kubernetes #  什么是Kubernetes #  随着微服务架构被越来越多的公司使用，大部分单体应用正逐步被拆解成小的、独立运行的微服务。微服务的优势这里不做探讨，但是其带来的服务维护问题大大增加，若想要在管理大量微服务的情况下同时还做到以下几点：\n  让资源利用率更高\n  让硬件成本相对更低\n  于是就自然而然地就产生了基于容器自动化部署微服务的需求，在容器编排这块的纷争，各大巨头参与，战况惨烈，但最终胜出的是谷歌的Kubernetes1，其提供的特性有：\n 服务发现和负载均衡 存储编排 自动发布和回滚 自愈 密钥及配置管理  通过下面架构图可以看到其有上下两部分对应的Master\u0026amp;Node节点构成，这两种角色分别对应着控制节点和计算节点。\n Master控制节点主要出发点在于如何编排、管理、调度用户提交的作业\n Kubernetes控制节点主要由以下几个核心组件组成：\n etcd保存了整个集群的状态 apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制 controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等 scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上  对于计算节点：\n kubelet负责维护容器的生命周期，同时也负责Volume（CSI）和网络（CNI）的管理 Container runtime负责镜像管理以及Pod和容器的真正运行（CRI） kube-proxy负责为Service提供cluster内部的服务发现和负载均衡  安装 #  单机安装 #  关于单机安装k8s，我使用的相关环境如下（于2022-08-13更新）：\n macOS：Monterey 12.4 Docker Desktop Vesion：4.11.1 Kubernetes：v1.24.2  由于镜像的下载涉及到网络原因，因此这里使用了开源项目k8s-docker-desktop-for-mac来解决这个问题，需要注意的是要修改images的相关镜像的版本，要和此时Kubernetes配对上才行，比如我设置的是：\nk8s.gcr.io/kube-proxy:v1.24.2=gotok8s/kube-proxy:v1.24.2 k8s.gcr.io/kube-controller-manager:v1.24.2=gotok8s/kube-controller-manager:v1.24.2 k8s.gcr.io/kube-scheduler:v1.24.2=gotok8s/kube-scheduler:v1.24.2 k8s.gcr.io/kube-apiserver:v1.24.2=gotok8s/kube-apiserver:v1.24.2 k8s.gcr.io/pause:3.7=gotok8s/pause:3.7 k8s.gcr.io/coredns/coredns:v1.8.6=gotok8s/coredns:v1.8.6 k8s.gcr.io/etcd:3.5.3-0=gotok8s/etcd:3.5.3-0 然后执行./load_images.sh 即可下载k8s依赖的镜像，随后打开Docker，进入设置界面，勾选Enable Kubernetes即可：\n不出意外，界面左下角会出现Kubernetes running的提示，这样就安装成功了。\n每个人的 Docker 版本都有差别，不同版本如何查找各个依赖容器对应的版本呢？参考一下命令：\nKUBERNETES_VERSION=v1.24.2 # Linux 下执行 curl -O -L https://storage.googleapis.com/kubernetes-release/release/${KUBERNETES_VERSION}/bin/linux/amd64/kubeadm chmod +x kubeadm ./kubeadm config images list --kubernetes-version=${KUBERNETES_VERSION} 版本号那里填写你自己的当前版本即可，不出意外可以得到如下输出：\nk8s.gcr.io/kube-apiserver:v1.24.2 k8s.gcr.io/kube-controller-manager:v1.24.2 k8s.gcr.io/kube-scheduler:v1.24.2 k8s.gcr.io/kube-proxy:v1.24.2 k8s.gcr.io/pause:3.7 k8s.gcr.io/etcd:3.5.3-0 k8s.gcr.io/coredns/coredns:v1.8.6 查看集群信息：\nkubectl cluster-info 输出：\nKubernetes control plane is running at https://kubernetes.docker.internal:6443 CoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. kubectl get nodes 输出：\n查看节点信息：\nkubectl get nodes 输出：\nNAME STATUS ROLES AGE VERSION docker-desktop Ready control-plane 11m v1.24.2 单机版本的k8s安装成功！接下来介绍集群安装。\n集群安装 #  准备 #    准备三台机器，比如（使用的配置是4核8G，IP换成你自己的）：\n 192.168.5.91：Master：  执行：  hostnamectl set-hostname master echo \u0026quot;127.0.0.1 $(hostname)\u0026quot; \u0026gt;\u0026gt; /etc/hosts     192.168.5.92：Node01  执行：  hostnamectl set-hostname node01 echo \u0026quot;127.0.0.1 $(hostname)\u0026quot; \u0026gt;\u0026gt; /etc/hosts     192.168.5.93：Node02  执行：  hostnamectl set-hostname node02 echo \u0026quot;127.0.0.1 $(hostname)\u0026quot; \u0026gt;\u0026gt; /etc/hosts        Kubernetes版本：v1.19.3\n  Docker版本：19.03.12\n  开始前请检查以下事项：\n CentOS 版本：\u0026gt;= 7.6 CPU：\u0026gt;=2 IP：互通 关闭swap：swapoff -a  配置国内kubernetes源：\ncat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 安装相关依赖工具：\nyum install -y kubelet-1.19.3 kubeadm-1.19.3 kubectl-1.19.3 # 设置开机启动 systemctl enable kubelet.service \u0026amp;\u0026amp; systemctl start kubelet.service # 查看状态 systemctl status kubelet.service 初始化Master #  在主节点（192.168.5.91）执行以下命令：\nexport MASTER_IP=192.168.5.91 export APISERVER_NAME=apiserver.demo export POD_SUBNET=10.100.0.1/16 echo \u0026#34;${MASTER_IP}${APISERVER_NAME}\u0026#34; \u0026gt;\u0026gt; /etc/hosts 新建脚本init_master.sh:\nvim init_master.sh 添加：\n#!/bin/bash  # 只在 master 节点执行 # 脚本出错时终止执行 set -e if [ ${#POD_SUBNET} -eq 0 ] || [ ${#APISERVER_NAME} -eq 0 ]; then echo -e \u0026#34;\\033[31;1m请确保您已经设置了环境变量 POD_SUBNET 和 APISERVER_NAME \\033[0m\u0026#34; echo 当前POD_SUBNET=$POD_SUBNET echo 当前APISERVER_NAME=$APISERVER_NAME exit 1 fi # 查看完整配置选项 https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 rm -f ./kubeadm-config.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; ./kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration # k8s 版本 kubernetesVersion: v1.19.3 imageRepository: registry.aliyuncs.com/k8sxio controlPlaneEndpoint: \u0026#34;${APISERVER_NAME}:6443\u0026#34; networking: serviceSubnet: \u0026#34;10.96.0.0/16\u0026#34; podSubnet: \u0026#34;${POD_SUBNET}\u0026#34; dnsDomain: \u0026#34;cluster.local\u0026#34; EOF # kubeadm init # 根据您服务器网速的情况，您需要等候 3 - 10 分钟 kubeadm config images pull --config=kubeadm-config.yaml kubeadm init --config=kubeadm-config.yaml --upload-certs # 配置 kubectl rm -rf /root/.kube/ mkdir /root/.kube/ cp -i /etc/kubernetes/admin.conf /root/.kube/config # 安装 calico 网络插件 # 参考文档 https://docs.projectcalico.org/v3.13/getting-started/kubernetes/self-managed-onprem/onpremises echo \u0026#34;安装calico-3.13.1\u0026#34; rm -f calico-3.13.1.yaml wget https://kuboard.cn/install-script/calico/calico-3.13.1.yaml kubectl apply -f calico-3.13.1.yaml 如果出错：\n# issue 01 # [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 # 所有机器执行 echo 1 \u0026gt; /proc/sys/net/bridge/bridge-nf-call-iptables echo 1 \u0026gt; /proc/sys/net/bridge/bridge-nf-call-ip6tables 检查master初始化结果：\n# 直到所有的容器组处于 Running 状态 watch kubectl get pod -n kube-system -o wide # 查看 master 节点初始化结果 kubectl get nodes -o wide 如下图：\n获得join命令参数 #  直接在master执行：\nkubeadm token create --print-join-command 比如此时输出：\n# 有效期两小时 kubeadm join apiserver.demo:6443 --token vh5hl9.9fccw1mzfsmsp4gh --discovery-token-ca-cert-hash sha256:6970397fdc6de5020df76de950c9df96349ca119f127551d109430c114b06f40 初始化Node #  在所有node执行：\nexport MASTER_IP=192.168.5.91 export APISERVER_NAME=apiserver.demo echo \u0026#34;${MASTER_IP}${APISERVER_NAME}\u0026#34; \u0026gt;\u0026gt; /etc/hosts # 替换为 master 节点上 kubeadm token create 命令的输出 kubeadm join apiserver.demo:6443 --token vh5hl9.9fccw1mzfsmsp4gh --discovery-token-ca-cert-hash sha256:6970397fdc6de5020df76de950c9df96349ca119f127551d109430c114b06f40 检查初始化结果 #  在master节点执行：\nkubectl get nodes -o wide 输出结果如下：\nsealos 快速安装 #  经过上面的流程，相信你也能体会到集群部署的麻烦，为了简化这个流程，Github上诞生了不少优秀的项目来简化安装流程，接下来以sealos为例进行命令行一键安装。\n准备 #  资源相关以集群安装配置为主，其中集群安装的准备工作也一样做，其他要求如下：\n ssh 可以访问各安装节点 各节点主机名不相同，并满足kubernetes的主机名要求。 各节点时间同步 网卡名称如果是不常见的，建议修改成规范的网卡名称， 如(eth.|en.|em.*) kubernetes1.20+ 使用containerd作为cri. 不需要用户安装docker/containerd. sealos会安装1.3.9版本containerd。 kubernetes1.19及以下 使用docker作为cri。 也不需要用户安装docker。 sealos会安装1.19.03版本docker  依赖包：\nyum install socat -y yum remove docker-ce containerd.io -y rm /etc/containerd/config.toml 安装 #  选一台服务器，我选择安装v1.22.0版本，执行命令即可：\nsealos init --passwd \u0026#39;pwd\u0026#39; --master 192.168.5.91 --node 192.168.5.92 --node 192.168.5.93 --pkg-url /root/kube1.22.0.tar.gz --version v1.22.0 检查初始化结果 #  在master节点执行：\nkubectl get nodes -o wide 输出结果如下：\nUI #  Kubernetes Dashboard #  Dashboard可以将容器化应用程序部署到Kubernetes集群，对容器化应用程序进行故障排除，以及管理集群资源。\n安装 #  安装命令如下：\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml -O dashboard.yaml kubectl apply -f dashboard.yaml kubectl -n kubernetes-dashboard get pods -o wide 查看是否成功：\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dashboard-metrics-scraper-8c47d4b5d-mfb6d 1/1 Running 0 83s 10.1.0.7 docker-desktop \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubernetes-dashboard-6c75475678-bdwtn 1/1 Running 0 83s 10.1.0.6 docker-desktop \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 如果执行完发现STATUS有ContainerCreating，可以查看日志找找原因（注意NAME）：\nkubectl describe pod dashboard-metrics-scraper-8c47d4b5d-mfb6d --namespace=kubernetes-dashboard 一般都是因为metrics-scraper:v1.0.8镜像下载不下来，可以手动执行下载：\ndocker pull kubernetesui/metrics-scraper:v1.0.8 拉下来之后就妥了，还有一个问题就是选用的服务类型是ClusterIP（默认类型，服务只能够在集群内部可以访问），所以我们需要将访问形式改为NodePort（通过每个 Node 上的 IP 和静态端口访问）：\nkubectl --namespace=kubernetes-dashboard edit service kubernetes-dashboard # 将里面的 type: ClusterIP 改为 type: NodePort 保存后，执行：\nkubectl --namespace=kubernetes-dashboard get service kubernetes-dashboard 终端输出：\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.110.197.167 \u0026lt;none\u0026gt; 443:32171/TCP 7m32s Token #  在浏览器访问https://0.0.0.0:32171/:\n看界面需要生成Token：\nvim admin-user.yaml # 输入 apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard # 保存退出 vim admin-user-role-binding.yaml # 输入 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard # 保存退出 # 执行命令加载配置 kubectl -n kubernetes-dashboard create -f admin-user.yaml kubectl -n kubernetes-dashboard create -f admin-user-role-binding.yaml # 若出现已存在 # 执行：kubectl -n kubernetes-dashboard delete -f xxx.yaml 即可 获取令牌：\nkubectl -n kubernetes-dashboard create token admin-user 复制token到刚才的界面登录即可，登录后界面如下：\n如果想延长Token的有效时间：\n然后在containners-\u0026gt;args加上--token-ttl=43200:\n通过kubectl edit deployment kubernetes-dashboard -n kubernetes-dashboard 修改也行。\nKubePi #   KubePi 是一个现代化的 K8s 面板，其允许管理员导入多个 Kubernetes 集群，并且通过权限控制，将不同 Cluster、Namespace 的权限分配给指定用户。\n 使用Docker安装如下：\ndocker run --privileged -d --restart always -v \u0026#34;`pwd`:/var/lib/kubepi\u0026#34; --name=kubepi --restart=unless-stopped -p 8080:80 kubeoperator/kubepi-server 然后打开KubePi地址，输入用户名@密码admin@kubepi登录，登陆成功后设置k8s配置（集群列表-\u0026gt;导入）：\n其中配置获取方式如下：\ncat ~/.kube/config 确认后，进入集群就可以开始进行管理：\n部署镜像 #  下拉一个你自己想部署的镜像，具体命令如下（主节点执行）：\n# 部署 kubectl run hello --image=xxx/hello --port=5000 # 列出 pod kubectl get pods # 创建一个服务对象 # NodePort 在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都被转发到对应服务 kubectl expose po hello --port=5000 --target-port=5000 --type=NodePort --name hello-http # 列出服务 kubectl get services 参考 #  本部分内容有参考如下文章：\n 使用kubeadm安装kubernetes_v1.19.x Web基础配置篇（十六）: Kubernetes集群的安装使用 Kubernetes架构 深入剖析Kubernetes：入门篇以及集群搭建部分 Kubernetes in Action中文版：第1、2章  k8s开源安装方案：\n kubeasz：使用Ansible脚本安装K8S集群，介绍组件交互原理，方便直接，不受国内网络环境影响 sealos：一条命令离线安装高可用kubernetes，3min装完，700M，100年证书，生产环境稳如老狗 follow-me-install-kubernetes-cluster：和我一步步部署 kubernetes 集群    k8s项目的基础特性是 Google 公司在容器化基础设施领域多年来实践经验的沉淀与升华，k8s和Swarm\u0026amp;Mesos的竞争有兴趣可自行查询详细看看。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "}),a.add({id:1,href:'/k8s/docs/01_basic/02.%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/',title:"02.概念介绍",section:"第一部分：基础",content:"基础概念介绍 #  俗话说，磨刀不误砍柴工。上一章，我们成功搭建了k8s集群，接下来我们主要花时间了解一下k8s的相关概念，为后续掌握更高级的知识提前做好准备。\n本文主要讲解以下四个概念：\n Pod Deployment Service Namespace  引入 #  让我们使用Deployment运行一个无状态应用来开启此章节吧，比如运行一个nginx Deployment（创建文件：nginx-deployment.yaml）：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 配置文件第二行，有个kind字段，表示的是此时yaml配置的类型，即Deployment。什么是Deployment？这里我先不做解释，让我们先实践，看能不能在使用过程中体会出这个类型的概念意义。\n在终端执行：\nkubectl apply -f ./nginx-deployment.yaml # 输出 deployment.apps/nginx-deployment created 然后通过以下命令分别查看集群中创建的 Deployment 和 Pod 的状态：\n# 查看 Deployment kubectl get deployments # 输出 NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 1/1 1 1 2m29s # 查看 Pod kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE nginx-deployment-585449566-qslv5 1/1 Running 0 2m38s # 查看 Deployment 的信息 kubectl describe deployment nginx	# 删除 Deployment kubectl delete deployment nginx-deployment # 查看 Pod 的信息 # kubectl describe pod \u0026lt;pod-name\u0026gt; # 这里的 \u0026lt;pod-name\u0026gt; 是某一 Pod 的名称 kubectl describe pod nginx-deployment-585449566-qslv5 # 进入容器 kubectl exec -it nginx-deployment-585449566-qslv5 -- /bin/bash 此时我们已经成功在k8s上部署了一个实例的nginx应用程序。但是，等等！我们好像又看到了一个新的名词Pod，这又是什么？让我们带着疑问继续往下看吧。\nPod #   在Kubernetes中，最小的管理元素不是一个个独立的容器，而是pod（目的在于解决容器间紧密协作关系的难题）\n Pod是一组并置的容器，代表了Kubernetes中的基本构建模块:\n 一个Pod包含：  一个或多个容器（container） 容器（container）的一些共享资源：存储、网络等   一个Pod的所有容器都运行在同一个节点  容器可以被管理，但是容器里面的多个进程实际上是不好被管理的，所以容器被设计为每个容器只运行一个进程。\n容器的本质实际上就是一个进程，Namespace 做隔离，Cgroups 做限制，rootfs 做文件系统。在一个容器只能运行一个进程的前提下，实际开发过程中一个应用是由多个容器紧密协作才可以成功地运行起来。因此，我们需要另一种更高级的结构来将容器绑定在一起，并将它们作为一个单元进行管理，这就是Pod出现的目的。\nPod另一个重要意义就是容器设计模式，这对传统虚拟机服务迁移起到了关键性的指导作用，Kubernetes 社区把容器设计模这个理论整理成了一篇小论文《Design Patterns for Container-based Distributed Systems》，我也将这篇论文做了一个翻译，阅读地址《设计模式——基于容器的分布式系统》。\n如何定义并创建一个Pod #  创建文件nginx-pod.yaml:\napiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true 相关字段解释如下：\n kind: 该配置的类型，这里是 Pod metadata：元数据  name：Pod的名称 labels：标签   spec：期望Pod实现的功能  containers：容器相关配置  name：container名称 image：镜像 ports：容器端口  containerPort：应用监听的端口        运行：\n# 创建 kubectl create -f nginx-pod.yaml # 输出 pod/nginx created # 查看 kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 43s # 查看 Pod 完整的描述性文件  # yaml 是你想看的格式 也可以是 json kubectl get po nginx -o yaml # 连接 shell 容器 kubectl attach -it nginx -c shell # 在容器执行 ps ax # 会发现 pause 以及 niginx 等进程 # 这意味着整个 Pod 里的每个容器的进程对所有容器来说都是可见的，它们共享了同一个 PID Namespace。 # 删除 Pod kubectl delete -f nginx-pod.yaml 这里简单介绍了用声明式API怎么创建Pod，但从技术角度看，Pod又是怎样被创建的呢？实际上Pod只是一个逻辑概念，Pod里的所有容器，共享的是同一个Network Namespace，并且可以声明共享同一个Volume。\nPod除了启动你定义的容器，还会启动一个Infra容器，这个容器使用的就是k8s.gcr.io/pause镜像，它的作用就是整一个Network Namespace方便用户容器加入，这就意味着Pod有以下特性：\n 内部直接使用127.0.0.1通信，网络设备一致（Infra容器决定） 只有一个IP地址 Pod的生命周期只跟Infra容器一致，而与用户容器无关  标签 #  现在我们的集群里面只运行了一个Pod，但在实际环境中，我们运行数十上百个Pod也是一件很正常的事情，这样就引出了Pod管理上的问题，我们可以通过标签来组织Pod和所有其他Kubernetes对象。\n前面nginx-pod.yaml里面就声明了labels字段，标签为name，相关操作记录如下：\n# 查看标签 kubectl get pods --show-labels # 输出 NAME READY STATUS RESTARTS AGE LABELS nginx 1/1 Running 0 17m name=nginx # 增加标签 kubectl label pods nginx version=latest # 输出 pod/nginx labeled # 查看特定标签 kubectl get pods -l \u0026#34;version=latest\u0026#34; --show-labels # 更新标签 kubectl label pods nginx version=1 --overwrite # 删除标签 kubectl label pods nginx version- 命名空间 #  利用标签，我们可以将Pod和其他对象组织成一个组，这是最小粒度的分类，当我们需要将对象分割成完全独立且不重叠的组时，比如我想单独基于k8s搭建一套Flink集群，我不想让我的Flink和前面搭建的Nginx放在一起，这个时候，命名空间（namespace）的作用就体现出来了。\n# 列出所有的命名空间 kubectl get ns # 输出，我们目前都是在 default 命名空间中进行操作 NAME STATUS AGE default Active 20d kube-node-lease Active 20d kube-public Active 20d kube-system Active 20d kubernetes-dashboard Active 19d 让我们创建一个命名空间vim cus-ns.yaml，输入:\napiVersion: v1 kind: Namespace metadata: name: cus-ns 让我们在终端实践一番：\n# 开始创建命名空间 kubectl create -f cus-ns.yaml # 输出 NAME STATUS AGE cus-ns Active 6s # 为新建资源选择命名空间 kubectl create -f nginx-pod.yaml -n cus-ns 这里我们可以暂时先做一个总结，如前面所说，Pod可以表示k8s中的基本部署单元。经过前面的讲解，你应该知道以下一些知识点：\n 手动增删改查Pod 让其服务化（Service）  但是在实际使用中，我们并不会直接人工干预来管理Pod，为什么呢？当Pod健康出问题或者需要进行更新等操作时，人是没有精力来做这种维护管理工作的，但我们擅长创造工具来自动化这些繁琐的事情，所以我们可以使用后面介绍的Deployment。\n外部访问 #  此时我们已经启动了一个nginx，我们有哪些方法可以对Pod进行连接测试呢？\n可以使用如下命令：\nkubectl port-forward nginx 8088:80 # 输出 Forwarding from 127.0.0.1:8088 -\u0026gt; 80 Forwarding from [::1]:8088 -\u0026gt; 80 # 再开一个终端访问测试或者打开浏览器 curl http://0.0.0.0:8088/ 显然，成功访问，但是这个有个问题就是此端口不会长期开放，一旦一定时间内没有访问，就会自动断掉，我们需要其他的方式来进行访问，比如后面会提到的Service，这里就简单运行个命令，大家感受一下：\n# 创建一个服务对象 # NodePort 在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都被转发到对应服务 kubectl expose po nginx --port=80 --target-port=80 --type=NodePort --name nginx-http # 输出 service/nginx-http exposed # 查看服务 kubectl get svc # 输出 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 16d nginx-http NodePort 10.102.141.232 \u0026lt;none\u0026gt; 80:32220/TCP 1s # 终端访问测试 curl http://0.0.0.0:32220/ # 输出 html, 表示成功端口成功开放给外部 Service #   Service 服务的主要作用就是替代 Pod 对外暴露一个不变的访问地址\n 在本文第二节Pod部分的外部访问小节，就已经提到并演示了Service，它很方便地将我们的服务端口成功开放给外部访问。\n介绍 #  我们的Pod是有生命周期的，它们可以被创建、销毁，但是一旦被销毁，这个对象的相关痕迹就没有了，哪怕我们用ReplicaSet让他又复生了，但是新Pod 的IP我们是没法管控的。\n很显然，如果我们的后端服务的接口地址总是在变，我们的前端人员心中定然大骂，怎么办？这就轮到Service出场了。\n定义 Service #  前面我们创建了一个名为nginx-http的Services，用的是命令行；接下来我们介绍一下配置文件的形式，在nginx-deployment.yaml后面增加以下配置：\n--- kind: Service apiVersion: v1 metadata: name: nginx spec: selector: app: nginx type: NodePort ports: - nodePort: 30068 port: 8068 protocol: TCP targetPort: 80 相信上述配置，大部分的字段看起来都没什么问题了吧，先说一下端口这块的含义：\n nodePort：通过任意节点的30068端口来访问Service port：集群内的其他容器组可通过8068端口访问Service targetPort：Pod内容器的开发端口  这里我想强调的是type字段，说明如下：\n ClusterIP：默认类型，服务只能够在集群内部可以访问 NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务 LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。  关于LoadBalancer，基本上是云商会提供此类型，如果是我们自行搭建的，就没有此类型可选，但是很多开源项目默认是启用这种类型，我们可以自行打一个补丁来解决这个问题：\nkubectl patch svc {your-svc-name} -n default -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;, \u0026#34;externalIPs\u0026#34;:[\u0026#34;0.0.0.0\u0026#34;]}}\u0026#39; 执行生效命令：\nkubectl apply -f ./nginx-deployment.yaml # 输出 deployment.apps/nginx-deployment unchanged service/nginx created # 查看服务 kubectl get services -o wide # 输出 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx NodePort 10.110.245.214 \u0026lt;none\u0026gt; 8068:30068/TCP 11m app=nginx # 终端测试 curl http://0.0.0.0:30068/ 除了前面提的两种方法（NodePort、LoadBalancer），还有另外一种方法——Ingress资源。我们为什么需要引入Ingress，最主要的原因是LoadBalancer需要公有的IP地址，自行搭建的就不要考虑了。\n而Ingress非常强大，它位于多个服务之前，充当集群中的智能路由器或入口点：\nDeployment #  窥一斑而知全豹，好好了解完Pod之后，再继续了解k8s的概念也就水到渠成了。我们一般不会直接创建Pod，毕竟通过创建Deployment资源可以很方便的创建管理Pod（水平扩展、伸缩），并支持声明式地更新应用程序。\n介绍 #  本章第一小节引入部分就是以Deployment举例，当时启动配置文件我们看到了一个Deployment资源和一个Pod，查看命令如下：\nkubectl get deployments # 输出 NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/1 1 0 4s kubectl get pods # 输出 如果名字有变化不用在意，只是我重新创建了一个 Deployment  NAME READY STATUS RESTARTS AGE nginx-deployment-585449566-mnrtn 1/1 Running 0 2m1s 这里我们再增加一条命令：\nkubectl get replicasets.apps # 输出 NAME DESIRED CURRENT READY AGE nginx-deployment-585449566 1 1 1 10m 嗯嗯~，让我们捋一捋，当我们创建一个Deployment对象时，k8s不会只创建一个Deployment资源，还会创建另外的ReplicaSet 以及1个Pod 对象。所以问题来了， ReplicaSet又是个是什么东西？\nReplicaSet #  如果你更新了Deployment的Pod模板，那么Deployment就需要通过滚动更新（rolling update）的方式进行更新。\n而滚动更新，离不开ReplicaSet，说到ReplicaSet就得说到ReplicationController（弃用）。\n ReplicationController是一种k8s资源，其会持续监控正在运行的pod列表，从而保证Pod的稳定（在现有Pod丢失时启动一个新Pod），也能轻松实现Pod的水平伸缩\n ReplicaSet的行为与ReplicationController完全相同，但Pod选择器的表达能力更强（允许匹配缺少某个标签的Pod，或包含特定标签名的Pod）。所以我们可以将Deployment当成一种更高阶的资源，用于部署应用程序，并以声明的方式管理应用，而不是通过ReplicaSet进行部署，上述命令的创建关系如下图：\n如上图，Deployment的控制器，实际上控制的是ReplicaSet的数目，以及每个ReplicaSet的属性。我们可以说Deployment是一个两层控制器：\n Deployment\u0026ndash;\u0026gt;ReplicaSet\u0026ndash;\u0026gt;Pod\n 这种形式下滚动更新是极好的，但这里有个前提条件那就是Pod是无状态的，如果运行的容器必须依赖此时的相关运行数据，那么回滚后这些存在于容器的数据或者一些相关运行状态值就不存在了，对于这种情况，该怎么办？此时需要的就是StatefulSet（部署有状态的多副本应用）。\nStatefulSet #  如果通过ReplicaSet创建多个Pod副本（其中描述了关联到特定持久卷声明的数据卷），那么这些副本都将共享这个持久卷声明的数据卷。\n那如何运行一个pod的多个副本，让每个pod都有独立的存储卷呢？对于这个问题，之前学习的相关知识都不能提供比较好的解决方案。k8s提供了Statefulset资源来运行这类Pod，它是专门定制的一类应用，这类应用中每一个实例都是不可替代的个体，都拥有稳定的名字和状态。\n对于有状态的应用（实例之间有不对等的关系或者依赖外部数据），主要需要对以下两种类型的状态进行复刻：\n 存储状态：应用的多个实例分别绑定了不同的存储数据，也就是让每个Pod都有自己独立的存储卷 拓扑状态：应用的多个实例之间不是完全对等的关系，各个Pod需要按照一定的顺序启动  参考 #  本章的基本概念就介绍到这里了，谢谢！本部分内容有参考如下文章：\n 学习Kubernetes基础知识 详解 Kubernetes Deployment 的实现原理 Kubernetes 中文指南：Deployment Kubernetes 中文指南：Service 深入剖析Kubernetes：容器编排部分 Kubernetes in Action中文版：第3、4、5、9章  "}),a.add({id:2,href:'/k8s/docs/01_basic/03.%E5%AE%B9%E5%99%A8%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/',title:"03.容器持久化存储",section:"第一部分：基础",content:"容器持久化存储 #  容器的本质是进程，对于进程，Linux系统有进程组的概念来将其组织在一起。在k8s里面，使用Pod这个逻辑概念来维护容器间的关系。\n有了Pod后，我们的应用程序需要被创建和管理，这就引出了ReplicaSet和Deployment；然后需要将部署好的应用暴露给外部进行访问，Service可以提供一个固定的ip和端口让外部访问。\n对于有状态的应用，可以使用StatefulSet来进行状态的恢复，在上一节概念介绍里面有提到，有状态的应用是离不开持久化存储的。\n引子 #  在Docker中，如果一个容器在运行过程中会产生数据并写入到文件系统，当关闭这个容器，用镜像再启动一个容器的时候，你就会意识到新容器并不会识别前一个容器写入文件系统内的任何内容。\n对于有状态的应用，我们希望下次启动的应用可以保持住上次的状态；在k8s里面可以通过定义存储卷来满足这个需求，它们不像Pod这样的顶级资源，而是被定义为Pod的一部分，并和Pod共享相同的生命周期。因此在Pod里面容器重新启动期间，卷的内容是不变的，\n卷 #  emptyDir #  在Pod中如何定义卷？让我们从emptyDir开始。设想一个这样的例子，一个Pod应用由两个容器，容器A不断产生数据，容器B将A产生的数据作为输出。此时，这两个容器就需要使用同一个卷。\n让我们实际操作一下，vim fortune-pod.yaml:\napiVersion: v1 kind: Pod metadata: name: fortune spec: containers: - image: luksa/fortune name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 说明一下上述配置文件的含义：fortune镜像是k8s in action书中示例打包的镜像，相当于上面说的不断产生数据的容器A，其中名为html的容器挂载在var/htdocs中；而nginx也挂载了相同的html卷，不过位置在/usr/share/nginx/html，上面两个容器共用的卷就是emptyDir: {}。\n启动来感受一下：\nkubectl create -f fortune-pod.yaml # 输出 pod/fortune created # 查看状态 kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE fortune 2/2 Running 0 2m27s # 暂时服务化 kubectl port-forward fortune 8080:80 此时服务就处于可用状态了，在终端输入curl http://localhost:8080/，基本上每隔10s，都会返回不同的响应，如下图：\nemptyDir卷是最简单的卷类型，但是其他类型的卷都是在它的基础上构建的，在创建空目录后，相应的容器会将数据写入。\ngitRepo #  假设你有在github上开发项目，gitRepo卷允许你定义好相关配置然后直接从github上下拉项目将数据共享给其他容器使用。\nhostPath #  前面说的卷都是停留在共享同一个Pod的文件，当其需要读取节点文件的时候，就需要hostPath卷出场了。和之前介绍的卷最大的不同之处是，hostPath是一个持久性存储的卷，其目录存在于对应节点主机的目录。\n所以，hostPath仅仅适用于在节点上读取数据，如果你的需求是跨Pod，那么NAS才是你的解决方案。\nNFS #  目前相关的云商都会有一套自己的持久化方式，我目前是自己搭建的k8s，所以我只能实践一下NAS方案，新建文件vim mongodb-pod-nfs.yaml，输入以下内容：\napiVersion: v1 kind: Pod metadata: name: mongodb-nfs spec: volumes: - name: mongodb-data nfs: server: 1.2.3.4 path: /some/path containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP 启动：\nkubectl create -f mongodb-pod-nfs.yaml # 查看状态 kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE mongodb-nfs 1/1 Running 0 3m13s 我们来验证一下数据持久化是否生效，输入命令kubectl exec -it mongodb-nfs mongo进入：\n\u0026gt; use test_data switched to db test_data # 插入 \u0026gt; db.test.insert({\u0026#34;name\u0026#34;: \u0026#34;howie\u0026#34;}) WriteResult({ \u0026#34;nInserted\u0026#34; : 1 }) # 查询 \u0026gt; db.test.find({}) { \u0026#34;_id\u0026#34; : ObjectId(\u0026#34;6041f5bc0c893dc3bb362e75\u0026#34;), \u0026#34;name\u0026#34; : \u0026#34;howie\u0026#34; } 接下来重新创建Pod看一下数据是不是还在：\nkubectl delete pod mongodb-nfs # 重新创建 kubectl create -f mongodb-pod-nfs.yaml # 输出 pod/mongodb-nfs created # 查看 kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE mongodb-nfs 1/1 Running 0 28s 进入Pod内的mongo容器，输入命令kubectl exec -it mongodb-nfs mongo进入：\n\u0026gt; use test_data switched to db test_data # 查询 \u0026gt; db.test.find({}) { \u0026#34;_id\u0026#34; : ObjectId(\u0026#34;6041f5bc0c893dc3bb362e75\u0026#34;), \u0026#34;name\u0026#34; : \u0026#34;howie\u0026#34; } 虽然我们成功让多个Pod享用了同一份数据，但这样做法有点问题，让开发人员在配置里面写具体NFS地址是很不友好的事情，我们可以使用持久卷来解决此问题。\n持久卷\u0026amp;持久卷声明 #  介绍 #  前面NFS用来做持久化存储是一个反面的例子，对于真实的基础设施，其详细配置应该是被隐藏的；但是k8s又实实在在需要对一些基础设施进行访问，怎么办？引入新的资源：\n 持久卷（PersistentVolume） 持久卷声明（PersistentVolumeClaim）  持久卷由管理员创建（各种配置信息），然后用户创建持久卷声明，提交后k8s就会找到匹配的持久卷并将其绑定到持久卷声明。\n这样做的好处在于，对于用户只需要关注声明一下需要多大的存储、需要什么权限（读写）等，然后pod通过其中一个卷的名称来引用声明就可以了，将细节完美地进行了隐藏。\n实践 #  首先建立一个NFS类型的PV（一般是管理员进行创建），在终端输入vim mongodb-pv-nfs.yaml：\napiVersion: v1 kind: PersistentVolume metadata: name: nfs-pv spec: capacity: storage: 10Mi accessModes: - ReadWriteMany nfs: server: 1.2.3.4 path: \u0026#34;/\u0026#34; 接下来创建持久卷：\nkubectl create -f mongodb-pv-nfs.yaml # 输出 persistentvolume/nfs created # 查看 kubectl get pv # 输出 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs-pv 10Mi RWX Retain Available 6s # 关于删除 # kubectl delete pv nfs-pv 可以看到状态已经生效。\n接下来就轮到使用者随意使用PV了，如果作为使用者，部署的Pod需要持久化存储，那么其需要做的就是创建PVC，在终端输入vim mongodb-pvc-nfs.yaml：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 10Mi storageClassName: \u0026#34;\u0026#34; 然后创建持久化声明：\nkubectl create -f mongodb-pvc-nfs.yaml # 输出 persistentvolumeclaim/nfs created # 查看 pvc kubectl get pvc # 输出，注意状态是绑定 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs-pvc Bound nfs-pv 10Mi RWX 6s # 查看 pv kubectl get pv # 输出，状态是绑定 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs-pv 10Mi RWX Retain Bound default/nfs-pvc 3m6s # 关于删除 # kubectl delete pvc nfs-pvc 对于ACCESS MODES，主要分为以下几种：\n RWO：ReadWriteOnce（仅允许单个节点挂载读写） ROX：ReadOnlyMany（允许多个节点挂载只读） RWX：ReadWriteMany（允许多个节点挂载读写这个卷）  现在，准备工作就绪，在Pod中使用持久卷就是引用持久卷名称，在终端输入vim mongo-pod-pvc.yaml：\napiVersion: v1 kind: Pod metadata: name: mongodb spec: containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP volumes: - name: mongodb-data persistentVolumeClaim: claimName: nfs-pvc 创建Pod：\n# 先删除原先创建的 mongo Pod kubectl delete pod mongodb-nfs # 创建引用持久卷声明的 Pod kubectl create -f mongodb-pod-pvc.yaml # 输出 pod/mongodb created 进入Pod内的mongo容器，输入命令kubectl exec -it mongodb mongo进入：\n\u0026gt; use test_data switched to db test_data # 查询 \u0026gt; db.test.find({}) { \u0026#34;_id\u0026#34; : ObjectId(\u0026#34;6041f5bc0c893dc3bb362e75\u0026#34;), \u0026#34;name\u0026#34; : \u0026#34;howie\u0026#34; } 没问题，引用了之前的NFS下的对应目录。\n动态卷 #  前面提到，PV需要管理人员进行创建，在实际生产环境下，这个PV的需求量可能是非常大的，所以这种协调方式是不合理的。所以，k8s提供了一套可以自动创建PV的机制——动态卷。\n这张图将使用StorageClass的流程描述地很清楚，管理员创建一个或多个StorageClass，用户创建Pod引用PVC声明相关的storageClassName就会通过管理员创建的StorageClass自动创建PV。\n参考 #  本章关于容器持久化就介绍到这里了，谢谢！本部分内容有参考如下文章：\n 深入剖析Kubernetes：持久化存储部分 Kubernetes in Action中文版：可以算是第6章的读书笔记  "}),a.add({id:3,href:'/k8s/docs/01_basic/04.%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/',title:"04.配置应用程序",section:"第一部分：基础",content:"配置应用程序 #  使用Docker部署应用程序时，一般常用的配置方式有：\n 配置内嵌 启动传参配置 环境变量  经过前面容器持久化存储的介绍，我们很容易能想到是以挂载卷的形式，比如：\n gitRepo hostPath NFS  再结合边车模式来进行配置文件的管控是可行的，然而有一种更加简便的方法能将配置数据置于Kubernetes的顶级资源对象中，那就是ConfigMap。\n传递命令行参数 #  在上一节容器持久化存储的emptyDir概念介绍部分，我们引入了一个fortune-pod的例子，再回顾一下之前的配置文件吧，如下：\napiVersion: v1 kind: Pod metadata: name: fortune spec: containers: - image: luksa/fortune name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 此应用程序设定了每隔10s就会自动生成输出到html，现在我们要做的是通过命令行参数，自行设定隔多少秒自动生成内容。\n创建文件fortune-pod-args.yaml，输入以下内容：\napiVersion: v1 kind: Pod metadata: name: fortune2s spec: containers: - image: luksa/fortune:args args: [\u0026#34;2\u0026#34;] name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 看到配置文件中的args字段了么？这个就是传给镜像luksa/fortune:args控制时间的参数，让我们启动看看吧。\nkubectl create -f fortune-pod-args.yaml # 输出 pod/fortune2s created # 查看状态 kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE fortune 2/2 Running 0 2m27s # 暂时服务化 kubectl port-forward fortune2s 8080:80 访问127.0.0.1:8080就会发现输出的频率变成了2s：\n\u0026gt; curl 127.0.0.1:8080 Stay away from flying saucers today. 设置环境变量 #   与容器的命令和参数设置相同，环境变量列表无法在pod创建后被修改\n 设置环境变量非常简单，我们只需要在pod中指定环境变量即可；当然，这里有个前提是你需要将修改镜像让其支持读取环境变量。\n我们直接使用书中的例子：vim fortune-pod-env.yaml：\napiVersion: v1 kind: Pod metadata: name: fortune-env spec: containers: - image: luksa/fortune:env env: - name: INTERVAL value: \u0026#34;30\u0026#34; name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 可以看到配置中声明了INTERVAL环境变量值为30，在终端中实践一下：\nkubectl create -f fortune-pod-env.yaml # 输出 pod/fortune-env created # 查看状态 kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE fortune-env 2/2 Running 0 10s # 暂时服务化 kubectl port-forward fortune-env 8080:80 访问127.0.0.1:8080就会发现输出的频率变成了30s。\n现在应用程序的所有配置基本上可以说是硬编码的形式进行配置，并且yaml配置文件总是有配置相关的字段，有没有办法让镜像和配置文件解耦呢？\nk8s提供了名为ConfigMap的资源对象解决这个问题。\nConfigMap #  资源对象ConﬁgMap提供了向容器中注入配置信息的机制，它本质上就是一个键/值对映射，可以用来保存单个值或者配置文件。\nConﬁgMap是不需要被读取的，它映射的内容通过环境变量或者卷文件的形式传给容器。一般直接在pod的定义里面就可以声明ConﬁgMap，这样就可以根据不同的环境创建不同的配置，流程交互如下图所示：\n创建 #  还是用之前应用程序为例，配置专注于环境变量INTERVAL，创建命令如下所示：\nkubectl create configmap fortune-config --from-literal=sleep-interval=25 # 输出 configmap/fortune-config created # 获取相关描述 kubectl get configmap fortune-config -o yaml # 查看 configmap kubectl get configmap # 删除 kubectl delete configmap fortune-config 此外，k8s还可以直接填写配置文件来进行创建vim fortune-config.yaml，输入如下内容：\napiVersion: v1 kind: ConfigMap metadata: name: fortune-config data: sleep-interval: \u0026#34;25\u0026#34; 然后执行：\nkubectl create -f fortune-config.yaml # 输出 configmap/fortune-config created 其实还有更多的创建方式，大概提一下：\n# 从文件内容创建ConfigMap条目 kubectl create configmap my-config --from-file=config-file.conf # 从文件夹创建ConfigMap kubectl create configmap my-config --from-file=/path/to/dir # 合并不同选项 kubectl create configmap my-config --from-file=foo.json --from-file=bar=foobar.conf --from-file=config-opts/ --from-literal=some=thing 作为环境变量传入容器 #  如何将映射中的值传递给pod的容器？最简单的方法是给容器设置环境变量，通过在配置中声明valueFrom，具体操作vim fortune-pod-env-configmap.yaml:\napiVersion: v1 kind: Pod metadata: name: fortune-env-from-configmap spec: containers: - image: luksa/fortune:env env: - name: INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 让我们启动pod实践看看：\nkubectl create -f fortune-pod-env-configmap.yaml # 输出 pod/fortune-env-from-configmap created # 查看状态 kubectl get pods # 输出 NAME READY STATUS RESTARTS AGE fortune-env-from-configmap 2/2 Running 0 19s # 暂时服务化 kubectl port-forward fortune-env-from-configmap 8080:80 # 删除pod kubectl delete pods fortune-env-from-configmap 现在我们已经成功在pod中使用了ConfigMap资源，不过这种使用方式是比较低效的，考虑这样一个情况：如果环境变量比较多，这样单独一个个环境变量的设置方式是个程序员都没法忍受的，一般在框架里面加载一些环境变量都会利用前缀机制来进行批量加载。\nk8s也提供了这样类似的机制，配置声明如下图示：\nConfigMap卷 #   环境变量或者命令行参数值作为配置值通常适用于变量值较短的场景。由于ConfigMap中可以包含完整的配置文件内容，当你想要将其暴露给容器时，可以借助前面章节提到过的一种称为configMap卷的特殊卷格式。\n 首先删除前面声明建立的cf资源：kubectl delete configmap fortune-config。然后建立文件夹configmap-files，首先建立Nginx配置文件：vim my-nginx-config.conf：\nserver { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } 另外在该文件夹中添加一个名为sleep-interval的文本文件，输入25，类似前面声明环境变量。\n接下来从文件夹创建ConfigMap：\nkubectl create configmap fortune-config --from-file=configmap-files # 输出 configmap/fortune-config created # 验证 kubectl get configmap fortune-config -o yaml 创建包含ConfigMap条目内容的卷只需要创建一个引用ConfigMap名称的卷并挂载到容器中：\n创建配置文件：vim fortune-pod-configmap-volume.yaml，输入如下内容：\napiVersion: v1 kind: Pod metadata: name: fortune-configmap-volume spec: containers: - image: luksa/fortune:env env: - name: INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: config mountPath: /tmp/whole-fortune-config-volume readOnly: true ports: - containerPort: 80 name: http protocol: TCP volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config Secret #  前面说的，都是针对非敏感信息的配置数据，对于一些敏感数据，例如：密码、OAuth token、ssh 密钥等，就可以使用Secret资源了。\nSecret结构和使用方法都与ConfigMap类似，其作用就是将加密数据放到etcd中，然后k8s将Secret分发到对应pod所在机器节点来保障其安全性。另外，Secret只会存储在节点的内存中，永不写入物理存储。\n创建 #  以创建一个包含用户名密码的secret资源为例：\n# 创建文件 echo -n \u0026#39;admin\u0026#39; \u0026gt; ./username.txt echo -n \u0026#39;1f2d1e2e67df\u0026#39; \u0026gt; ./password.txt # 单独创建 kubectl create secret generic user --from-file=./username.txt kubectl create secret generic pass --from-file=./password.txt # 批量创建资源 kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt # 输出 secret/db-user-pass created # 查看 kubectl get secrets # 输出 NAME TYPE DATA AGE db-user-pass Opaque 2 6h48m default-token-nlxkq kubernetes.io/service-account-token 3 2d pass Opaque 1 7s user Opaque 1 10s db-user-pass很好理解，就是刚才创建的secret资源，default-token-nlxkq是什么呢？这是一种默认被挂载至所有容器的Secret。\n这里同样也可以是用yaml文件形式创建，先将用户名密码进行基础的Base64转码：\necho -n \u0026#39;admin\u0026#39; | base64 echo -n \u0026#39;1f2d1e2e67df\u0026#39; | base64 vim db-user-pass-secret.yaml：\napiVersion: v1 kind: Secret metadata: name: db-user-pass type: Opaque data: user: YWRtaW4= pass: MWYyZDFlMmU2N2Rm 终端执行：\n# 启动 kubectl create -f db-user-pass-secret.yaml # 查看 kubectl get secrets # 输出 NAME TYPE DATA AGE db-user-pass Opaque 2 15s default-token-ds2lv kubernetes.io/service-account-token 3 41h # 删除 kubectl delete secrets db-user-pass 使用 #  利用投射数据卷在Pod中使用Secret，编辑配置文件vim test-projected-volume.yaml：\napiVersion: v1 kind: Pod metadata: name: test-projected-volume  spec: containers: - name: test-secret-volume image: busybox args: - sleep - \u0026#34;86400\u0026#34; volumeMounts: - name: mysql-cred mountPath: \u0026#34;/projected-volume\u0026#34; readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: user - secret: name: pass 终端执行：\n# 启动 kubectl create -f test-projected-volume.yaml # 查看 kubectl get pods -o wide # 输出 NAME READY STATUS RESTARTS AGE test-projected-volume 1/1 Running 0 37s # 查看数据 kubectl exec -it test-projected-volume -- /bin/sh \u0026gt; ls /projected-volume/ \u0026gt; cat /projected-volume/username.txt # 删除 kubectl delete pods test-projected-volume 参考 #  本章关于配置应用程序就介绍到这里了，谢谢！本部分内容有参考如下文章：\n 配置 Pod 使用 ConfigMap Secret概述 Kubernetes in Action中文版：可以算是第7章的读书笔记  "}),a.add({id:4,href:'/k8s/docs/01_basic/05.%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/',title:"05.容器网络",section:"第一部分：基础",content:"容器网络 #  "}),a.add({id:5,href:'/k8s/docs/03_appendix/00.%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/',title:"00.设计模式——基于容器的分布式系统",section:"第三部分：附录",content:"设计模式——基于容器的分布式系统 #  20世纪80年代末至90年代初，面向对象编程思想给软件开发带来了一轮技术革新，就像润物细无声的春雨那般，向全世界的程序员们快速普及了模块化构建应用程序的方法，一直流行至今。\n当下，我们可以看到类似的革新出现在了分布式系统开发，具体特点如下：\n 基于容器的微服务架构体系日益流行 容器天然隔离的属性非常适合作为分布式系统中的基本对象  基于面向对象，四人帮基于经验提出和总结了对于一些常见软件设计问题的标准解决方案，其描述了一系列基于接口的模式，可以在各种环境中重用，这被称之为软件设计模式。历史一定程度上来说是重复的，随着这种架构模式的成熟，基于容器的分布式系统的设计模式也就自然而然地浮现了。\n本篇主要阐述的是Brendan Burns在基于容器的分布式系统中发现的三种设计模式：\n single-container patterns for container management：容器管理之单容器模式 single-node patterns of closely cooperating containers：容器协调之单节点（多容器）模式 multi-node patterns for distributed algorithms：分布式算法之多节点模式  基于容器分布式系统的设计模式会给分布式计算编码带来以下优势：\n 最佳实践，给没有经验的程序员带来相对正确的使用方式 简化开发 提升系统可靠性  模式的价值 #  模式的目的是提供一般建议或结构来指导设计，这样做的好处有以下三点：\n 站在巨人的肩膀上，对于经验不怎么丰富的开发者，可以通过模式来指引走在正确的道路上，从而少踩坑，提升项目质量 提供通用的名称和定义，有共同的领域语言进行交流是一件很重要的事情 方便识别并构建共享的通用组件  单容器模式 #  就像对象会定义边界一样，容器为定义接口提供了天然的边界；它不仅可以暴露特定应用的功能，还可以通过钩子函数来管理系统。传统的容器管理接口是极其有限的，如：\n run pause stop  这些接口只能说满足基础的使用需求，但是就目前的现状来看，更丰富的接口可以为系统开发者与操作者提供更多的功能。鉴于HTTP和JSON的普及程度，可以考虑通过容器在特定的节点托管一个Web服务来实现。这样做的目的是什么，可以从下面两个角度来看待：\n upward：容器可以暴露丰富的应用信息，比如：  各类监控指标（QPS、应用健康等） 一些开发人员感兴趣的信息如（线程、堆栈、锁、网络消息统计等） 组件配置、日志等   downward：任何开发者在编写软件组件的时候，都可以使用容器原生支持的生命周期接口来进行管控。比如一个集群管理系统通常会给任务分配对应的优先级，高优先级的任务即使在集群被超额订阅的情况下也能保证运行，这种保证是通过逐出已经运行的低优先级任务来实现的，然后这些低优先级任务能否运行取决于后面是否还有资源分配过来；但是这样有个问题就是开发者需要承担一些没必要的复返，比如处理一些优先级比较低的任务被抛弃的情况。相反，如果在应用程序和管理系统之间定义了正式的生命周期，那么应用程序组件将变得更易于管理，比如k8s使用Docker 的graceful deletion功能，这就允许应用程序通过完成当前任务，把状态写入磁盘等等操作之后再终止，将这个功能扩展一下就可以使使有状态的分布式系统的状态管理更加容易。  单节点（多容器）模式 #  上面提到了单容器的接口，我们稍稍延伸一下，对于一个多容器组成的应用，会有怎样的设计模式呢？当然，此时我们仍旧有些限制条件需要讲清楚：\n 容器都处于单节点下 容器管理系统需要支持将多容器作为原子单元协调编排，这也侧面印证为什么k8s需要有Pod这个逻辑概念  边车模式（Sidecar pattern） #   扩展和增强现有的应用容器\n 目前最常见的多容器部署模式就是边车模式，边车模式就是由两个容器组成的单节点模式：\n 核心是应用程序容器，这个就是应用程序的轴心 其次就是边车容器，作用就是改进和增强应用程序容器  边车模式的一般方式如上图所示，可以看到应用程序容器和边车容器共享了许多资源：\n 部分文件系统 主机名 网络 其他  我们通过下面的例子来看一下边车容器存在的必要性以及好处，图示如下：\n其中主容器是一个web服务，而日志处理边车容器的工作就是收集本地磁盘的服务器日志，并将其流式传输至存储集群，这样做的好处有：\n 容器是资源计算和调度的基本单位，所以可以优先配置主Web服务器的cgroup使得其处理延时降低，而日志处理容器则在web服务器空闲时使用cpu时间片进行日志处理 将模块化和可重用的组件封装成边车，可达到功能内聚，应用可被划分明确的边界进行解耦（方便接入、测试调试、状态处理等），最重要的是可以被不同主容器作为边车容器复用  大使模式（Ambassadors pattern） #   改变和管理应用容器与外部世界的通信方式\n 第一次看大使模式，很可能会想这不就是另一种形式的边车模式吗？其实不然，首先第一点，大使模式下所有的请求响应信息交换全部是大使容器来完成的，应用程序容器只能和大使容器进行交流。\n这种模式主要利用的特性是同一Pod中的容器可以共享相同的localhost网络接口，而且可以从两个角度看大使容器：\n 内到外：让我们以访问一个存储区域为例，假设该存储区域的大小不断增长，必须分成更多的子系统。在这种情况下，为了不干预主容器并且必须对所有受影响的服务实施相同的新访问逻辑，创建一个大使容器来调解对存储区域的访问是个不错的选择 外到内：让我们设想一下，我们要测试微服务的新版本，可以通过大使容器控制请求量到相关部署  适配器模式（Adapter pattern） #   确保应用程序实现统一的监控接口\n 真实世界的应用程序大概率会有出现下面列出的几种情况：\n 一部分服务自行开发（可能有新老标准差异），一部分使用开源项目 服务的编写语言多样，日志记录、监控也多样  假设我们需要有效地监控和运维应用程序，这就要求应用程序可以提供统一的通用接口来进行指标收集。这就是适配器发挥作用的场景了，对于不同应用容器提供的不同接口，可以使用适配器适配这种异构性并转化为一致的接口且原有服务代码不需要做任何改动。\n主应用程序通过localhost或者volume与适配器容器通信，适配器经过一层处理提供统一的输出给外部使用者，一些常用的使用场景如下：\n 监控：适配器将应用程序容器公开的监控接口转换为通用监控系统所期望的接口 日志：适配器提供统一的日志记录输出  多节点模式 #  不要将模块化容器局限于单机容器协调上，其实模块化容器还可以使构建协调的多节点分布式应用程序变得更加容易。接下来将描述其中的三种分布式系统模，与前一节中的模式一样，这些模式也需要对 Pod 这个逻辑概念的支持。\n领导选举模式（Leader election pattern） #  分布式系统中最常见的问题就是领导选举问题，副本被普遍使用在一个组件的多个相同的实例之间共享负载，副本的另一个更加复杂的作用就是使得某一特定副本作为整个部署集的leader，其他副本作为热备（这个区分过程比较复杂），当原本Leader宕机时可以快速被选举为新的Leader，以恢复系统功能。系统甚至可以并行地进行领导者选举，例如多个分片均需要确定领导者。\n上图介绍了一个简单的分布式选举的例子：图中三个副本，任何一个副本都有可能成为主副本，首先第一个副本为主，若其不巧发生故障，第二阶段就会通过选举将第三个副本变成主副本，最后，第一个副本回复，重新加入集群，第三副本依旧作为主节点运行调度。\n现在确实有许多类库可以进行领导者选举，但它们通常比较复杂并且难以被正确理解和使用，此外，它们还受到特定编程语言实现的限制。\n所以本部分探讨的就是将领导者选举机制从应用程序中剥离至领导者选举专属容器中，我们可以考虑提供一组领导者选举容器，每个容器都与需要进行领导者选举的应用程序共同调度，这样就可以在这些领导者选举容器之间执行选举。\n同时，它们可以在localhost 上为需要进行领导者选举的应用程序容器提供一个简化的HTTP API (例如becomeLeader、renewLeadership 等)。\n这些领导者选举容器只需要由这个复杂领域的专家进行一次性构建即可，然后不管应用程序开发人员选择何种编程语言，都可以复用其简化的接口。这种方式代表了软件工程中最好的抽象和封装过程。\n工作队列模式（Work queue pattern） #  一个简单通用的容器化工作队列图示如下：\n最左侧提供了一组需要被执行的工作项，然后工作队列管理容器接受输入工作项，将其分发给多个执行器进行消费，并且多个执行器中间没有任何交互，这样的好处是可以根据实际运行情况增加执行器数量来赢取时间。\n虽然工作队列和领导者选举一样，是一个研究得很透彻的课题并且有很多框架对它们进行了很好的实现，但这些分布式系统设计模式仍然是可以在面向容器的架构中获益。在以前的系统中，框架将程序限制在单一的语言环境中（如Python中的Celery）。\n对于一个容器，由于run()\u0026amp;mount()接口的实现，使得实现一个通用的工作队列框架变得简单直接，可以将任意的处理代码打包成一个容器，再结合任意数据就构建成了一个完整的工作队列系统。开发完整工作队列所涉及的所有其他工作都可以由通用工作队列框架处理，并且可以被任何有相同需求的系统复用，用户代码集成等细节让我们看看下面的图示：\n 通用工作队列的图示，可重用框架容器以深灰色显示，而开发人员容器以浅灰色显示。\n 让我们结合上面的两张图一起看看，在我看来这就是一个从第一张图抽象出用户自定义代码从而形成第二张图的过程，且看我详细列出关键点。\n源容器接口 #  用户定义的工作项由工作队列管理容器接收，此时涉及到一个论文中没有描述的点就是队列管理容器对于接收的工作项是默认进行了标准定义的，也就是说工作项都是定义好的标准输入。但实际情况并不可能让所有的输入项都有相同的输入标准，必须由一段用户自定义的处理代码来将输入标准化，不知机智的你是否想到了大使模式：\n执行器容器接口 #  当工作队列管理容器获取了对应的工作项，接下来的工作就是下发给执行器进行处理，具体做法是调用一次性API触发工作流，并且在工作容器的整个生命周期是不会有其他调用产生的。\n分散/聚集模式（Scatter/gather pattern） #  最后一个我们要着重介绍的设计模式是分散/收集模式（终于是最后一个:face_with_thermometer:），首先简单介绍一下这个模式：\n 分散/收集模式是一个树形模式，当外部客户端向根节点发送一个初始请求，根节点会将这个请求分发给大量服务器，每个服务器分片均返回部分数据，然后跟节点将这些部分结果组合起来形成一个针对原始请求的完整响应。\n 对于上面这样一个流程，实际上可以抽象出以下几个标准出来：\n 分散请求（fanning out the requests） 收集响应（gathering the responses） 与客户端交互（interacting with the client） \u0026hellip;  上述流程的大部分代码都是通用的，就像面向对象编程中一样，我们可以将这个一个个拆分实现并进行容器化。\n值得一提的是要实现一个分散/收集系统，需要一个用户提供两个容器：\n 第一种容器实现叶子节点计算，该容器执行部分计算并返回相应的结果 第二种容器是合并容器，该容器需要汇总所有叶子容器的计算结果，并组织成一个单一的响应后输出给用户  说明 #  主体内容来自：\n 论文《Design patterns for container-based distributed systems》 书籍《Designing Distributed Systems》  不错的同类型资料：\n 【译】基于容器的分布式系统设计模式  最近一直在了解低代码开发平台相关知识，就是前面提到的一站式机器学习云研发平台，其中关于资源管理这块离不开k8s的使用，因此花了不少精力在这上面，不出意外会出一个系列的学习笔记，而且在学习分布式设计模式的同时也对分布式系统产生了一些兴趣，这一块后续会一起好好研究研究。\n"}),a.add({id:6,href:'/k8s/docs/03_appendix/01.%E5%9B%BE%E8%A7%A3OSI%E4%B8%83%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/',title:"01.图解 Osi七层网络模型",section:"第三部分：附录",content:"简单图解OSI七层网络模型 #   翻译自Bradley Mitchell的《The Layers of the OSI Model Illustrated》\n Open Systems Interconnection(OSI)定义了一个网络框架，其以层为单位实现了各种协议，同时会将控制权逐层传递。目前OSI主要作为教学工具被使用，其在概念上将计算机网络结构按逻辑顺序划分为7层。\n较低层处理电信号、二进制数据块以及路由这些数据以便在网络中的穿梭；从用户的角度来看，更高的层次包括网络请求和响应、数据的表示和网络协议。\nOSI模型最初被认为是构建网络系统的标准体系结构，今天许多流行的网络技术都可以看出OSI的分层设计。\n物理层（Physical Layer） #  物理层是OSI模型的第一层，其职责在于通过网络通信媒介将比特流数据从发送（源）设备的物理层传输到接收（终）设备的物理层。\n第一层技术的例子包括以太网电缆和集线器。此外，集线器和其他中继器是在物理层起作用的标准网络设备，电缆连接器也是如此。\n在物理层，数据通过物理介质支持的以下信号类型进行传输:\n 电压 无线电频率 红外脉冲 普通光  数据链路层（Data Link Layer） #  当从物理层获取数据时，数据链路层会检查物理传输错误，并将比特数据打包成数据帧。数据链路层还管理着物理寻址方案，例如以太网的MAC地址，用于控制网络设备对物理介质的访问。\n因为数据链路层是 OSI 模型中最复杂的一层，所以它通常被分成两部分: 媒体访问控制子层和逻辑链路控制子层。\n网络层（Network Layer） #  网络层在数据链路层之上增加了路由的概念。每当数据抵达网络层时，就会检查每个帧中包含的源地址和目标地址，以确定数据是否已到达其最终目的地。如果数据已经到达最终目的地，第3层就会将数据格式化并打包为数据包交付给运输层，否则网络层会更新目的地址并将帧推送到下层。\n为了支持路由，网络层需要一个维护逻辑地址，比如网络设备的IP地址。网络层还管理着这些逻辑地址和物理地址之间的映射，在IPv4网络中，这种映射通过地址解析协议(ARP)完成，IPv6使用邻居发现协议(NDP)。\n运输层（Transport Layer） #  传输层通过网络连接传输数据。TCP (传输控制协议)和 `UDP (用户数据报协议)是传输层比较常见且有代表性的协议。不同的传输协议可能支持一系列可选功能，包括错误恢复、流控制和支持重新传输。\n会话层（Session Layer） #  会话层位于第五层，其管理着网络连接事件顺序和流程的启动和关闭。它支持多种类型的连接，这些连接可以动态地创建并在单个网络上运行。\n表示层（Presentation Layer） #  表示层位于第六层，就功能相对来说是OSI模型各层中最简单的。其着力于消息数据的语法处理，如格式转换和支持其上一层（应用层）所需的加密/解密。\n应用层（Application Layer） #  应用层为终端用户使用的应用提供网络服务（处理用户数据的协议）。举个例子，在Web浏览器应用程序中，应用层协议HTTP打包发送和接收网页内容所需的数据。同时应用层也会向表示层提供或获取数据。\n说明 #  本文主体内容来翻译自Bradley Mitchell的《The Layers of the OSI Model Illustrated》，衍生开的话还有以下不错的书籍资料：\n 计算机网络-第7版-谢希仁 趣谈网络协议 图解TCP/IP  大家有兴趣的可以看一看。\n"})})()