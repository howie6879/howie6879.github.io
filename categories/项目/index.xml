<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>项目 on 老胡的储物柜</title>
    <link>https://www.howie6879.com/categories/%E9%A1%B9%E7%9B%AE/</link>
    <description>Recent content in 项目 on 老胡的储物柜</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 09 May 2024 21:39:47 +0800</lastBuildDate><atom:link href="https://www.howie6879.com/categories/%E9%A1%B9%E7%9B%AE/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>写了个夸克网盘免费资源搜索引擎</title>
      <link>https://www.howie6879.com/post/2024/02.quark_so/</link>
      <pubDate>Thu, 09 May 2024 21:39:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2024/02.quark_so/</guid>
      <description>夸克搜，免费优雅的网盘资源搜索引擎，资源来自于互联网 👉 https://www.quark.so/ ，文末可进群。 介绍 为了方便我老婆做网盘分佣项目创业，业余和朋友整了个网盘搜索引擎方便她操作，接下来简单介绍下这套系统的功能。 界面 首页大家在前面已经看到了，进来直接在输入框搜索功能就行，比如搜索三体网飞版本： 点进去是详情页，转</description>
    </item>
    
    <item>
      <title>周刊的今日推荐功能上线</title>
      <link>https://www.howie6879.com/post/2022/09.weekly_today/</link>
      <pubDate>Wed, 11 May 2022 20:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2022/09.weekly_today/</guid>
      <description>我的信息周刊，记录这周我看到的有价值的信息，主要针对计算机领域，内容主题极大程度被我个人喜好主导。这个项目核心目的在于记录让自己有印象的信息做一个留存以及共享。 我的周刊自21-08-16日启动，自此已经写了快一年了，不出意外也会持续更新下去🥳。 和大家通知一件事，我的周刊上线了今日</description>
    </item>
    
    <item>
      <title>Liuli在树莓派上的部署教程</title>
      <link>https://www.howie6879.com/post/2022/08.liuli_deploy_on_pi/</link>
      <pubDate>Thu, 05 May 2022 20:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2022/08.liuli_deploy_on_pi/</guid>
      <description>Liuli的目标是帮助你一站式构建多源、干净、个性化的阅读环境，目前还在缓慢开发迭代中，欢迎更多的朋友参与进来，其当前可应用的场景有： 基于Liuli构建纯净的RSS公众号信息流 基于Liuli追更&amp;amp;阅读小说 建议使用前看一下上面的文章，有个大概印象，接下来将根据以下方式来介绍</description>
    </item>
    
    <item>
      <title>基于Liuli追更&amp;阅读小说</title>
      <link>https://www.howie6879.com/post/2022/04_build_novel_info_flow_based_on_liuli/</link>
      <pubDate>Sun, 13 Feb 2022 20:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2022/04_build_novel_info_flow_based_on_liuli/</guid>
      <description>Liuli历史文章介绍： 起因: 打造一个干净且个性化的公众号阅读环境 公众号应用场景：基于Liuli构建纯净的RSS公众号信息流 这次Liuli给大家带来了小说书籍阅读场景的订阅解决方案，搭建方式和之前基于Liuli构建纯净的RSS公众号信息流没什么区别。 最终效果如下图： 使用 Liuli</description>
    </item>
    
    <item>
      <title>基于Liuli构建纯净的RSS公众号信息流</title>
      <link>https://www.howie6879.com/post/2022/02_build_a_clean_wechat_rss_based_liuli/</link>
      <pubDate>Wed, 26 Jan 2022 20:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2022/02_build_a_clean_wechat_rss_based_liuli/</guid>
      <description>首先介绍下，Liuli是什么？这是我最近开发的一个开源项目，主要目的是为了让有阅读习惯的朋友快速构建一个多源、干净、个性化的阅读环境。 为什么叫Liuli？ Liuli原来命名为2C，后面交流群的朋友提供了琉璃这个名字，取自梅尧臣《缑山子晋祠 会善寺》中的琉璃开净界，薜荔启禅关，其寓意</description>
    </item>
    
    <item>
      <title>老胡的周刊2021年度汇总|附PDF下载</title>
      <link>https://www.howie6879.com/post/2022/01_2021_weekly_all/</link>
      <pubDate>Mon, 03 Jan 2022 20:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2022/01_2021_weekly_all/</guid>
      <description>在互联网时代，信息的过滤与聚合是非常重要的，作为一名程序员，我经常会浏览过程中看到各种有意思的项目、资源、软件以及一些网站，如果浏览目标让人眼前一亮，那就说明我过滤到了有意思的东西。 一般我会选择相应的软件进行记录然后聚合起来慢慢看，但随着时间流逝，一些好的资源总是错过了，或者某个</description>
    </item>
    
    <item>
      <title>🎉我的周刊|静态网站上线</title>
      <link>https://www.howie6879.com/post/2021/25_howie_weekly_site/</link>
      <pubDate>Sun, 31 Oct 2021 20:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2021/25_howie_weekly_site/</guid>
      <description>后续我的周刊相关文章全部在此网站展现，提供专门的RSS订阅，有兴趣可以专注下。 我的信息周刊，每周记录我看到的有价值的信息，主要针对计算机领域，内容主题极大程度被我个人喜好主导。这个项目核心目的在于记录让自己有印象的信息做一个留存以及共享。 我的周刊项目开始于2021-08-16，到</description>
    </item>
    
    <item>
      <title>打造一个干净且个性化的公众号阅读环境</title>
      <link>https://www.howie6879.com/post/2021/12_build_a_clean_env_for_reading/</link>
      <pubDate>Sun, 09 May 2021 20:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2021/12_build_a_clean_env_for_reading/</guid>
      <description>背景 一个月前的下班时间，当时我正在看公众号，准备感受下今天的技术文章，不愉快的事情发生了，竟然一连好几篇文档点进去都是广告，至今想起，那种心情仍旧挥散不去。 于是我产生了一个想法，为什么不构建一个干净且个性化的个人阅读环境呢？作为一名微信公众号的重度用户，公众号一直被我设为汲取知识</description>
    </item>
    
    <item>
      <title>Liuli 使用教程</title>
      <link>https://www.howie6879.com/post/2021/11_2c_quick_start/</link>
      <pubDate>Sun, 11 Apr 2021 21:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2021/11_2c_quick_start/</guid>
      <description>2C的目的是为了构建一个多源（公众号、RSS）、干净、个性化的阅读环境，如果你在公众号阅读体验下深切感受到对于广告的无奈，那么这个项目就是你需要的，一起看看怎么安装部署2C吧。 开始 2C项目对于一些基础环境是有一点要求的，为了尽可能减少开发者部署使用的复杂度（特别是非Python开</description>
    </item>
    
    <item>
      <title>不论微信钉钉，我写了个通用消息监控处理机器人</title>
      <link>https://www.howie6879.com/post/2019/10_monitor_os_notifications/</link>
      <pubDate>Mon, 21 Oct 2019 08:37:56 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2019/10_monitor_os_notifications/</guid>
      <description>上一篇推文中提到，我希望通过监控微信对应的聊天记录，来实现一个消息自动处理的机器人，上篇文章实现的就是自动保存感兴趣的文章到Bear。 虽说那篇文章比较实用，也有很多朋友表示喜欢，但还有不少缺陷： 对技术薄弱的朋友复现困难，项目很多配置需要手动生成，前期校验工作很多 二次开发比较困难，</description>
    </item>
    
    <item>
      <title>利用微信同步文章到Bear</title>
      <link>https://www.howie6879.com/post/2019/09_save_wechat_article_to_bear/</link>
      <pubDate>Mon, 16 Sep 2019 08:37:56 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2019/09_save_wechat_article_to_bear/</guid>
      <description>如果图片失效：见【教程&amp;amp;工具】微信同步文章到Bear 在我日常工作中，我会将各种互联网以及生活中产出的信息汇总到Bear，再通过Bear的云同步使我各个终端的信息保持一致。 以前在使用有道云笔记的时候，有个功能我很喜欢，就是当看到一篇想收藏的文章的话，就可以直接右上角发送到有</description>
    </item>
    
    <item>
      <title>以Ruia为例：如何实现一个Python爬虫框架</title>
      <link>https://www.howie6879.com/post/2019/03_how_to_build_a_web_scraping_framework_with_python/</link>
      <pubDate>Fri, 15 Mar 2019 08:37:56 +0800</pubDate>
      
      <guid>https://www.howie6879.com/post/2019/03_how_to_build_a_web_scraping_framework_with_python/</guid>
      <description>这篇文章的题目有点大，但这并不是说我自觉对Python爬虫这块有多大见解，我只不过是想将自己的一些经验付诸于笔，对于如何写一个爬虫框架，我想一步一步地结合具体代码来讲述如何从零开始编写一个自己的爬虫框架 2018年到如今，我花精力比较多的一个开源项目算是Ruia了，这是一个基于Py</description>
    </item>
    
    <item>
      <title>talospider - 简单的爬虫框架</title>
      <link>https://www.howie6879.com/post/2017/08_talospider-intro/</link>
      <pubDate>Wed, 07 Jun 2017 23:56:08 +0000</pubDate>
      
      <guid>https://www.howie6879.com/post/2017/08_talospider-intro/</guid>
      <description>为什么写这个？ 一些简单的页面，无需用比较大的框架来进行爬取，自己纯手写又比较麻烦 因此针对这个需求写了talospider: 1.针对单页面的item提取 - 具体介绍点这里 2.spider模块 - 具体介绍点这里 介绍&amp;amp;&amp;amp;使用 item 这个模块是可以独立使用的，对于一些请求比较简单的</description>
    </item>
    
    <item>
      <title>owllook -- 一个简洁的网络小说搜索引擎</title>
      <link>https://www.howie6879.com/post/2017/07_owllook-intro/</link>
      <pubDate>Fri, 10 Mar 2017 19:09:50 +0000</pubDate>
      
      <guid>https://www.howie6879.com/post/2017/07_owllook-intro/</guid>
      <description>前言 上一篇介绍了自己在使用sanic过程中遇到的一些问题，这次就想介绍下这个owllook，上面是演示demo，具体可以见https://www.owllook.net/ 本项目纯属共享学习之用，不得用于商业！ 首先我想说下目前的项目进度： v0.1.0： 小说的基本搜索解析功能 搜索记录</description>
    </item>
    
    <item>
      <title>ITBooks—简单的书籍下载小工具</title>
      <link>https://www.howie6879.com/post/2017/01_itbooks/</link>
      <pubDate>Thu, 26 Jan 2017 23:07:42 +0000</pubDate>
      
      <guid>https://www.howie6879.com/post/2017/01_itbooks/</guid>
      <description>1.前言 我有个习惯就是收藏一些书籍，比如说编程类的，总是会去某些网站刷刷，若有新书籍更新恰又是自己感兴趣的，自然会立马下载下来，写程序的都知道，编程书籍更新换代太快，国内的翻译的速度很难全面地跟上，对此，阅读国外的电子书籍是个途径。 很早就想写个书籍集成的脚本，本周女朋友回学校改论</description>
    </item>
    
  </channel>
</rss>
