<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML&amp;DL on 老胡的储物柜</title>
    <link>https://www.howie6879.cn/categories/mldl/</link>
    <description>Recent content in ML&amp;DL on 老胡的储物柜</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 29 Jan 2021 22:35:47 +0800</lastBuildDate><atom:link href="https://www.howie6879.cn/categories/mldl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>一站式机器学习云开发平台</title>
      <link>https://www.howie6879.cn/post/2021/04_ml_cloud_dev_platform/</link>
      <pubDate>Fri, 29 Jan 2021 22:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2021/04_ml_cloud_dev_platform/</guid>
      <description>本篇是关于自身在机器学习这块工作经验的思考总结 我希望构建一个机器学习云开发平台，目标在于解决以下问题： 团队协作：项目管理，技术&amp;amp;业务的共享如何体现在实际解决问题的过程中； 资源调度：数据处理、模型训练； 模块共享：低代码甚至无代码； 快速开发：快速试错、实践、测试、部署； 需求-</description>
    </item>
    
    <item>
      <title>梯度下降推导</title>
      <link>https://www.howie6879.cn/post/2021/03_gd_math_note/</link>
      <pubDate>Thu, 14 Jan 2021 22:35:47 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2021/03_gd_math_note/</guid>
      <description>以感知器为例，可以梯度下降来学习合适的权重和偏置： 假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为： \[ \bar{y}^i = w_1x_1^i+w_2x_2^i+...+w_nx_n^i+b \] 任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示： \[ e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2} \] 那么所有误差的和可以表示为： \[ \begin{aligned} E &amp;= e^1+e^2+...+e^n \\ &amp;= \sum_{i=1}^ne^i \\ &amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 \end{aligned} \] 想象一下，当你从山顶往下</description>
    </item>
    
    <item>
      <title>nndl_note: 深度神经⽹络为何很难训练</title>
      <link>https://www.howie6879.cn/post/2020/02_why_are_deep_neural_networks_hard_to_train/</link>
      <pubDate>Fri, 18 Dec 2020 21:13:58 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2020/02_why_are_deep_neural_networks_hard_to_train/</guid>
      <description>消失的梯度问题 导致梯度消失的原因 在更加复杂⽹络中的不稳定梯度 其它深度学习的障碍 上一章提到了神经网络的一种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入得到一个近似的输出。 普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许</description>
    </item>
    
    <item>
      <title>nndl_note: 神经⽹络可以计算任何函数的可视化证明</title>
      <link>https://www.howie6879.cn/post/2019/12_a_visual_proof_that_neural_nets_can_compute_any_function/</link>
      <pubDate>Sat, 02 Nov 2019 21:13:58 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/12_a_visual_proof_that_neural_nets_can_compute_any_function/</guid>
      <description>两个预先声明 一个输入和一个输出的普遍性 多个输入变量 S型神经元的延伸 修补阶跃函数 结论 本章其实和前面章节的关联性不大，所以大可将本章作为小短文来阅读，当然基本的深度学习基础还是要有的。 主要介绍了神经⽹络拥有的⼀种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入$x</description>
    </item>
    
    <item>
      <title>nndl_note: 改进神经⽹络的学习⽅法</title>
      <link>https://www.howie6879.cn/post/2019/11_improving_the_way_neural_networks_learn/</link>
      <pubDate>Wed, 30 Oct 2019 19:40:02 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/11_improving_the_way_neural_networks_learn/</guid>
      <description>交叉熵代价函数 引⼊交叉熵代价函数 交叉熵的含义？源⾃哪⾥？ 过度拟合和规范化 规范化 为何规范化可以帮助减轻过度拟合 规范化的其他技术 权重初始化 如何选择神经⽹络的超参数 参考 万丈高楼平地起，反向传播是深度学习这栋大厦的基石，所以在这块花多少时间都是值得的 前面一章，我们深入理解了反向传播算法如</description>
    </item>
    
    <item>
      <title>nndl_note: 反向传播算法如何工作</title>
      <link>https://www.howie6879.cn/post/2019/08_how_does_the_back_propagation_algorithm_work/</link>
      <pubDate>Fri, 10 May 2019 16:36:44 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/08_how_does_the_back_propagation_algorithm_work/</guid>
      <description>热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 关于代价函数的两个假设 反向传播的四个基本方程 输出层误差的⽅程 使用下一层的误差表示当前层的误差 代价函数关于⽹络中任意偏置的改变率 代价函数关于任何⼀个权重的改变率 反向传播算法 反向传播：全局观 参考 前面一章，我们通过了梯度下降算法实现目标函数的最</description>
    </item>
    
    <item>
      <title>nndl_note: 识别手写字</title>
      <link>https://www.howie6879.cn/post/2019/07_use_neural_network_recognize_handwriting/</link>
      <pubDate>Wed, 08 May 2019 16:36:44 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/07_use_neural_network_recognize_handwriting/</guid>
      <description>感知器 S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 随机梯度下降算法 实现数字分类模型 参考 Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。 神经网络是一门重要的机器</description>
    </item>
    
    <item>
      <title>神经网络基础</title>
      <link>https://www.howie6879.cn/post/2019/01_neural_network_foundation/</link>
      <pubDate>Thu, 03 Jan 2019 08:37:56 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/01_neural_network_foundation/</guid>
      <description>要想入门以及往下理解深度学习，其中一些概念可能是无法避免地需要你理解一番，比如，备份地址： 什么是感知器 什么是神经网络 张量以及运算 微分 梯度下降 带着问题出发 在开始之前希望你有一点机器学习方面的知识，解决问题的前提是提出问题，我们提出这样一个问题，对MNIST数据集进行分析，然后在解决</description>
    </item>
    
    <item>
      <title>读 Character-level CNN for Text Classiﬁcation</title>
      <link>https://www.howie6879.cn/post/2019/00_character-level_cnn_for_text_classi%EF%AC%81cation/</link>
      <pubDate>Thu, 03 Jan 2019 08:37:56 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2019/00_character-level_cnn_for_text_classi%EF%AC%81cation/</guid>
      <description>这篇论文提供了一个关于字符级卷积网络(ConvNets)在文本分类中应用实证研究，我们构建了几个大型数据集，以表明字符级卷积网络可以实现最先进的或竞争性的结果，针对传统模型（如词袋，n-gram及其TFIDF变体）和深度学习模型（如基于单词的ConvNets和循环神经网络）进行比</description>
    </item>
    
    <item>
      <title>如何用Python创建一个简单的神经网络</title>
      <link>https://www.howie6879.cn/post/2018/07_how-to-create-asimple-neural-network-in-python/</link>
      <pubDate>Thu, 13 Dec 2018 16:03:05 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2018/07_how-to-create-asimple-neural-network-in-python/</guid>
      <description>如何用Python创建一个简单的神经网络 原文地址：How to Create a Simple Neural Network in Python 作者：Dr. Michael J. Garbade 翻译：howie6879 理解神经网络如何工作的最好方式是自己动手创建一个，这篇文章将会给你演示怎么做到这一点 神经网络(NN)，也称之为人工神经网络(ANN)，它是机器学习领域中学习算法集合中</description>
    </item>
    
    <item>
      <title>统计学习方法笔记：3.k近邻法</title>
      <link>https://www.howie6879.cn/post/2018/06_39/</link>
      <pubDate>Sat, 29 Sep 2018 17:54:42 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2018/06_39/</guid>
      <description>这是我参加mlhub123组织的书籍共读计划的读书笔记，活动见mlhub第一期读书计划 阅读章节：第三章：k近邻法 开始时间：2018-09-29 结束时间：2018-10-23（比较忙） 目标：读完第三章，掌握基本概念，产出一篇笔记 博客地址 k近邻法（k-nearest neighbor，</description>
    </item>
    
    <item>
      <title>统计学习方法笔记：2.感知机</title>
      <link>https://www.howie6879.cn/post/2018/05_38/</link>
      <pubDate>Tue, 18 Sep 2018 17:54:42 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2018/05_38/</guid>
      <description>这是我参加mlhub123组织的书籍共读计划的读书笔记，活动见mlhub第一期读书计划 阅读章节：第二章：感知机 开始时间：2018-09-18 结束时间：2018-09-21 目标：读完第二章，掌握基本概念，产出一篇笔记 博客地址 本章主要介绍了二类分类的线性分类模型：感知机： 感知机模型 感</description>
    </item>
    
    <item>
      <title>统计学习方法笔记：1.统计学习方法概论</title>
      <link>https://www.howie6879.cn/post/2018/04_37/</link>
      <pubDate>Sun, 16 Sep 2018 17:54:42 +0800</pubDate>
      
      <guid>https://www.howie6879.cn/post/2018/04_37/</guid>
      <description>这是我参加mlhub123组织的书籍共读计划的读书笔记，活动见mlhub第一期读书计划 阅读章节：第一章：统计学习方法概论 开始时间：2018-09-14 结束时间：2018-09-16 目标：读完第一章，掌握基本概念，产出一篇笔记 博客地址 第一章主要对全书内容做了一个内容的概括： 统计学习</description>
    </item>
    
    <item>
      <title>Python之朴素贝叶斯对展会数据分类</title>
      <link>https://www.howie6879.cn/post/2016/06_python-naive-bayes-classification-of-exhibition-data/</link>
      <pubDate>Thu, 08 Sep 2016 20:34:40 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/post/2016/06_python-naive-bayes-classification-of-exhibition-data/</guid>
      <description>目的 在公司实习，分别从国内国外两个网站爬取了一些展会数据，在数据处理上目前需要将其按照各个类别分类好，并提供对应展会地址的经纬度，国内数据如下： 国内数据比较少，占四百多条，在类别上来看有所属行业这一列，所以比较好处理，国外数据就有些尴尬： 国外网站展会数据将近五万多条，跟分类有关的</description>
    </item>
    
  </channel>
</rss>
