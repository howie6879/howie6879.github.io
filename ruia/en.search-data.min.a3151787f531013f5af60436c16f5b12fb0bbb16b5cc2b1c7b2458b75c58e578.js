'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/ruia/docs/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/','title':"快速开始",'section':"Docs",'content':"快速开始 #   基于Ruia快速实现一个以Hacker News为目标的爬虫\n 本文主要通过对Hacker News的爬取示例来展示如何使用Ruia，下图红框中的数据就是爬虫脚本需要爬取的目标：\n开始前的准备工作：\n 确定已经安装Ruia：pip install ruia -U 确定可以访问Hacker News  第一步：定义 Item #  Item的目的是定义目标网站中你需要爬取的数据，此时，爬虫的目标数据就是页面中的Title和Url，怎么提取数据，Ruia的Field类提供了以下三种方式提取目标数据：\n XPath Re CSS Selector  这里我们使用CSS Selector来提取目标数据，用浏览器打开Hacker News，右键审查元素：\n Notice: 本教程爬虫例子都默认使用CSS Selector的规则来提取目标数据\n 显而易见，每页包含30条资讯，那么目标数据的规则可以总结为：\n   Param Rule Description     target_item tr.athing 表示每条资讯   title a.storylink 表示每条资讯里的标题   url a.storylink-\u0026gt;href 表示每条资讯里标题的链接    规则明确之后，就可以用Item来实现一个针对于目标数据的ORM，创建文件items.py，复制下面代码：\nfrom ruia import AttrField, TextField, Item class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) 这段代码含义是：针对我们提取的目标HTML，我们定义了一个HackerNewsItem类，其包含了两个field：\n title：直接从文本提取 url：从属性提取  等等！target_item是什么？对于一个Item类来说，当其定义好网页目标数据后，Ruia提供两种方式进行获取Item：\n get_item：获取网页的单目标，比如目标网页的标题，此时无需定义target_item； get_items：获取网页的多目标，比如当前目标网页Hacker News中的title和url一共有30个，这时就必须定义target_item来寻找多个目标块；target_item的作用就是针对这样的工作而诞生的，开发者只要定义好这个属性（此时Ruia会自动获取网页中30个target_item），然后每个target_item里面包含的title和url就会被提取出来。  第二步：测试 Item #  Ruia为了方便扩展以及自由地组合使用，本身各个模块之间耦合度是极低的，每个模块都可以在你的项目中单独使用；你甚至只使用ruia.Item、Ruia.TextField和ruia.AttrField来编写一个简单的爬虫。\n脚本调试 #  基于这个特性，我们可以直接在脚本里面测试HackerNewsItem：\nimport asyncio from ruia import Item, TextField, AttrField class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) async def test_item(): url = \u0026#39;https://news.ycombinator.com/news?p=1\u0026#39; async for item in HackerNewsItem.get_items(url=url): print(\u0026#39;{}: {}\u0026#39;.format(item.title, item.url)) if __name__ == \u0026#39;__main__\u0026#39;: # Python 3.7 Required. asyncio.run(test_item()) # For Python 3.6 # loop = asyncio.get_event_loop() # loop.run_until_complete(test_item()) 接下来，终端会输出以下日志：\n[2021:04:04 21:37:23] INFO Request \u0026lt;GET: https://news.ycombinator.com/news?p=1\u0026gt; How to bypass Cloudflare bot protection: https://jychp.medium.com/how-to-bypass-cloudflare-bot-protection-1f2c6c0c36fb The EU has archived all of the “Euromyths” printed in UK media: https://www.thelondoneconomic.com/news/the-eu-have-archived-all-of-the-euromyths-printed-in-uk-media-and-it-makes-for-some-disturbing-reading-108942/ Laser: Learning a Latent Action Space for Efficient Reinforcement Learning: https://arxiv.org/abs/2103.15793 StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery: https://github.com/orpatashnik/StyleCLIP 命令行调试 #  为了使Ruia的脚本调试过程更加方便优雅，开发者还可以直接使用ruia-shell插件进行调试，首先进行安装：\npip install -U ruia-shell pip install ipython 具体使用如下：\n➜ ~ ruia_shell https://news.ycombinator.com/news\\?p\\=1 ✨ Write less, run faster(0.8.2). __________ .__ .__ .__ .__ \\______ \\__ __|__|____ _____| |__ ____ | | | | | _/ | \\  \\__ \\  / ___/ | \\_/ __ \\| | | | | | \\  | / |/ __ \\_ \\___ \\| Y \\  ___/| |_| |__ |____|_ /____/|__(____ / /____ \u0026gt;___| /\\___ \u0026gt;____/____/ \\/ \\/ \\/ \\/ \\/ Available Objects : response : ruia.Response request : ruia.Request Available Functions : attr_field : Extract attribute elements by using css selector or xpath text_field : Extract text elements by using css selector or xpath fetch : Fetch a URL or ruia.Request In [1]: request Out[1]: \u0026lt;GET https://news.ycombinator.com/news?p=1\u0026gt; In [2]: response Out[2]: \u0026lt;Response url[GET]: https://news.ycombinator.com/news?p=1 status:200\u0026gt; In [3]: text_field(css_select=\u0026#34;a.storylink\u0026#34;) Out[3]: \u0026#39;The EU has archived all of the “Euromyths” printed in UK media\u0026#39; In [4]: attr_field(css_select=\u0026#34;a.storylink\u0026#34;, attr=\u0026#34;href\u0026#34;) Out[4]: \u0026#39;https://www.thelondoneconomic.com/news/the-eu-have-archived-all-of-the-euromyths-printed-in-uk-media-and-it-makes-for-some-disturbing-reading-108942/\u0026#39; 如果文字不清楚，可看下图：\n第三步：编写 Spider #  Ruia.Spider是Ruia框架里面的核心控制类，它作用在于：\n 控制目标网页的请求Ruia.Request和响应Ruia.Response 可加载自定义钩子、插件、以及相关配置等，让开发效率更高  接下来会基于前面的Item脚本继续开发，具体代码如下：\n\u0026#34;\u0026#34;\u0026#34; Target: https://news.ycombinator.com/ pip install aiofiles \u0026#34;\u0026#34;\u0026#34; import aiofiles from ruia import Item, TextField, AttrField, Spider class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) class HackerNewsSpider(Spider): start_urls = [f\u0026#39;https://news.ycombinator.com/news?p={index}\u0026#39; for index in range(3)] concurrency = 3 # 设置代理 # aiohttp_kwargs = {\u0026#34;proxy\u0026#34;: \u0026#34;http://0.0.0.0:8765\u0026#34;} async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): yield item async def process_item(self, item: HackerNewsItem): \u0026#34;\u0026#34;\u0026#34;Ruia build-in method\u0026#34;\u0026#34;\u0026#34; async with aiofiles.open(\u0026#39;./hacker_news.txt\u0026#39;, \u0026#39;a\u0026#39;) as f: await f.write(str(item.title) + \u0026#39;\\n\u0026#39;) 本段代码的作用是：\n 爬取Hacker News的前三页内容，设置并发数为3，然后全部持久化到文件hacker_news.txt\n 开发者实现HackerNewsSpider必须是Spider的子类，代码中出现的两个方法都是Spider内置的：\n parse：此方法是Spider的入口，每一个start_urls的响应必然会被parse方法捕捉并执行； process_item：此方法作用是抽离出对Item提取结果的处理过程，比如这里会接受自定义Item类作为输入，然后进行处理持久化到文件。  第四步：运行 Start #   希望Ruia可以为你带来编写爬虫的乐趣 ：)\n 一切准备就绪，启动你的爬虫脚本吧！\nimport aiofiles from ruia import AttrField, Item, Spider, TextField class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#34;tr.athing\u0026#34;) title = TextField(css_select=\u0026#34;a.storylink\u0026#34;) url = AttrField(css_select=\u0026#34;a.storylink\u0026#34;, attr=\u0026#34;href\u0026#34;) class HackerNewsSpider(Spider): start_urls = [f\u0026#34;https://news.ycombinator.com/news?p={index}\u0026#34; for index in range(3)] concurrency = 3 async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): yield item async def process_item(self, item: HackerNewsItem): \u0026#34;\u0026#34;\u0026#34;Ruia build-in method\u0026#34;\u0026#34;\u0026#34; async with aiofiles.open(\u0026#34;./hacker_news.txt\u0026#34;, \u0026#34;a\u0026#34;) as f: await f.write(str(item.title) + \u0026#34;\\n\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: HackerNewsSpider.start()  Tips：如果你想在异步函数里面调用，执行await HackerNewsSpider.start() 即可\n 不到30行代码，你就实现了对Hacker News的爬虫脚本，并且脚本带有自动重试、并发控制、语法简单等特性。\n通过这个例子，你已经基本掌握了Ruia中Item、Middleware、Request等模块的用法，结合自身需求，你可以编写任何爬虫，例子代码见hacker_news_spider。\n第五步：扩展 #  Middleware #  Middleware的目的是对每次请求前后进行一番处理，分下面两种情况：\n 在每次请求之前做一些事 在每次请求后做一些事  比如此时爬取Hacker News，若希望在每次请求时候自动添加Headers的User-Agent，可以添加以下代码引入中间件：\nfrom ruia import AttrField, Item, Middleware, Spider, TextField middleware = Middleware() @middleware.request async def print_on_request(spider_ins, request): ua = \u0026#34;ruia user-agent\u0026#34; request.headers.update({\u0026#34;User-Agent\u0026#34;: ua}) print(request.headers) class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#34;tr.athing\u0026#34;) title = TextField(css_select=\u0026#34;a.storylink\u0026#34;) url = AttrField(css_select=\u0026#34;a.storylink\u0026#34;, attr=\u0026#34;href\u0026#34;) class HackerNewsSpider(Spider): start_urls = [f\u0026#34;https://news.ycombinator.com/news?p={index}\u0026#34; for index in range(3)] concurrency = 3 async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): yield item if __name__ == \u0026#34;__main__\u0026#34;: HackerNewsSpider.start(middleware=middleware) 这样，程序会在爬虫请求网页资源之前自动加上User-Agent，针对自动UA的功能点，Ruia已经专门编写了一个名为ruia-ua的插件来为开发者提升效率，使用非常简单，代码示例如下：\nfrom ruia import AttrField, TextField, Item, Spider from ruia_ua import middleware class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) class HackerNewsSpider(Spider): start_urls = [\u0026#39;https://news.ycombinator.com/news?p=1\u0026#39;, \u0026#39;https://news.ycombinator.com/news?p=2\u0026#39;] async def parse(self, response): # Do something... print(response.url) if __name__ == \u0026#39;__main__\u0026#39;: HackerNewsSpider.start(middleware=middleware) MongoDB #  对于数据持久化，你可以按照自己喜欢的方式去做，前面实例中介绍了如何将目标Item持久化到文件中。\n如果想将数据持久化到数据库（MongoDB）中，该怎么做？此时就到了凸显Ruia插件优势的时候了，你只需要安装ruia-motor：\npip install -U ruia-motor 然后再代码中引入ruia-motor：\nfrom ruia_motor import RuiaMotorInsert, init_spider from ruia import AttrField, Item, Spider, TextField class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#34;tr.athing\u0026#34;) title = TextField(css_select=\u0026#34;a.storylink\u0026#34;) url = AttrField(css_select=\u0026#34;a.storylink\u0026#34;, attr=\u0026#34;href\u0026#34;) class HackerNewsSpider(Spider): start_urls = [f\u0026#34;https://news.ycombinator.com/news?p={index}\u0026#34; for index in range(3)] concurrency = 3 # aiohttp_kwargs = {\u0026#34;proxy\u0026#34;: \u0026#34;http://0.0.0.0:1087\u0026#34;} async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): yield RuiaMotorInsert(collection=\u0026#34;news\u0026#34;, data=item.results) async def init_plugins_after_start(spider_ins): spider_ins.mongodb_config = {\u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 27017, \u0026#34;db\u0026#34;: \u0026#34;ruia_motor\u0026#34;} init_spider(spider_ins=spider_ins) if __name__ == \u0026#34;__main__\u0026#34;: HackerNewsSpider.start(after_start=init_plugins_after_start) 数据库中可以看到目标字段：\n是不是更简单了呢？\n第六步：深入了解Ruia #  本章简单介绍了下Ruia的快速实践，如果想要深入了解Ruia欢迎继续往下看，如果有和我交流的想法或者想加入交流群，请关注公众号加我微信：\n"});index.add({'id':1,'href':'/ruia/docs/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/1.%E6%A6%82%E8%A7%88/','title':"1.概览",'section':"入门指南",'content':"概览 #  Why Ruia #   Write less, Run faster :heart:\n Ruia是一个基于asyncio和aiohttp的异步爬虫框架，其诞生的核心理念也异常清晰，那就是：\n 更少的代码：能通用的功能就插件化，让开发者直接引用即可 更快的速度：由异步驱动  介绍 #  启程 #  "});index.add({'id':2,'href':'/ruia/docs/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/2.%E5%AE%89%E8%A3%85/','title':"2.安装",'section':"入门指南",'content':"安装 #  安装Python #  安装Ruia前，需要你的系统环境安装有Python3.6+环境，由于Ruia是第三方包，所以还需要你提前装有Python的包管理工具pip。\n如果确认准备好环境，请进入终端，做环境检查：\n[~] python --version Python 3.7.3 [~] pip --version pip 21.0.1 from ~/anaconda3/lib/python3.7/site-packages/pip (python 3.7) 安装Ruia #  请进入所在项目环境，如果没有特定环境就默认使用的是系统全局Python环境，然后利用pip进行安装：\n# For Linux \u0026amp; Mac pip install -U ruia[uvloop] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia 校验 #  让我们看看ruia是否安装成功：\n[~] python Python 3.7.3 (default, Mar 27 2019, 16:54:48) [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import ruia \u0026gt;\u0026gt;\u0026gt; ruia.__version \u0026#39;0.8.0\u0026#39; 上述ruia具体版本号请根据实际情况校验。\n"});index.add({'id':3,'href':'/ruia/docs/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/3.%E5%AE%9A%E4%B9%89-Item/','title':"3.定义 Item",'section':"入门指南",'content':"定义Item #  "});index.add({'id':4,'href':'/ruia/docs/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/4.%E8%BF%90%E8%A1%8C-Spider/','title':"4.运行 Spider",'section':"入门指南",'content':"运行Spider #  "});index.add({'id':5,'href':'/ruia/docs/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/5.%E4%B8%AA%E6%80%A7%E5%8C%96/','title':"5.个性化",'section':"入门指南",'content':"个性化 #  "});index.add({'id':6,'href':'/ruia/docs/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/6.%E6%8F%92%E4%BB%B6/','title':"6.插件",'section':"入门指南",'content':"插件 #  "});index.add({'id':7,'href':'/ruia/docs/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/7.%E5%B8%AE%E5%8A%A9/','title':"7.帮助",'section':"入门指南",'content':"帮助 #  在使用过程中有问题？你可以通过以下任意形式寻求帮助：\n 直接在本文下方进行留言 直接提Issue 查看Ruia更多基础概念 联系我：微信  希望你在使用Ruia开发的过程中，能提升效率、节省时间，哪怕只有一点点，那也是值得高兴的一件事。\n"});index.add({'id':8,'href':'/ruia/docs/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/1.Request/','title':"1. Request",'section':"基础概念",'content':"Request #  Request的主要作用是方便地处理网络请求，最终返回一个Response对象。\n主要提供的方法有：\n Request().fetch：请求一个网页资源，可以单独使用 Request().fetch_callback：为Spider类提供的和核心方法  Core arguments #   url：请求的资源链接 method：请求的方法，GET或者POST callback：回调函数 headers：请求头 load_js：目标网页是否需要加载js metadata：跨请求传递的一些数据 request_config：请求配置 request_session：aiohttp的请求session aiohttp_kwargs：请求目标资源可定义的其他参数  Usage #  通过上面的参数介绍可以知道，Request除了需要结合Spider使用，也可以单独使用：\nimport asyncio from ruia import Request request = Request(\u0026#34;https://news.ycombinator.com/\u0026#34;) response = asyncio.get_event_loop().run_until_complete(request.fetch()) # Output # [2018-07-25 11:23:42,620]-Request-INFO \u0026lt;GET: https://news.ycombinator.com/\u0026gt; # \u0026lt;Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}\u0026gt; How It Works? #  Request通过对aiohttp和pyppeteer的封装来实现对网页资源的异步请求\n"});index.add({'id':9,'href':'/ruia/docs/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/2.Response/','title':"2. Response",'section':"基础概念",'content':"Response #  Response的目的是返回一个统一且友好的响应对象，主要属性如下：\n url：请求的资源链接 metadata：跨请求传递的一些数据 html：源网站返回的资源数据 cookies：网站 cookies history：访问历史 headers：请求头 status：请求状态码  "});index.add({'id':10,'href':'/ruia/docs/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/3.Item/','title':"3. Item",'section':"基础概念",'content':"Item #  Item的主要作用是定义以及通过一定的规则提取源网页中的目标数据，它主要提供一下两个方法：\n get_item：针对页面单目标数据进行提取 get_items：针对页面多目标数据进行提取  Core arguments #  get_item和get_items方法接收的参数是一致的：\n html：网页源码 url：网页链接 html_etree：etree._Element对象  Usage #  通过上面的参数介绍可以知道，不论是源网站链接或者网站HTML源码，甚至是经过lxml处理过的etree._Element对象，Item能接收这三种类型的输入并进行处理\nimport asyncio from ruia import AttrField, TextField, Item class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) async def clean_title(self, value): return value async_func = HackerNewsItem.get_items(url=\u0026#34;https://news.ycombinator.com/\u0026#34;) items = asyncio.get_event_loop().run_until_complete(async_func) for item in items: print(item.title, item.url) 有时你会遇见这样一种情况，例如爬取Github的Issue时，你会发现一个Issue可能对应多个Tag。 这时，将Tag作为一个独立的Item来提取是不划算的， 我们可以使用Field字段的many=True参数，使这个字段返回一个列表。\nimport asyncio from ruia import Item, TextField, AttrField class GithiubIssueItem(Item): title = TextField(css_select=\u0026#39;title\u0026#39;) tags = AttrField(css_select=\u0026#39;a.IssueLabel\u0026#39;, attr=\u0026#39;data-name\u0026#39;, many=True) item = asyncio.run(GithiubIssueItem.get_item(url=\u0026#39;https://github.com/pypa/pip/issues/72\u0026#39;)) assert isinstance(item.tags, list) 同样，TextField也支持many参数。\nHow It Works? #  最终Item类会将输入最终转化为etree._Element对象进行处理，然后利用元类的思想将每一个Field构造的属性计算为源网页上对应的真实数据\n"});index.add({'id':11,'href':'/ruia/docs/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/4.Field/','title':"4. Field",'section':"基础概念",'content':"Selector #  Selector通过Field类实现，为开发者提供了CSS Selector和XPath两种方式提取目标数据，具体由下面两个类实现：\n AttrField(BaseField)：提取网页标签的属性数据 TextField(BaseField)：提取网页标签的text数据  Core arguments #  所有的Field共有的参数：\n default: str, 设置默认值，建议定义，否则找不到字段时会报错 many: bool, 返回值将是一个列表  AttrField、TextField、HtmlField共用参数：\n css_select：str, 利用CSS Selector提取目标数据 xpath_select：str, 利用XPath提取目标数据  AttrField需要一个额外的参数：\n attr：目标标签属性  RegexField需要一个额外的参数：\n re_select: str, 正则表达式字符串  Usage #  from lxml import etree from ruia import AttrField, TextField, HtmlField, RegexField HTML = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;ruia\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;¬ \u0026lt;p\u0026gt; \u0026lt;a class=\u0026#34;test_link\u0026#34; href=\u0026#34;https://github.com/howie6879/ruia\u0026#34;\u0026gt;hello github.\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; html = etree.HTML(HTML) def test_css_select(): field = TextField(css_select=\u0026#34;head title\u0026#34;) value = field.extract(html_etree=html) assert value == \u0026#34;ruia\u0026#34; def test_xpath_select(): field = TextField(xpath_select=\u0026#39;/html/head/title\u0026#39;) value = field.extract(html_etree=html) assert value == \u0026#34;ruia\u0026#34; def test_attr_field(): attr_field = AttrField(css_select=\u0026#34;p a.test_link\u0026#34;, attr=\u0026#39;href\u0026#39;) value = attr_field.extract(html_etree=html) assert value == \u0026#34;https://github.com/howie6879/ruia\u0026#34; def test_html_field(): field = HtmlField(css_select=\u0026#34;a.test_link\u0026#34;) assert field.extract(html_etree=html) == \u0026#39;\u0026lt;a class=\u0026#34;test_link\u0026#34; href=\u0026#34;https://github.com/howie6879/ruia\u0026#34;\u0026gt;hello github.\u0026lt;/a\u0026gt;\u0026#39; def test_re_field(): field = RegexField(re_select=\u0026#39;\u0026lt;title\u0026gt;(.*?)\u0026lt;/title\u0026gt;\u0026#39;) href = field.extract(html=HTML) assert href == \u0026#39;ruia\u0026#39; How It Works? #  定好CSS Selector或XPath规则，然后利用lxml实现对目标html进行目标数据的提取\n关于RegexField #  详细信息请参阅英文文档。\n"});index.add({'id':12,'href':'/ruia/docs/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/5.Spider/','title':"5. Spider",'section':"基础概念",'content':"Spider #  Spider是爬虫程序的入口，它将Item、Middleware、Request、等模块组合在一起，从而为你构造一个稳健的爬虫程序。你只需要关注以下两个函数：\n Spider.start：爬虫的启动函数 parse：爬虫的第一层解析函数，继承Spider的子类必须实现这个函数  Core arguments #  Spider.start的参数如下：\n after_start：爬虫启动后的钩子函数 before_stop：爬虫启动前的钩子函数 middleware：中间件类，可以是一个中间件Middleware()实例，也可以是一组Middleware()实例组成的列表 loop：事件循环  Usage #  import aiofiles from ruia import AttrField, TextField, Item, Spider class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) async def clean_title(self, value): return value class HackerNewsSpider(Spider): start_urls = [\u0026#39;https://news.ycombinator.com/news?p=1\u0026#39;, \u0026#39;https://news.ycombinator.com/news?p=2\u0026#39;] async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): yield item async def process_item(self, item: HackerNewsItem): \u0026#34;\u0026#34;\u0026#34;Ruia build-in method\u0026#34;\u0026#34;\u0026#34; async with aiofiles.open(\u0026#39;./hacker_news.txt\u0026#39;, \u0026#39;a\u0026#39;) as f: await f.write(str(item.title) + \u0026#39;\\n\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: HackerNewsSpider.start() How It Works? #  Spider会自动读取start_urls列表里面的请求链接，然后维护一个异步队列，使用生产消费者模式进行爬取，爬虫程序一直循环直到没有调用函数为止\n"});index.add({'id':13,'href':'/ruia/docs/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/6.Middleware/','title':"6. Middleware",'section':"基础概念",'content':"Middleware #  Middleware的主要作用是在进行一个请求的前后进行一些处理，比如监听请求或者响应：\n Middleware().request：在请求前处理一些事情 Middleware().response：在请求后处理一些事情  Usage #  使用中间件有两点需要注意，一个是处理函数需要带上特定的参数，第二个是不需要返回值，具体使用如下：\nfrom ruia import Middleware middleware = Middleware() @middleware.request async def print_on_request(spider_ins, request): \u0026#34;\u0026#34;\u0026#34; 每次请求前都会调用此函数 request: Request类的实例对象 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;request: print when a request is received\u0026#34;) @middleware.response async def print_on_response(spider_ins, request, response): \u0026#34;\u0026#34;\u0026#34; 每次请求后都会调用此函数 request: Request类的实例对象 response: Response类的实例对象 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;response: print when a response is received\u0026#34;) How It Works? #  Middleware通过装饰器来实现对函数的回调，从而让开发者可以优雅的实现中间件功能，Middleware类中的两个属性request_middleware和response_middleware分别维护着一个队列来处理开发者定义的处理函数\n"});index.add({'id':14,'href':'/ruia/docs/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/1.%E6%90%AD%E5%BB%BA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/','title':"1.搭建开发环境",'section':"开发指南",'content':"搭建开发环境 #  "});index.add({'id':15,'href':'/ruia/docs/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/2.%E6%B5%85%E8%B0%88-Ruia-%E6%9E%B6%E6%9E%84/','title':"2.浅谈 Ruia 架构",'section':"开发指南",'content':"浅谈Ruia架构 #  "});index.add({'id':16,'href':'/ruia/docs/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/3.%E4%B8%BA-Ruia-%E7%BC%96%E5%86%99%E6%8F%92%E4%BB%B6/','title':"3.为 Ruia 编写插件",'section':"开发指南",'content':"为Ruia编写插件 #  扩展的目的是将一些在爬虫程序中频繁使用的功能封装起来作为一个模块供第三方调用，Ruia通过Middleware来让开发者快速地实现第三方扩展\n前面一节已经说过，Middleware的目的是对每次请求前后进行一番处理，然后我们实现了一个功能，就是在请求头里面加入User-Agent\n可能任意一个爬虫都会需要自动添加随机User-Agent的功能，让我将这个功能封装下，使其成为Ruia的一个第三方扩展吧，让我们现在就开始吧\nCreating a project #  项目名称为：ruia-ua，因为Ruia基于Python3.6+，所以扩展ruia-ua也亦然，假设你此时使用的是Python3.6+，请按照如下操作：\n# 安装包管理工具 pipenv pip install pipenv # 创建项目文件夹 mkdir ruia-ua cd ruia-ua # 安装虚拟环境 pipenv install # 安装 ruia pipenv install ruia # 安装 aiofiles pipenv install aiofiles # 创建项目目录 mkdir ruia_ua cd ruia_ua # 实现代码放在这里 touch __init__.py\t目录结构如下：\nruia-ua ├── LICENSE\t# 开源协议 ├── Pipfile\t# pipenv 管理工具生成文件 ├── Pipfile.lock ├── README.md\t├── ruia_ua │ ├── __init__.py\t# 代码实现 │ └── user_agents.txt\t# 随机ua集合 └── setup.py\tFirst extension #  user_agents.txt文件包含了各种ua，接下来我们只要利用ruia的Middleware实现在每次请求前随机添加一个User-Agent即可，实现代码如下：\nimport os import random import aiofiles from ruia import Middleware __version__ = \u0026#34;0.0.1\u0026#34; async def get_random_user_agent() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Get a random user agent string. :return: Random user agent string. \u0026#34;\u0026#34;\u0026#34; USER_AGENT = \u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36\u0026#39; return random.choice(await _get_data(\u0026#39;./user_agents.txt\u0026#39;, USER_AGENT)) async def _get_data(filename: str, default: str) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34; Get data from all user_agents :param filename: filename :param default: default value :return: data \u0026#34;\u0026#34;\u0026#34; root_folder = os.path.dirname(__file__) user_agents_file = os.path.join(root_folder, filename) try: async with aiofiles.open(user_agents_file, mode=\u0026#39;r\u0026#39;) as f: data = [_.strip() for _ in await f.readlines()] except: data = [default] return data middleware = Middleware() @middleware.request async def add_random_ua(spider_ins, request): ua = await get_random_user_agent() if request.headers: request.headers.update({\u0026#39;User-Agent\u0026#39;: ua}) else: request.headers = { \u0026#39;User-Agent\u0026#39;: ua } 编写完成后，我们只需要将ruia-ua上传至社区，这样所有的ruia使用者都可以直接使用你编写的第三方扩展，多么美好的一件事\nUsage #  所有的爬虫程序都可以直接使用ruia-ua来实现自动添加User-Agent\npip install ruia-ua 举个实际使用的例子：\nfrom ruia import AttrField, TextField, Item, Spider from ruia_ua import middleware class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) async def clean_title(self, value): return value class HackerNewsSpider(Spider): start_urls = [\u0026#39;https://news.ycombinator.com/news?p=1\u0026#39;, \u0026#39;https://news.ycombinator.com/news?p=2\u0026#39;] concurrency = 10 async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): print(item.title) if __name__ == \u0026#39;__main__\u0026#39;: HackerNewsSpider.start(middleware=middleware) 第三方扩展的实现将会大大减少爬虫工程师的开发周期，ruia非常希望你可以开发并提交自己的第三方扩展\n"});index.add({'id':17,'href':'/ruia/docs/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/4.%E8%B4%A1%E7%8C%AE%E4%BB%A3%E7%A0%81/','title':"4.贡献代码",'section':"开发指南",'content':"贡献代码 #  "});index.add({'id':18,'href':'/ruia/docs/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/1.%E8%B0%88%E8%B0%88%E5%AF%B9-Python-%E7%88%AC%E8%99%AB%E7%9A%84%E7%90%86%E8%A7%A3/','title':"1.谈谈对 Python 爬虫的理解",'section':"实践指南",'content':"谈谈对Python爬虫的理解 #   爬虫也可以称为Python爬虫\n 不知从何时起，Python这门语言和爬虫就像一对恋人，二者如胶似漆 ，形影不离，你中有我、我中有你，一提起爬虫，就会想到Python，一说起Python，就会想到人工智能……和爬虫\n所以，一般说爬虫的时候，大部分程序员潜意识里都会联想为Python爬虫，为什么会这样，我觉得有两个原因：\n Python生态极其丰富，诸如Request、Beautiful Soup、Scrapy、PySpider等第三方库实在强大 Python语法简洁易上手，分分钟就能写出一个爬虫（有人吐槽Python慢，但是爬虫的瓶颈和语言关系不大）  任何一个学习Python的程序员，应该都或多或少地见过甚至研究过爬虫，我当时写Python的目的就非常纯粹——为了写爬虫。所以本文的目的很简单，就是说说我个人对Python爬虫的理解与实践，作为一名程序员，我觉得了解一下爬虫的相关知识对你只有好处，所以读完这篇文章后，如果能对你有帮助，那便再好不过\n什么是爬虫 #  爬虫是一个程序，这个程序的目的就是为了抓取万维网信息资源，比如你日常使用的谷歌等搜索引擎，搜索结果就全都依赖爬虫来定时获取\n看上述搜索结果，除了wiki相关介绍外，爬虫有关的搜索结果全都带上了Python，前人说Python爬虫，现在看来果然诚不欺我～\n爬虫的目标对象也很丰富，不论是文字、图片、视频，任何结构化非结构化的数据爬虫都可以爬取，爬虫经过发展，也衍生出了各种爬虫类型：\n 通用网络爬虫：爬取对象从一些种子 URL 扩充到整个 Web，搜索引擎干的就是这些事 垂直网络爬虫：针对特定领域主题进行爬取，比如专门爬取小说目录以及章节的垂直爬虫 增量网络爬虫：对已经抓取的网页进行实时更新 深层网络爬虫：爬取一些需要用户提交关键词才能获得的 Web 页面  不想说这些大方向的概念，让我们以一个获取网页内容为例，从爬虫技术本身出发，来说说网页爬虫，步骤如下：\n 模拟请求网页资源 从HTML提取目标元素 数据持久化  什么是爬虫，这就是爬虫：\n\u0026#34;\u0026#34;\u0026#34;让我们根据上面说的步骤来完成一个简单的爬虫程序\u0026#34;\u0026#34;\u0026#34; import requests from bs4 import BeautifulSoup target_url = \u0026#39;http://www.baidu.com/s?wd=爬虫\u0026#39; # 第一步 发起一个GET请求 res = requests.get(target_url) # 第二步 提取HTML并解析想获取的数据 比如获取 title soup = BeautifulSoup(res.text, \u0026#34;lxml\u0026#34;) # 输出 soup.title.text title = soup.title.text # 第三步 持久化 比如保存到本地 with open(\u0026#39;title.txt\u0026#39;, \u0026#39;w\u0026#39;) as fp: fp.write(title) 加上注释不到20行代码，你就完成了一个爬虫，简单吧\n怎么写爬虫 #  网页世界多姿多彩、亿万网页资源供你选择，面对不同的页面，怎么使自己编写的爬虫程序够稳健、持久，这是一个值得讨论的问题\n俗话说，磨刀不误砍柴工，在开始编写爬虫之前，很有必要掌握一些基本知识：\n 网页的结构是HTML，爬虫的目标就是解析HTML，获取目标字段并保存 客户端展现的网页由浏览器渲染，客户端和服务端的信息交互依靠HTTP协议  这两句描述体现了一名爬虫开发人员需要掌握的基本知识，不过一名基本的后端或者前端工程师都会这些哈哈，这也说明了爬虫的入门难度极低，从这两句话，你能思考出哪些爬虫必备的知识点呢？\n 基本的HTML知识，了解HTML才方便目标信息提取 基本的JS知识 ，JS可以异步加载HTML 了解CSS Selector、XPath以及正则，目的是为了提取数据 了解HTTP协议，为后面的反爬虫斗争打下基础 了解基本的数据库操作，为了数据持久化  有了这些知识储备，接下来就可以选择一门语言，开始编写自己的爬虫程序了，还是按照上一节说的三个步骤，然后以Python为例，说一说要在编程语言方面做那些准备：\n 网页请求：内置有urllib库，第三方库的话，同步请求可以使用requests，异步请求使用aiohttp 分析HTML结构并提取目标元素：CSS Selector和XPath是目前主流的提取方式，第三方库可以使用Beautiful Soup或者PyQuery 数据持久化：目标数据提取之后，可以将数据保存到数据库中进行持久化，MySQL、MongoDB等，这些都有对应的库支持，当然你也可以保存在硬盘，谁硬盘没点东西对吧（滑稽脸）  掌握了上面这些，你大可放开手脚大干一场，万维网就是你的名利场，去吧～\n我觉得对于一个目标网站的网页，可以分下面四个类型：\n 单页面单目标 单页面多目标 多页面单目标 多页面多目标  具体是什么意思呢，可能看起来有点绕，但明白这些，你之后写爬虫，只要在脑子里面过一遍着网页对应什么类型，然后套上对应类型的程序（写多了都应该有一套自己的常用代码库），那写爬虫的速度，自然不会慢\n 单页面单目标\n 通俗来说，就是在这个网页里面，我们的目标就只有一个，假设我们的需求是抓取这部 电影-肖申克的救赎 的名称，首先打开网页右键审查元素，找到电影名称对应的元素位置，如下图所示：\n在某个单一页面内，看目标是不是只有一个，一眼就能看出标题的CSS Selector规则为：#content \u0026gt; h1 \u0026gt; span:nth-child(1)，然后用我自己写的常用库，我用不到十行代码就能写完抓取这个页面电影名称的爬虫：\nimport asyncio from ruia import Item, TextField class DoubanItem(Item): \u0026#34;\u0026#34;\u0026#34; 定义爬虫的目标字段 \u0026#34;\u0026#34;\u0026#34; title = TextField(css_select=\u0026#39;#content \u0026gt; h1 \u0026gt; span:nth-child(1)\u0026#39;) async_func = DoubanItem.get_item(url=\u0026#34;https://movie.douban.com/subject/1292052/\u0026#34;) item = asyncio.get_event_loop().run_until_complete(async_func) print(item) 多页面多目标就是此情况下多个url的衍生情况\n 单页面多目标\n 假设现在的需求是抓取 豆瓣电影250 第一页中的所有电影名称，你需要提取25个电影名称，因为这个目标页的目标数据是多个item的，所以目标需要循环获取。\n对于这个情况，我在Item中限制了一点，当你定义的爬虫需要在某一页面循环获取你的目标时，则需要定义target_item属性（就是截图中的红框）\n对于豆瓣250这个页面，我们的目标是25部电影信息，所以该这样定义：\n   field css_select     target_item（必须） div.item   title span.title    代码实现如下：\nimport asyncio from ruia import Item, TextField class DoubanItem(Item): \u0026#34;\u0026#34;\u0026#34; 定义爬虫的目标字段 \u0026#34;\u0026#34;\u0026#34; target_item = TextField(css_select=\u0026#34;div.item\u0026#34;) title = TextField(css_select=\u0026#34;span.title\u0026#34;) async def clean_title(self, title): \u0026#34;\u0026#34;\u0026#34; 对提取的目标数据进行清洗 可选 :param title: 初步提取的目标数据 :return: \u0026#34;\u0026#34;\u0026#34; if isinstance(title, str): return title else: return \u0026#34;\u0026#34;.join([i.text.strip().replace(\u0026#34;\\xa0\u0026#34;, \u0026#34;\u0026#34;) for i in title]) async def run_item(url: str): async for item in DoubanItem.get_items(url=url): print(item) items = asyncio.get_event_loop().run_until_complete( run_item(\u0026#34;https://movie.douban.com/top250\u0026#34;) )  多页面多目标\n 多页面多目标是上述单页面多目标情况的衍生，在这个问题上来看，此时就是获取所有分页的电影名称\nfrom ruia import Item, Request, Spider, TextField class DoubanItem(Item): \u0026#34;\u0026#34;\u0026#34; 定义爬虫的目标字段 \u0026#34;\u0026#34;\u0026#34; target_item = TextField(css_select=\u0026#34;div.item\u0026#34;) title = TextField(css_select=\u0026#34;span.title\u0026#34;) async def clean_title(self, title): if isinstance(title, str): return title else: return \u0026#34;\u0026#34;.join([i.text.strip().replace(\u0026#34;\\xa0\u0026#34;, \u0026#34;\u0026#34;) for i in title]) class DoubanSpider(Spider): start_urls = [\u0026#34;https://movie.douban.com/top250\u0026#34;] concurrency = 10 async def parse(self, res): etree = res.html_etree(html=await res.text()) pages = [\u0026#34;?start=0\u0026amp;filter=\u0026#34;] + [ i.get(\u0026#34;href\u0026#34;) for i in etree.cssselect(\u0026#34;.paginator\u0026gt;a\u0026#34;) ] for index, page in enumerate(pages): url = self.start_urls[0] + page yield Request( url, callback=self.parse_item, metadata={\u0026#34;index\u0026#34;: index}, request_config=self.request_config, ) async def parse_item(self, res): res_list = [] async for item in DoubanItem.get_items(html=await res.text()): res_list.append(item.title) return res_list if __name__ == \u0026#34;__main__\u0026#34;: DoubanSpider.start() 如果网络没问题的话，会得到如下输出：\n注意爬虫运行时间，1s不到，这就是异步的魅力\n用Python写爬虫，就是这么简单优雅，诸位，看着网页就思考下：\n 是什么类型的目标类型 用什么库模拟请求 怎么解析目标字段 怎么存储  一个爬虫程序就成型了，顺便一提，爬虫这东西，可以说是防君子不防小人，robots.txt大部分网站都有（它的目的是告诉爬虫什么可以爬取什么不可以爬取，比如：https://www.baidu.com/robots.txt），各位想怎么爬取，自己衡量\n如何进阶 #  不要以为写好一个爬虫程序就可以出师了，此时还有更多的问题在前面等着你，你要含情脉脉地看着你的爬虫程序，问自己三个问题：\n 爬虫抓取数据后是正当用途么？ 爬虫会把目标网站干掉么？ 爬虫会被反爬虫干掉么？  前两个关于人性的问题在此不做过多叙述，因此跳过，但你们如果作为爬虫工程师的话，切不可跳过\n 会被反爬虫干掉么？\n 最后关于反爬虫的问题才是你爬虫程序强壮与否的关键因素，什么是反爬虫？\n当越来越多的爬虫在互联网上横冲直撞后，网页资源维护者为了防止自身数据被抓取，开始进行一系列的措施来使得自身数据不易被别的程序爬取，这些措施就是反爬虫\n比如检测IP访问频率、资源访问速度、链接是否带有关键参数、验证码检测机器人、ajax混淆、js加密等等\n对于目前市场上的反爬虫，爬虫工程师常有的反反爬虫方案是下面这样的：\n 不断试探目标底线，试出单IP下最优的访问频率 构建自己的IP代理池 维护一份自己常用的UA库 针对目标网页的Cookie池 需要JS渲染的网页使用无头浏览器进行代码渲染再抓取 一套破解验证码程序 扎实的JS知识来破解混淆函数  爬虫工程师的进阶之路其实就是不断反反爬虫，可谓艰辛，但换个角度想也是乐趣所在\n 关于框架\n 爬虫有自己的编写流程和标准，有了标准，自然就有了框架，像Python这种生态强大的语言，框架自然是多不胜数，目前世面上用的比较多的有：\n Scrapy PySpider Portia Ruia：容我自荐一波  这里不过多介绍，框架只是工具，是一种提升效率的方式，看你选择\n说明 #  任何事物都有两面性，爬虫自然也不例外，因此我送诸位一张图，关键时刻好好想想\n 最后，欢迎一起交流：\n "});})();