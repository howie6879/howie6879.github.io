<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习之路</title>
    <link>https://www.howie6879.cn/ml_book/</link>
    <description>Recent content on 机器学习之路</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://www.howie6879.cn/ml_book/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/01_basic/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/01_basic/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</guid>
      <description>神经网络基础 #  要想入门以及往下理解深度学习，其中一些概念可能是无法避免地需要你理解一番，比如：
 什么是感知器 什么是神经网络 张量以及运算 微分 梯度下降  带着问题出发 #  在开始之前希望你有一点机器学习方面的知识，解决问题的前提是提出问题，我们提出这样一个问题，对MNIST数据集进行分析，然后在解决问题的过程中一步一步地来捋清楚其中涉及到的概念
MNIST数据集是一份手写字训练集，出自MNIST，相信你对它不会陌生，它是机器学习领域的一个经典数据集，感觉任意一个教程都拿它来说事，不过这也侧面证明了这个数据集的经典，这里简单介绍一下：
 拥有60,000个示例的训练集，以及10,000个示例的测试集 图片都由一个28 ×28 的矩阵表示，每张图片都由一个784 维的向量表示 图片分为10类， 分别对应从0～9，共10个阿拉伯数字  压缩包内容如下：
 train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)  上图：
图片生成代码如下：
%matplotlib inline import matplotlib import matplotlib.pyplot as plt import numpy as np from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/01.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/01.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/</guid>
      <description>识别手写字 #  Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。
神经网络是一门重要的机器学习技术，它通过模拟人脑的神经网络来实现人工智能的目的，所以其也是深度学习的基础，了解它之后自然会受益颇多，本章主要是以识别手写字这个问题来贯穿整篇，那么，人类的视觉系统和神经网络到底在识别一个目标的时候，主要区别在哪？
 人类视觉系统：通过数十亿年不断地进化与学习，最终能够极好地适应理解视觉世界的任务，从而无意识地就可以对目标进行判断识别 神经网络：通过提供的样本来推断出识别某种目标的规则，作为判断标准  本章的主要内容是介绍神经网络的基本概念以及引入一个识别手写数字的例子来加深我们的理解，你将了解到：
 两个重要的人工神经元：感知器和S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 标准的神经网络学习算法：随机梯度下降算法  感知器 #  1943年，心理学家McCulloch和数学家Pitts发表了《A logical calculus of the ideas immanent in nervous activity》，其中提出了抽象的神经元模型MP，但是在这个模型中权重都是要求提前设置好才可以输出目标值，所以很遗憾，它不可以学习，但这不影响此模型给后来者带来的影响，比如感知器：
 感知器是Frank Rosenblatt提出的一个由两层神经元组成的人工神经网络，它的出现在当时可是引起了轰动，因为感知器是首个可以学习的神经网络
 感知器的工作方式如下所示：
$x_{1},x_{2},x_{3}$ 分别表示三个不同的二进制输入，output则是一个二进制输出，对于多种输入，可能有的输入成立有的不成立，在这么多输入的影响下，该如何判断输出output呢？Rosenblatt引入了权重来表示相应输入的重要性。
对于$x_{1},x_{2},&amp;hellip;,x_{j}$个输入，每个输入都有对应的权重$w_{1},w_{2},&amp;hellip;,w_{j}$，最后的输出output由每个输入与其对应的权重相乘之和与阈值之差$\sum _{j} w{_j}x{_j}$来决定，如下：
假设$b=-threshold$且$w$和$x$对应权重和输⼊的向量，即：
 $x=(x_{1},x_{2},&amp;hellip;,x_{j})$ $w=(w_{1},w_{2},&amp;hellip;,w_{j})$  那么感知器的规则可以重写为:
这就是感知器的数学模型，是不是就像一个逻辑回归模型？只要将感知器输出规则替换为($f(x)=x$)，后面我们会知道这称之为激活函数，其实这种感知器叫做线性单元。
它的出现让我们可以设计学习算法，从而实现自动调整人工神经元的权重和偏置，与此同时output也会随之改变，这就是学习！如果你有兴趣可以看我用python写的一个感知器自动学习实现与非门，代码在nndl_chapter01。
说句题外话，由于感知器是单层神经网络，它只能实现简单的线性分类任务，所以它无法对异或问题进行分类，异或的真值表如下：
   $x$ $y$ $output$     0 0 0   0 1 1   1 0 1   1 1 0    可以看出来，异或问题是线性不可分的，不信你画个坐标轴试试看，那么问题来了？怎么解决，大部分都能很快地想出解决方案，既然感知器可以实现线性分类，也就是说实现与非门是没有问题的，逻辑上来说我们可以实现任何逻辑功能（比如四个与非门实现异或），但前提是为感知器加入一个隐藏层，意思就是多了一个隐藏层的神经网络之后，就可以解决异或问题。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/</guid>
      <description>反向传播算法如何工作 #  前面一章，我们通过了梯度下降算法实现目标函数的最小化，从而学习了该神经网络的权重和偏置，但是有一个问题并没有考虑到，那就是如何计算代价函数的梯度，本章的重点就是介绍计算这些梯度的快速算法——反向传播算法，首先先介绍一下上图中以及下面文章中会出现的一些数学符号：
 $L$ : 表示网络层数 $b_j^l$ : 表示第$l$层的第$j$个神经元的偏置 $a_j^l$ : 表示第$l$层的第$j$个神经元的激活值 $w_{j k}^{l}$ : 表示从$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接上的权重 $w^l$ : 权重矩阵，其中元素表示$l-1$层连接到$l$层神经元的权重 $b^l$ : 第$l$层神经元的偏置向量 $z^l$ : 第$l$层神经元的带权输入向量 $a^l$ : 第$l$层每个神经元激活值构成的向量 $\delta_{j}^{l}$ : 第$l$层第$j$个神经元的**误差**  本篇文章是公式重灾区，但是涉及的知识并不高级，这也说明一个道理：
 很多看似显而易见的想法只有在事后才变得显而易见。
 热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 #  通过第一章，我们已经知道每个神经元的激活值的计算方法，根据上面的公式，我们可以得出$a_j^l$的表达方式：
$$ a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right) $$
举个例子：$a_3^2$表示第二层的第三个神经元的激活值，那么该输出值怎么同上一层的输出值以及权重关联起来的呢，根据激活值的计算公式，我们可以得出：
$$ \begin{aligned} a_3^2 &amp;amp;= \sigma(w_{3 1}^{2} a_1^1 + w_{3 2}^{2} a_2^1+ w_{3 3}^{2} a_3^1 + b_3^2 ) \ &amp;amp;= \sigma\left(\sum_{k=1}^3 w_{3 k}^2 a_{k}^1 + b_3^2 \right) \end{aligned} $$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/03.%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/03.%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%B3%95/</guid>
      <description>改进神经⽹络的学习⽅法 #   万丈高楼平地起，反向传播是深度学习这栋大厦的基石，所以在这块花多少时间都是值得的
 前面一章，我们深入理解了反向传播算法如何工作，本章的主要目的是改进神经网络的学习方法，本章涉及的技术包括：
 交叉熵代价函数 四种称为规范化的⽅法（L1 和 L2 规范化，弃权和训练数据的⼈为扩展） 更好的权重初始化⽅法 如何选择神经网络的超参数  交叉熵代价函数 #  通过前面的学习，我们知道，神经网络一直在努力地让代价函数变小，现在我们抛出一个问题，代价函数从初始值到我们理想状态的值，这个过程所消耗的时间是怎样的情况呢？
对于人类来说，我们犯比较明显的错误的时候，改正的效果也很明显，神经网络也是这样的么？让我们通过下面的一个小例子来详细看看：
假设有这样一个神经元，我们希望当输入为1的时候，输出为0，让我们通过梯度下降的方式来训练权重和偏置，⾸先将权重和偏置初始化为0.6 和 0.9，前面我们说过，S型神经元的输出计算方式为：
输⼊$x_{1},x_{2},&amp;hellip;,x_{j}$，权重为$w_{1},w_{2},&amp;hellip;,w_{j}$，和偏置b的S型神经元的输出是：
$$ \frac{1}{1+\exp(-\sum_j w_j x_j-b)} $$
将0.6 和 0.9带入，即可得到输出为：0.82，很显然和目标0相差甚远，那么就只能继续寻找最优解了，随着迭代期的增加，神经元的输出、权重、偏置和代价的变化如下⾯⼀系列图形所⽰：
可以看到，200次迭代后，输出已经满足我们的需求了，毕竟只要小于0.5我们都可以归为0，接下来再将初始权重和偏置都设置为2.0，此时初始输出为0.98，这是和⽬标值的差距相当⼤的，现在看看神经元学习的过程：
这次可以明显地看到迭代150次左右没有很明显的变化，随后速度就快了起来。
这个例子引出了一个问题，包括在其他一般的神经网络中也会出现，为何会学习缓慢呢？我们在训练中应该怎么避免这个情况？思考一下，问题的原因其实很简单
 权重和偏置是根据梯度下降的形式来更新其对应的值，学习缓慢就意味着代价函数的偏导数很小
 让我们从数学公式的角度来理解这句话和上面的图，对于目前提供的单个样例的输入和输出：
 $x=1$ $y=0$  再结合前面第一章讲的代价函数的定义：
$$ C(w,b) \equiv \frac{1}{2n} \sum_x | y(x) - a|^2 $$
 w 表⽰所有的⽹络中权重的集合 b 是所有的偏置 n 是训练输⼊数据的个数 a 是表⽰当输⼊为x时输出的向量，可以理解为output  类比一下，此时的代价函数为：
$$ C=\frac{(y-a)^{2}}{2} $$
接下来就是求权重和偏置的偏导数，推导之前说明一下前面的公式：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/04.%E7%A5%9E%E7%BB%8F%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/04.%E7%A5%9E%E7%BB%8F%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/</guid>
      <description>神经⽹络可以计算任何函数的可视化证明 #  本章其实和前面章节的关联性不大，所以大可将本章作为小短文来阅读，当然基本的深度学习基础还是要有的。
主要介绍了神经⽹络拥有的⼀种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入$x$，其值$f(x)$（或者说近似值）是网络的输出，哪怕是多输入和多输出也是如此，我们大可直接得出一个结论：
 不论我们想要计算什么样的函数，我们都确信存在⼀个神经⽹络（多层）可以计算它
 试想以下这种普遍性代表着什么，我觉得代表着做更多可能的事情（将其看做计算一种函数）：
 比如将中文翻译成英文 比如根据⼀个mp4视频⽂件⽣成⼀个描述电影情节并讨论表演质量的问题 &amp;hellip;  现实往往是残酷的，我们知道有这个网络存在，比如中文翻译成英文的网络，通常情况下想得往往不可得，网络在那里，但更可能我们得不到，怎么办？
前面我们知道，我们通过学习算法来拟合函数，学习算法和普遍性的结合是⼀种有趣的混合，直到现在，本书⼀直是着重谈学习算法，到了本章，我们来看看普遍性，看看它究竟意味着什么。
两个预先声明 #  在解释为何普遍性定理成⽴前，关于神经⽹络可以计算任何函数有两个预先声明需要注意一下：
 这句话不是说⼀个⽹络可以被⽤来准确地计算任何函数，而是说，我们可以获得尽可能好的⼀个近似，通过增加隐藏元的数量，我们可以提升近似的精度，同时对于目标精度，我们需要确定精度范围：$|g(x)-f(x)|&amp;lt;\epsilon$，其中$\epsilon&amp;gt;0$ 按照上⾯的⽅式近似的函数类其实是连续函数，如果函数不是连续的，也就是会有突然、极陡的跳跃，那么⼀般来说⽆法使⽤⼀个神经⽹络进⾏近似，这并不意外，因为神经⽹络计算的就是输⼊的连续函数   普遍性定理的表述：包含⼀个隐藏层的神经⽹络可以被⽤来按照任意给定的精度来近似任何连续函数
 接下来的内容会使⽤有两个隐藏层的⽹络来证明这个结果的弱化版本，在问题中会简要介绍如何通过⼀些微调把这个解释适应于只使⽤⼀个隐藏层的⽹络并给出证明。
一个输入和一个输出的普遍性 #  先从一个简单的函数$f(x)$（即只有一个输入和一个输出）开始，我们将利用神经网络来近似这个连续函数：
第一章我们就探讨过多层感知机实现异或，这次同样的，我们加入一个隐藏层就可以让函数舞动起来，比如下面这个有一个隐藏层、两个隐藏神经元的网络：
第一步，暂时只考虑顶层的神经元，第一章也讲过S型神经元，所以输出范围类似上图右上角，重点看看这个S型函数，前面已经说过：
$$ \sigma(z) \equiv 1 /\left(1+e^{-z}\right) $$
其中：$z=wx+b$，参见右上角的图，让我们考虑一下几个情况：
 当$x$不变，$b$逐渐增加的情况下，输出会在原来的基础上变大，图像会相对向左边运动，因为$w$没变，所以图像形状不会变   上述情况让$b$键减小，图像会右移，同样图像形状不变 当$b$不变，$w$减小，很显然，图像的陡峭程度会下降，反之亦然  下图是书中给出的图示：
其实我们完全可以自己绘制这个过程，利用Python的matplotlib可以很好地完成这个事情：
import matplotlib.pyplot as plt import numpy as np def sigmoid(w, b, x): return 1.0 / (1.0 + np.exp(-(w * x + b))) def plot_sigmoid(w, b): x = np.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/05.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/05.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83/</guid>
      <description>深度神经⽹络为何很难训练 #  上一章提到了神经网络的一种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入得到一个近似的输出。
 普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许多现实世界问题的函数
 而且理论上我们只要一个隐藏层就可以计算任何函数，第一章我们就用如下的网络结构完成了一个手写字识别的模型：
这时候，大家心中可能都会有这样一个想法，如果加大网络的深度，模型的识别准确率是否会提高？
随即我们会基于反向传播的随机梯度下降来训练神经网络，但实际上这会产生一些问题，因为我们的深度神经网络并没有比浅层网络好很多。那么此处就引出了一个问题，为什么深度神经网络相对训练困难？
仔细研究会发现：
  如果网络后面层学习状况好的时候，前面的层次经常会在训练时停滞不前
  反之情况也会发生，前面层训练良好，后面停滞不前
  实际上，我们发现在深度神经⽹络中使⽤基于梯度下降的学习⽅法本⾝存在着内在不稳定性，这种不稳定性使得先前或者后⾯神经网络层的学习过程阻滞。
消失的梯度问题 #  我们在第一章识别手写字曾经以MNIST数字分类问题做过示例，接下来我们同样通过这个样例来看看我们的神经网络在训练过程中究竟哪里出了问题。
简单回顾一下，之前我们的训练样本路径为/pylab/datasets/mnist.pkl.gz ，相关案例代码在pylab都可以找到（我稍做了改动以支持Python3）。
import mnist_loader import network2 training_data, validation_data, test_data = mnist_loader.load_data_wrapper() # 输入层 784 # 隐藏层 30 # 输出层 10 sizes = [784, 30, 10] net = network2.Network(sizes=sizes) # 随机梯度下降开始训练 net.SGD( training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True, ) &amp;#34;&amp;#34;&amp;#34; Epoch 0 training complete Accuracy on evaluation data: 9280 / 10000 Epoch 1 training complete Accuracy on evaluation data: 9391 / 10000 .</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/06.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/06.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</guid>
      <description>深度学习 #  基于第一章节的学习，我们知道单层神经网络无法处理异或问题；如果需要计算多层神经网络，当时的计算机又缺少足够的计算能力，因此60年代末神经网络第一次进入了寒冬。
随后80年代，方向传播算法的出现解决了多层神经网络所需要的复杂计算问题（这也意味着异或问题的解决），但是由于数据集量级的影响，神经网络的能力被过拟合问题限制住了脚本，同时当时的SVM如日中天并且拥有极强的解释性，于是神经有一次陷入风雪。
摩尔定律一直在发挥着作用，终于到了2006年，Hinton在Science上提出了深度置信网络的神经网络模型，这是神经网络重回巅峰的代表性事件。随后发生的事情大家都有所耳闻：
 2012年ImageNet竞赛夺冠 2017年AlphaGo和柯洁的围棋对决 各种深度神经网络的商业化实践&amp;hellip;  现在，我们正处于第三次神经网络的复兴潮流中央。经过前面几章的探索，我们已经了解了深度神经⽹络通常⽐浅层神经⽹络更加难以训练，但是为了获得比浅层网络更加强大的能力，我们需要训练深度网络，本章主要讨论可以⽤来训练深度神经⽹络的技术，并在实战中应⽤它们。
本章是一个大章节，主要围绕的是从简单的浅层网络到卷积神经网络来慢慢构建强大的网络，从而解决MNIST手写字识别的问题。
在这个过程中，我们需要研究的相关技术有：卷积、池化、使用GPU来更好地训练、训练数据的算法性拓展、Dropout、网络的综合使用等。
介绍卷积网络 #  在第一章识别手写字中，我们就利用全连接神经网络基于MNIST训练集构建了一个可以识别手写字的深度学习模型。
让我们基于上面的网络结构思考下面几个问题：
 参数过多：对于手写字，第一层的输入图像大小是$28*28=784$个，因此第一个隐藏层的每个神经元到输入层都有784个权重参数，大家可以基于此计算一下，五层的全连接神经网络，参数将极其庞大会导致训练效率不高并且容易出现过拟合； 局表示不变性特征：目前的网络并没有考虑图像的空间结构，它在完全相同的基础上去对待相距很远和彼此接近的输⼊像素；自然图像中的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息．而全连接前馈网络很难提取这些 局部不变性特征，一般需要进行数据增强来提高性能。  卷积神经网络（Convolutional Neural Network，CNN 或 ConvNet）是一类特殊的人工神经网络，也是第一章提到的多层感知器（MLP）的变种。
我们可以从生物学角度出发，来看看卷积神经网络有哪些基本特征。我们知道视觉皮层的细胞存在 一个复杂的构造，这些细胞对视觉输入空间的子区域非常敏感，我们称之为感受野。在全连接网络中我们会将所有的输入神经元都连接到下一个隐藏的神经元，但这次我们不这么干，我们选择一个$5*5$的区域进行局部区域的连接：
这个输⼊图像的区域被称为隐藏神经元的局部感受野，然后在整个输⼊图像上交叉移动局部感受野，此时我们的$2828=784$个输入，用$55$的局部感受野，此时第一个隐藏层就会有$24*24$个神经元：
参考 #   Neural Networks and Deep Learning Neural Networks and Deep Learning 中文版 神经网络与深度学习-邱锡鹏 解析卷积神经网络  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/03_lihang/01.%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/03_lihang/01.%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/</guid>
      <description>统计学习方法概述 #  第一章主要对全书内容做了一个内容的概括：
 统计学习：定义、研究对象和方法 监督学习 统计学习三要素：模型、策略、算法 模型评估与选择：包括正则化、交叉验证与学习的泛化能力 生成模型与判别模型 分类问题、标注问题与回归问题  统计学习 #  什么是统计学习
统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，统计学习也称为统计机器学习
统计学习的特点
 统计学习以计算机及网络为平台，是建立在计算机及 网络之上的 统计学习以数据为研究对象，是数据驱动的学科 统计学习的目的是对数据进行预测与分析 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论  什么是机器学习
如果一个程序可以在任务T上，随着经验E的增加，效果P也可以随之增加，则称这个程序可以从经验中学习。 &amp;mdash; 卡内基美隆大学的Tom Michael Mitchell教授
 如果以垃圾邮件为例，一个程序指的是用到的机器学习算法，比如：朴素贝叶斯、逻辑回归；任务T指的是区分垃圾邮件的任务；经验E为已经区分过是否为垃圾邮件的历史邮件；效果P为机器学习算法在区分是否为垃圾邮件任务上的准确率
 统计学习的目的
统计学习用于对数据进行预测与分析，特别是对未知新数据进行预测与分析
统计学习的方法
 监督学习（supervised learning）：KNN、决策树、贝叶斯、逻辑回归 非监督学习（unsupervised learning）：聚类、降维 半监督学习（semisupervised learning）：self-training（自训练算法）、Graph-based Semi-supervised Learning（基于图的半监督算法）、Semi-supervised supported vector machine（半监督支持向量机，S3VM） 强化学习（reinforcement learning）：蒙特卡洛方法  监督学习 #  基本概念 #   输入空间（input space）：输入所有可能取值的集合 输出空间（output space）：输出所有可能取值的集合 特征空间（feature space）：每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。这时，所有特征向量存在的空间称为特征空间 联合概率分布：统计学习假设数据存在一定的统计规律，X和Y具有联合概率分布的假设就是监督学习关于数据的基本假设 - 机器学习-联合概率分布笔记 假设空间：学习的目的在于找到最好的模型，模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）  人们根据输入、输出变量的不同类型，对预测任务给予不同的名称：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/03_lihang/02.%E6%84%9F%E7%9F%A5%E6%9C%BA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/03_lihang/02.%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid>
      <description>感知机 #  本章主要介绍了二类分类的线性分类模型：感知机：
 感知机模型 感知机学习策略 感知机学习算法  说明：个人感觉这本书偏理论化，讲究的是一招定天下，好处是内功深厚自然无敌，一通百通，但难处是语言有点晦涩，这章可以考虑结合我之前的一篇关于感知器的笔记，或许能加深理解，见这里
感知机模型 #  感知机（perceptron）：是一个二类分类的线性判断模型，其输入为实例的特征向量，输出为实例的类别，取+1和–1值，属于判别模型
注：+1 -1 分别代表正负类，有的可能用 1 0 表示
在介绍感知机定义之前，下面几个概念需要说明一下：
 输入空间：输入所有可能取值的集合 输出空间：输出所有可能取值的集合 特征空间：每个具体的输入是一个实例，由特征向量表示  所以对于一个感知机模型，可以这样表示：
 输入空间（特征空间）：$\chi \subseteq \mathbb{R} ^n$ 输出空间：$\gamma = \{+1,-1 \}$  那么感知机就是由输入空间到输出空间的函数：
$$\displaystyle f( x) \ =\ sign( w\cdot x+b)$$
其中：
 $sign$: 符号函数 $w$: 权值（weight）或权值向量（weight vector） $b$: 偏置（bias）  感知机的几何解释如下：线性方程
$$w\cdot x + b =0$$
如果是二维空间，感知机就是一个线性函数，将正负样本一分为二，如何是三维空间，那么感知机就是一个平面将类别一切为二，上升到n维空间的话，其对应的是特征空间$\mathbb{R} ^n$的一个超平面$S$：
 $w$: 超平面的法向量 $b$: 超平面的截距  感知机学习策略 #  数据集的线性可分性 #  什么是数据集的线性可分性，很简单，对于一个数据集：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/04_appendix/00.%E4%B8%80%E7%AB%99%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%91%E7%A0%94%E5%8F%91%E5%B9%B3%E5%8F%B0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/04_appendix/00.%E4%B8%80%E7%AB%99%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%91%E7%A0%94%E5%8F%91%E5%B9%B3%E5%8F%B0/</guid>
      <description>一站式机器学习云开发平台 #   本篇是关于自身在机器学习这块工作经验的思考总结
 我希望构建一个机器学习云开发平台，目标在于解决以下问题：
 团队协作：项目管理，技术&amp;amp;业务的共享如何体现在实际解决问题的过程中； 资源调度：数据处理、模型训练； 模块共享：低代码甚至无代码； 快速开发：快速试错、实践、测试、部署； 需求-&amp;gt;开发的闭环  背景 #  自17年毕业以来，我从事于游戏行业的风控领域，主要涉及的系统的是风控和画像这两块。尽管我从团队初始建立的时候就定义了各种服务模板以及脚手架，但在数据处理、模型构建、管理这块我并没有着重去管控，仅仅是设定了一些服务的标准（初始阶段需要的是快速响应需求），如今随着业务场景和团队成员的增加，有如下问题需要考虑：
 脚本模块共享问题，团队成员长期开发积累的技术脚本、模型等是否可以模块化用于给其他成员共用； 不同数据及不同模型快速试验的的效率问题，我理想的解决方案是类似工作流那样自由组合脚本，从数据提取到模型构建再到模型管控以作业流的形式来完成（这也意味着只要公有模块覆盖面足够广，就可以进行组件拖拽式开发）； 模型测试管理问题，每个模型有哪些版本？怎么快速测试？分别被哪些服务使用以及如何快速上线； 资源调度问题，公司的数据存于云商，本地开发涉及的资源问题如何解决。  一站式机器学习云研发平台 #  目标 #  上面提到的问题可能也和大数据计算领域有些交集，毕竟都涉及到了 ETL，这些问题实际上涵盖了一个数据模型从开发到上线完整的生命周期。对于上述的需求，其实我们可以分别拆开来看看会衍生出怎样的目标：
 第1、2两点在我看来可以归纳为一个问题，模块需要抽象，然后让开发者可以自由地从模块仓库选取自己需要的模块进行组合，最终形成一个从数据提取到模型构建再到模型管控的工作流；一个项目可以有多个工作流，工作流的最小元素是模块，工作流的运行参数可以自由定义，开发者可以通过调整参数来快速调优；模块（函数）即服务，组合完毕就意味着一个需求的实现； 第3点的实现离不开两个基础服务，一是模型自动API服务化，二是模型管控服务； 第4点可以从两个方向入手，引入资源管理系统，如k8s或者云商的资源管理服务。  最终目标呼之欲出了，总结下我的需求，我需要的是一个满足数据计算、训练、管理的一站式机器学习云研发平台；而这个平台具有管理一个模型生命周期且形成闭环的功能，核心功能如：
 数据的访问与计算是无限制的； 模块即服务且可多用户互相共享； 一个需求的解决方案是模块的自由组合，需求的最终产物是模型（也可以是数据）； 对于模型可配置、易管控，可自动API服务化且可快速测试上线。  对于一站式机器学习云研发平台，我们需要其具有怎样的功能已经描述的差不多，既然问题已经抛了出来，接下来就说说怎么解决。
一切事情都有很多种解决方案，随着云原生的普及，容器技术的引入，中台在国内企业的覆盖率越来越广，该有的技术和业务的基础设施都开始有了一定的积累，怎么利用这些基础设施来进一步提升开发效率是接下来的一个方向，我期望最终解决方案有如下两个特性：
  低代码（高层次人员无代码）：基于现有的基础设施（脚手架、模型代码库、模板）可以根据通用模块结合个性化配置形成工作流就可以构建一个机器学习应用，工作流为核心的低代码、甚至无代码工作方式是我的期望，那么代码谁来写？这又是一个值得深入探讨的问题，目前暂不探讨，现在就假设有底层程序员在持续奋斗；
  云开发：因为涉及到资源的调度（数据计算、模型训练等），所以一个模型从数据输入到模型输出的开发流程都可以在云端完成（特别是数据处理和模型训练），浏览器在手，天下我有。
  就目前垂直的机器学习数据科学领域，再结合最新的技术方向，我个人很看好这两块，我认为低代码、云开发是构建新一代云原生应用的新式武器！（如果一直往前走，容器技术暂且不谈，单单微服务架构是不适用于这种原生应用的构建了，目前有大佬在研究的云研发架构或许是个方向。）
流程 #  先说说低代码，这里的低代码表达的是我们实现一个模型需求的方式，也就是说我们用低代码的方式来快速实现需求。
前面提到的模块即服务，通过连接各个模块最终形成一个工作流就是一个友好的方式，这也是市面上大部分产品的实现方式；每个模块的展现形式应该是友好可定义的，然后根据输入参数的不同来展现不同的行为（比如通过DSL定义，再基于DSL构建交互式的界面进行快速低代码开发）。
前面我们强调了闭环，那么在满足低代码这种特性下，我们的开发流程是怎样的呢？
 需求分析 方案确立（文档&amp;amp;会议讨论） 组件选择（通过DSL定义） 在机器学习云平台勾选组件、进行DSL配置，形成工作流（工作流的输出就是解决方案的核心） 验证&amp;amp;验收&amp;amp;部署（业务介入）  其实和传统方式一样，低代码的核心点也要求从需求分析到部署可以形成一个独立的闭环，不过在中间开发、验收、部署的过程都尽量实现低代码。低代码带来的效率提升是肉眼可见的，而实现低代码的关键点在于技术与业务的抽象，这里的表现为模块[函数]即服务；通过组合各个函数，就可以很方便地形成一个端到端的工作流以作为对某个需求的解决方案。
再来说说云开发，在机器学习领域，计算资源是一个不可忽视的问题，特别是在大数据计算清洗以及模型训练这块，目前基本上绕不过几大云商。我的想法是数据在哪，我们的开发环境就可以在哪，结合k8s，我们可以轻松建立团队的云研发环境，比如：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/04_appendix/01.%E8%AF%91%E5%A6%82%E4%BD%95%E7%94%A8python%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/04_appendix/01.%E8%AF%91%E5%A6%82%E4%BD%95%E7%94%A8python%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>如何用Python创建一个简单的神经网络 #   原文地址：How to Create a Simple Neural Network in Python 作者：Dr. Michael J. Garbade 翻译：howie6879   理解神经网络如何工作的最好方式是自己动手创建一个，这篇文章将会给你演示怎么做到这一点
 神经网络(NN)，也称之为人工神经网络(ANN)，它是机器学习领域中学习算法集合中的子集，其核心概念略似生物神经网络的概念。
拥有五年以上经验的德国机器学习专家Andrey Bulezyuk说过：神经网络正在使机器学习发生革命性的巨变，因为他们能够跨越广泛的学科和行业来高效地建模复杂的抽象。
基本上，一个ANN由以下组件构成：
 输入层：接受传递数据 隐藏层 输出层 各层之间的权重 每个隐藏层都会有一个精心设计的激活函数，对于此教程，我们将会使用Sigmoid激活函数  神经网络的类型有很多，在这个项目中，我们准备创建一个前馈神经网络，此类型的ANN将会直接从前到后传递数据
训练前馈神经元往往需要反向传播，反向传播为神经网络提供了相应的输入和输出集合，输入数据在被传递到神经元的时候被处理，然后产生一个输出
下面展示了一个简单的神经网络结构图：
而且，理解神经网络如何工作做好的办法就是去学习从头开始构建一个神经网络(不使用任何第三方库，作者意思应该是不使用任何机器学习库)。
在本文中，我们将演示如何使用Python编程语言创建一个简单的神经网络。
问题 #  这里用表格列出了我们需要解决的问题：
我们将会训练一个特定的神经网络模型，当提供一组新数据的时候，使其能够准确地预测输出值。
如你在表中所见，输出值总是等于输入数据的第一个值，因此我们期望的表中输出(?)值是1。
让我们思考看看能不能使用一些Python代码来给出相同的结果(再继续阅读之前，你可以在文章末尾仔细地阅读此项目的代码)
创建一个神经网络类 #  我们将在Python中创建一个NeuralNetwork类来训练神经元以提供准确的预测，该类还具有一些其他的辅助函数
尽管我们没有使用任何一个神经网络库用于这个简单的神经网络示例，我们也会导入numpy包来协助计算。
该库带有以下四个重要方法：
 exp：用于生成自然指数 array：用于生成矩阵 dot：用于乘法矩阵 random：用于生成随机数(注意：我们将对随机数进行播种以确保其有效分布)  应用 Sigmoid 激活函数 #  该神经网络将使用Sigmoid function作为激活函数，其绘制了一个典型的S形曲线：
此函数可以将任意值映射到区间0~1之间，它将帮助我们规范化输入值的和权重乘积之和。
随后，我们将创建Sigmoid函数的导数来帮助计算机对权重进行必要的调整。
一个Sigmoid函数的输出可以用来生成它的导数，例如，如果输出变量是X，那么它的导数将是x * (1-x)。
推导过程如下：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/04_appendix/02.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/04_appendix/02.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/</guid>
      <description>梯度下降数学推导 #  以感知器为例，可以梯度下降来学习合适的权重和偏置：
假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为：
$$ \bar{y}^i = w_1x_1^i+w_2x_2^i+&amp;hellip;+w_nx_n^i+b $$
任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示：
$$ e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2} $$
那么所有误差的和可以表示为：
$$ \begin{aligned} E &amp;amp;= e^1+e^2+&amp;hellip;+e^n \\ &amp;amp;= \sum_{i=1}^ne^i \\ &amp;amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 \end{aligned} $$
梯度下降 #  想象一下，当你从山顶往下走，只要你沿着最陡峭的位置往下走，那么终将走到最底部（也可能是局部最低）：
我们学习的目的就是在$E$尽量最小，然后得到此时的$w$和$b$，前面说的最陡峭的位置该怎么定义呢？我们可以引入梯度，这是一个向量，指的是函数值上升最快的方向，那么最陡峭的位置就可以用在最陡峭的方向迈出一步（步长，学习速率），用数学公式表示为：
 其中：
 $\nabla$表示梯度算子 $\nabla f(x)$表示函数的梯度 $\eta$表示梯度、学习速率，可以理解为找准下山的方向后要迈多大步子  推导 #  现在有了目标函数，也知道怎么找到让目标函数值最小的办法，对于参数$w$： $$ E_{(w)} = \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 $$
那么$W$值的更新公式为：
$$ w_{n e w}=w_{\text {old }}-\eta \nabla E_{(w)} $$
关键步骤来了，来看看$E_{(w)}$的推导吧：
$$ \begin{aligned} \nabla E(\mathrm{w}) &amp;amp;=\frac{\partial}{\partial \mathrm{w}} E(\mathrm{w}) \\
&amp;amp;=\frac{\partial}{\partial \mathrm{w}} \frac{1}{2} \sum_{i=1}^{n}\left(y^{(i)}-\bar{y}^{(i)}\right)^{2} \\&amp;amp;=\frac{1}{2}\frac{\partial}{\partial \mathrm{w}} \sum_{i=1}^{n} \left(y^{(i)2}-2y^{(i)}\bar{y}^{(i)}+\bar{y}^{(i)2}\right) \end{aligned} $$</description>
    </item>
    
  </channel>
</rss>
