<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>第一部分：探索 on Kubernetes 学习之路</title>
    <link>https://www.howie6879.cn/k8s/docs/01_explore/</link>
    <description>Recent content in 第一部分：探索 on Kubernetes 学习之路</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://www.howie6879.cn/k8s/docs/01_explore/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://www.howie6879.cn/k8s/docs/01_explore/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/k8s/docs/01_explore/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</guid>
      <description>神经网络基础 #  要想入门以及往下理解深度学习，其中一些概念可能是无法避免地需要你理解一番，比如：
 什么是感知器 什么是神经网络 张量以及运算 微分 梯度下降  带着问题出发 #  在开始之前希望你有一点机器学习方面的知识，解决问题的前提是提出问题，我们提出这样一个问题，对MNIST数据集进行分析，然后在解决问题的过程中一步一步地来捋清楚其中涉及到的概念
MNIST数据集是一份手写字训练集，出自MNIST，相信你对它不会陌生，它是机器学习领域的一个经典数据集，感觉任意一个教程都拿它来说事，不过这也侧面证明了这个数据集的经典，这里简单介绍一下：
 拥有60,000个示例的训练集，以及10,000个示例的测试集 图片都由一个28 ×28 的矩阵表示，每张图片都由一个784 维的向量表示 图片分为10类， 分别对应从0～9，共10个阿拉伯数字  压缩包内容如下：
 train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)  上图：
图片生成代码如下：
%matplotlib inline import matplotlib import matplotlib.pyplot as plt import numpy as np from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/k8s/docs/01_explore/02.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/k8s/docs/01_explore/02.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/</guid>
      <description>识别手写字 #  Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。
神经网络是一门重要的机器学习技术，它通过模拟人脑的神经网络来实现人工智能的目的，所以其也是深度学习的基础，了解它之后自然会受益颇多，本章主要是以识别手写字这个问题来贯穿整篇，那么，人类的视觉系统和神经网络到底在识别一个目标的时候，主要区别在哪？
 人类视觉系统：通过数十亿年不断地进化与学习，最终能够极好地适应理解视觉世界的任务，从而无意识地就可以对目标进行判断识别 神经网络：通过提供的样本来推断出识别某种目标的规则，作为判断标准  本章的主要内容是介绍神经网络的基本概念以及引入一个识别手写数字的例子来加深我们的理解，你将了解到：
 两个重要的人工神经元：感知器和S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 标准的神经网络学习算法：随机梯度下降算法  感知器 #  1943年，心理学家McCulloch和数学家Pitts发表了《A logical calculus of the ideas immanent in nervous activity》，其中提出了抽象的神经元模型MP，但是在这个模型中权重都是要求提前设置好才可以输出目标值，所以很遗憾，它不可以学习，但这不影响此模型给后来者带来的影响，比如感知器：
 感知器是Frank Rosenblatt提出的一个由两层神经元组成的人工神经网络，它的出现在当时可是引起了轰动，因为感知器是首个可以学习的神经网络
 感知器的工作方式如下所示：
$x_{1},x_{2},x_{3}$ 分别表示三个不同的二进制输入，output则是一个二进制输出，对于多种输入，可能有的输入成立有的不成立，在这么多输入的影响下，该如何判断输出output呢？Rosenblatt引入了权重来表示相应输入的重要性。
对于$x_{1},x_{2},&amp;hellip;,x_{j}$个输入，每个输入都有对应的权重$w_{1},w_{2},&amp;hellip;,w_{j}$，最后的输出output由每个输入与其对应的权重相乘之和与阈值之差$\sum _{j} w{_j}x{_j}$来决定，如下：
假设$b=-threshold$且$w$和$x$对应权重和输⼊的向量，即：
 $x=(x_{1},x_{2},&amp;hellip;,x_{j})$ $w=(w_{1},w_{2},&amp;hellip;,w_{j})$  那么感知器的规则可以重写为:
这就是感知器的数学模型，是不是就像一个逻辑回归模型？只要将感知器输出规则替换为($f(x)=x$)，后面我们会知道这称之为激活函数，其实这种感知器叫做线性单元。
它的出现让我们可以设计学习算法，从而实现自动调整人工神经元的权重和偏置，与此同时output也会随之改变，这就是学习！如果你有兴趣可以看我用python写的一个感知器自动学习实现与非门，代码在**nndl_chapter01**。
说句题外话，由于感知器是单层神经网络，它只能实现简单的线性分类任务，所以它无法对异或问题进行分类，异或的真值表如下：
   $x$ $y$ $output$     0 0 0   0 1 1   1 0 1   1 1 0    可以看出来，异或问题是线性不可分的，不信你画个坐标轴试试看，那么问题来了？怎么解决，大部分都能很快地想出解决方案，既然感知器可以实现线性分类，也就是说实现与非门是没有问题的，逻辑上来说我们可以实现任何逻辑功能（比如四个与非门实现异或），但前提是为感知器加入一个隐藏层，意思就是多了一个隐藏层的神经网络之后，就可以解决异或问题。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/k8s/docs/01_explore/03.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/k8s/docs/01_explore/03.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/</guid>
      <description>梯度下降数学推导 #  以感知器为例，可以梯度下降来学习合适的权重和偏置：
假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为：
$$ \bar{y}^i = w_1x_1^i+w_2x_2^i+&amp;hellip;+w_nx_n^i+b $$
任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示：
$$ e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2} $$
那么所有误差的和可以表示为：
$$ \begin{aligned} E &amp;amp;= e^1+e^2+&amp;hellip;+e^n \ &amp;amp;= \sum_{i=1}^ne^i \ &amp;amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 \end{aligned} $$
想象一下，当你从山顶往下走，只要你沿着最陡峭的位置往下走，那么终将走到最底部（也可能是局部最低）：
我们学习的目的就是在$E$尽量最小，然后得到此时的$w$和$b$，前面说的最陡峭的位置该怎么定义呢？我们可以引入梯度，这是一个向量，指的是函数值上升最快的方向，那么最陡峭的位置就可以用在最陡峭的方向迈出一步（步长，学习速率），用数学公式表示为：
$$ \mathbf{x}{n e w}=\mathbf{x}{\text {old }}-\eta \nabla f(x) $$
其中：
 $\nabla$表示梯度算子 $\nabla f(x)$表示函数的梯度 $\eta$表示梯度、学习速率，可以理解为找准下山的方向后要迈多大步子  现在有了目标函数，也知道怎么找到让目标函数值最小的办法，对于参数$w$：
$$ E_{(w)} = \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 $$
那么$W$值的更新公式为：
$$ w_{n e w}=w_{\text {old }}-\eta \nabla E_{(w)} $$
关键步骤来了，来看看$E_{(w)}$的推导吧：
$$ \begin{aligned} \nabla E(\mathrm{w}) &amp;amp;=\frac{\partial}{\partial \mathrm{w}} E(\mathrm{w}) \</description>
    </item>
    
  </channel>
</rss>
