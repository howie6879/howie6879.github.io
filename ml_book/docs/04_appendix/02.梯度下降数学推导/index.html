<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta name="generator" content="Hugo 0.88.1" />
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="梯度下降数学推导 #  以感知器为例，可以梯度下降来学习合适的权重和偏置：
假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为：
$$ \bar{y}^i = w_1x_1^i&#43;w_2x_2^i&#43;&hellip;&#43;w_nx_n^i&#43;b $$
任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示：
$$ e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2} $$
那么所有误差的和可以表示为：
$$ \begin{aligned} E &amp;= e^1&#43;e^2&#43;&hellip;&#43;e^n \\ &amp;= \sum_{i=1}^ne^i \\ &amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 \end{aligned} $$
梯度下降 #  想象一下，当你从山顶往下走，只要你沿着最陡峭的位置往下走，那么终将走到最底部（也可能是局部最低）：
我们学习的目的就是在$E$尽量最小，然后得到此时的$w$和$b$，前面说的最陡峭的位置该怎么定义呢？我们可以引入梯度，这是一个向量，指的是函数值上升最快的方向，那么最陡峭的位置就可以用在最陡峭的方向迈出一步（步长，学习速率），用数学公式表示为：
 其中：
 $\nabla$表示梯度算子 $\nabla f(x)$表示函数的梯度 $\eta$表示梯度、学习速率，可以理解为找准下山的方向后要迈多大步子  推导 #  现在有了目标函数，也知道怎么找到让目标函数值最小的办法，对于参数$w$： $$ E_{(w)} = \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 $$
那么$W$值的更新公式为：
$$ w_{n e w}=w_{\text {old }}-\eta \nabla E_{(w)} $$
关键步骤来了，来看看$E_{(w)}$的推导吧：
$$ \begin{aligned} \nabla E(\mathrm{w}) &amp;=\frac{\partial}{\partial \mathrm{w}} E(\mathrm{w}) \\
&amp;=\frac{\partial}{\partial \mathrm{w}} \frac{1}{2} \sum_{i=1}^{n}\left(y^{(i)}-\bar{y}^{(i)}\right)^{2} \\&amp;=\frac{1}{2}\frac{\partial}{\partial \mathrm{w}} \sum_{i=1}^{n} \left(y^{(i)2}-2y^{(i)}\bar{y}^{(i)}&#43;\bar{y}^{(i)2}\right) \end{aligned} $$">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="梯度下降数学推导 #  以感知器为例，可以梯度下降来学习合适的权重和偏置：
假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为：
$$ \bar{y}^i = w_1x_1^i&#43;w_2x_2^i&#43;&hellip;&#43;w_nx_n^i&#43;b $$
任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示：
$$ e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2} $$
那么所有误差的和可以表示为：
$$ \begin{aligned} E &amp;= e^1&#43;e^2&#43;&hellip;&#43;e^n \\ &amp;= \sum_{i=1}^ne^i \\ &amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 \end{aligned} $$
梯度下降 #  想象一下，当你从山顶往下走，只要你沿着最陡峭的位置往下走，那么终将走到最底部（也可能是局部最低）：
我们学习的目的就是在$E$尽量最小，然后得到此时的$w$和$b$，前面说的最陡峭的位置该怎么定义呢？我们可以引入梯度，这是一个向量，指的是函数值上升最快的方向，那么最陡峭的位置就可以用在最陡峭的方向迈出一步（步长，学习速率），用数学公式表示为：
 其中：
 $\nabla$表示梯度算子 $\nabla f(x)$表示函数的梯度 $\eta$表示梯度、学习速率，可以理解为找准下山的方向后要迈多大步子  推导 #  现在有了目标函数，也知道怎么找到让目标函数值最小的办法，对于参数$w$： $$ E_{(w)} = \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2 $$
那么$W$值的更新公式为：
$$ w_{n e w}=w_{\text {old }}-\eta \nabla E_{(w)} $$
关键步骤来了，来看看$E_{(w)}$的推导吧：
$$ \begin{aligned} \nabla E(\mathrm{w}) &amp;=\frac{\partial}{\partial \mathrm{w}} E(\mathrm{w}) \\
&amp;=\frac{\partial}{\partial \mathrm{w}} \frac{1}{2} \sum_{i=1}^{n}\left(y^{(i)}-\bar{y}^{(i)}\right)^{2} \\&amp;=\frac{1}{2}\frac{\partial}{\partial \mathrm{w}} \sum_{i=1}^{n} \left(y^{(i)2}-2y^{(i)}\bar{y}^{(i)}&#43;\bar{y}^{(i)2}\right) \end{aligned} $$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.howie6879.cn/ml_book/docs/04_appendix/02.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/" /><meta property="article:section" content="docs" />



<title>02.梯度下降数学推导 | 机器学习之路</title>
<link rel="manifest" href="/ml_book/manifest.json">
<link rel="icon" href="/ml_book/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/ml_book/book.min.e935e20bd0d469378cb482f0958edf258c731a4f895dccd55799c6fbc8043f23.css" integrity="sha256-6TXiC9DUaTeMtILwlY7fJYxzGk&#43;JXczVV5nG&#43;8gEPyM=">
<script defer src="/ml_book/en.search.min.a8a5fa00b257a439744f5e2ded1501ccf866eb4708141aeb82d9953b30346981.js" integrity="sha256-qKX6ALJXpDl0T14t7RUBzPhm60cIFBrrgtmVOzA0aYE="></script>
<style>

img[src$='#center']
{
    display: block;
    margin: 0.7rem auto;  
     
}

img[src$='#floatleft']
{
    float:left;
    margin: 0.7rem;       
     
}

img[src$='#floatright']
{
    float:right;
    margin: 0.7rem;       
     
}
</style>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a href="/ml_book"><span>机器学习之路</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>






  
<ul>
  
  <li>
    <a href="https://www.howie6879.cn/" target="_blank" rel="noopener">
        老胡的储物柜
      </a>
  </li>
  
  <li>
    <a href="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/wechat_howie.png" target="_blank" rel="noopener">
        微信公众号
      </a>
  </li>
  
  <li>
    <a href="https://github.com/howie6879" target="_blank" rel="noopener">
        Github
      </a>
  </li>
  
</ul>







  



  
  <ul>
    
      
        <li>
          
  
  

  
    <span>第一部分：基石</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/01_basic/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="">01.神经网络基础</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>第二部分：探索 NNDL</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/01.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/" class="">01.识别手写字</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/" class="">02.反向传播算法如何工作</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/03.%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%B3%95/" class="">03.改进神经 络的学习 法</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/04.%E7%A5%9E%E7%BB%8F%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/" class="">04.神经 络可以计算任何函数的可视化证明</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/05.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83/" class="">05.深度神经 络为何很难训练</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/06.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="">06.深度学习</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>第三部分：统计学习方法</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/03_lihang/01.%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/" class="">01.统计学习方法概论</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/03_lihang/02.%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="">02.感知机</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>第四部分：附录</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/04_appendix/00.%E4%B8%80%E7%AB%99%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%91%E7%A0%94%E5%8F%91%E5%B9%B3%E5%8F%B0/" class="">00.一站式机器学习云研发平台</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/04_appendix/01.%E8%AF%91%E5%A6%82%E4%BD%95%E7%94%A8python%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="">01.[译]如何用 Python创建一个简单的神经网络</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/04_appendix/02.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/" class=" active">02.梯度下降数学推导</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/ml_book/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>02.梯度下降数学推导</strong>

  <label for="toc-control">
    
    <img src="/ml_book/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#梯度下降">梯度下降</a></li>
    <li><a href="#推导">推导</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="梯度下降数学推导">
  梯度下降数学推导
  <a class="anchor" href="#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc">#</a>
</h1>
<p>以感知器为例，可以梯度下降来学习合适的权重和偏置：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/image-20210114211230457.png" alt="shadow-感知器图示" /></p>
<p>假设有<code>n</code>个样本，第<code>i</code>次的实际输出为<code>y</code>，对于样本的预测输出可以表示为：</p>
<p>$$
\bar{y}^i = w_1x_1^i+w_2x_2^i+&hellip;+w_nx_n^i+b
$$</p>
<p>任意一个样本的实际输出和预测输出单个样本的误差，可以使用<code>MES</code>表示：</p>
<p>$$
e^i=\frac{1}{2}(y^i-\bar{y}^i)^{2}
$$</p>
<p>那么所有误差的和可以表示为：</p>
<p>$$
\begin{aligned}
E &amp;= e^1+e^2+&hellip;+e^n
\\ &amp;= \sum_{i=1}^ne^i
\\ &amp;= \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2
\end{aligned}
$$</p>
<h2 id="梯度下降">
  梯度下降
  <a class="anchor" href="#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d">#</a>
</h2>
<p>想象一下，当你从山顶往下走，只要你沿着最陡峭的位置往下走，那么终将走到最底部（也可能是局部最低）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/1042406-20161017221342935-1872962415.png" alt="img" /></p>
<p>我们学习的目的就是在$E$尽量最小，然后得到此时的$w$和$b$，前面说的<strong>最陡峭的位置</strong>该怎么定义呢？我们可以引入<strong>梯度</strong>，这是一个向量，指的是函数值上升最快的方向，那么<strong>最陡峭的位置</strong>就可以用在最陡峭的方向迈出一步（步长，学习速率），用数学公式表示为：</p>
<div align=center><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/image-20210321203448530.png" style="zoom: 25%;" ></div>
<p>其中：</p>
<ul>
<li>$\nabla$表示梯度算子</li>
<li>$\nabla f(x)$表示函数的梯度</li>
<li>$\eta$表示梯度、学习速率，可以理解为找准下山的方向后要迈多大步子</li>
</ul>
<h2 id="推导">
  推导
  <a class="anchor" href="#%e6%8e%a8%e5%af%bc">#</a>
</h2>
<p>现在有了目标函数，也知道怎么找到让目标函数值最小的办法，对于参数$w$：
$$
E_{(w)} = \frac{1}{2}\sum_{i=1}^n(y^i-w^Tx^i)^2
$$</p>
<p>那么$W$值的更新公式为：</p>
<p>$$
w_{n e w}=w_{\text {old }}-\eta \nabla E_{(w)}
$$</p>
<p>关键步骤来了，来看看$E_{(w)}$的推导吧：</p>
<p>$$
\begin{aligned}
\nabla E(\mathrm{w}) &amp;=\frac{\partial}{\partial \mathrm{w}} E(\mathrm{w}) \\<br>
&amp;=\frac{\partial}{\partial \mathrm{w}} \frac{1}{2} \sum_{i=1}^{n}\left(y^{(i)}-\bar{y}^{(i)}\right)^{2}
\\&amp;=\frac{1}{2}\frac{\partial}{\partial \mathrm{w}} \sum_{i=1}^{n}
\left(y^{(i)2}-2y^{(i)}\bar{y}^{(i)}+\bar{y}^{(i)2}\right)
\end{aligned}
$$</p>
<p>再引入链式求导法则：</p>
<p>$$
\begin{aligned}
\nabla E(\mathrm{w}) &amp;=\frac{\partial}{\partial \mathrm{w}} E(\mathrm{w}) \\<br>
&amp;=\frac{1}{2} \sum_{i=1}^{n} \frac{\partial}{\partial \mathrm{w}}\left(y^{(i) 2}-2 y^{(i)} \bar{y}^{(i)}+\bar{y}^{(i) 2}\right) \\<br>
&amp;=\frac{1}{2} \sum_{i=1}^{n}\left(\frac{\partial}{\partial \bar{y}^{(i)}}\left(y^{(i) 2}-2 y^{(i)} \bar{y}^{(i)}+\bar{y}^{(i) 2}\right) \frac{\partial y_{(i)}}{\partial \mathrm{w}}\right) \\<br>
&amp;=\frac{1}{2} \sum_{i=1}^{n}\left(\left(-2 y^{(i)}+2 \bar{y}^{(i)}\right) \mathbf{x}^{(i)}\right) \\<br>
&amp;=-\sum_{i=1}^{n}\left(y^{(i)}-\bar{y}^{(i)}\right) \mathrm{x}^{(i)}
\end{aligned}
$$</p>
<p>前面提到$W$值的更新公式为：</p>
<p>$$
w_{n e w}=w_{\text {old }}-\eta \nabla E_{(w)}
$$</p>
<p>将上面计算结果带入：</p>
<p>$$
w_{n e w}=w_{\text {old }}+\eta \sum_{i=1}^{n}\left(y^{(i)}-\bar{y}^{(i)}\right) \mathrm{x}^{(i)}
$$</p>
<p>参数的更新方式就这样计算出来了，其实所谓的学习，就是确定一个目标函数用一定的计算方法让其算出最优的参数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/wechat_howie.png" alt="wechat_howie" /></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {equationNumbers: { autoNumber: "AMS" }}
  });
</script>


 
        
<div class="guide-links">
    
    
    </div>
      </footer>

      
  
  <div class="book-comments">

<footer>
    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://www.howie6879.cn/css/gitalk.css?v=0.0.0">
    <script src="https://www.howie6879.cn/js/gitalk.min.js?v=0.0.0"></script>
    <script>
        var gitalk = new Gitalk({
            clientID: '2c38ed8e7f85da0f1510',
            clientSecret: '1984f14456cb1a999dde013ec6a3e6123a92d59a',
            repo: 'howie6879.github.io',
            owner: 'howie6879',
            admin: ['howie6879'],
            id: location.pathname.substr(0, 48), 
            distractionFreeMode: false 
        })

        gitalk.render('gitalk-container')
    </script>
</footer></div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#梯度下降">梯度下降</a></li>
    <li><a href="#推导">推导</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>

</html>












