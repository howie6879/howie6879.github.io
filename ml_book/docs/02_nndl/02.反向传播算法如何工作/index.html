<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta name="generator" content="Hugo 0.88.1" />
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="反向传播算法如何工作 #  前面一章，我们通过了梯度下降算法实现目标函数的最小化，从而学习了该神经网络的权重和偏置，但是有一个问题并没有考虑到，那就是如何计算代价函数的梯度，本章的重点就是介绍计算这些梯度的快速算法——反向传播算法，首先先介绍一下上图中以及下面文章中会出现的一些数学符号：
 $L$ : 表示网络层数 $b_j^l$ : 表示第$l$层的第$j$个神经元的偏置 $a_j^l$ : 表示第$l$层的第$j$个神经元的激活值 $w_{j k}^{l}$ : 表示从$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接上的权重 $w^l$ : 权重矩阵，其中元素表示$l-1$层连接到$l$层神经元的权重 $b^l$ : 第$l$层神经元的偏置向量 $z^l$ : 第$l$层神经元的带权输入向量 $a^l$ : 第$l$层每个神经元激活值构成的向量 $\delta_{j}^{l}$ : 第$l$层第$j$个神经元的**误差**  本篇文章是公式重灾区，但是涉及的知识并不高级，这也说明一个道理：
 很多看似显而易见的想法只有在事后才变得显而易见。
 热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 #  通过第一章，我们已经知道每个神经元的激活值的计算方法，根据上面的公式，我们可以得出$a_j^l$的表达方式：
$$ a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}&#43;b_{j}^{l}\right) $$
举个例子：$a_3^2$表示第二层的第三个神经元的激活值，那么该输出值怎么同上一层的输出值以及权重关联起来的呢，根据激活值的计算公式，我们可以得出：
$$ \begin{aligned} a_3^2 &amp;= \sigma(w_{3 1}^{2} a_1^1 &#43; w_{3 2}^{2} a_2^1&#43; w_{3 3}^{2} a_3^1 &#43; b_3^2 ) \ &amp;= \sigma\left(\sum_{k=1}^3 w_{3 k}^2 a_{k}^1 &#43; b_3^2 \right) \end{aligned} $$">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="反向传播算法如何工作 #  前面一章，我们通过了梯度下降算法实现目标函数的最小化，从而学习了该神经网络的权重和偏置，但是有一个问题并没有考虑到，那就是如何计算代价函数的梯度，本章的重点就是介绍计算这些梯度的快速算法——反向传播算法，首先先介绍一下上图中以及下面文章中会出现的一些数学符号：
 $L$ : 表示网络层数 $b_j^l$ : 表示第$l$层的第$j$个神经元的偏置 $a_j^l$ : 表示第$l$层的第$j$个神经元的激活值 $w_{j k}^{l}$ : 表示从$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接上的权重 $w^l$ : 权重矩阵，其中元素表示$l-1$层连接到$l$层神经元的权重 $b^l$ : 第$l$层神经元的偏置向量 $z^l$ : 第$l$层神经元的带权输入向量 $a^l$ : 第$l$层每个神经元激活值构成的向量 $\delta_{j}^{l}$ : 第$l$层第$j$个神经元的**误差**  本篇文章是公式重灾区，但是涉及的知识并不高级，这也说明一个道理：
 很多看似显而易见的想法只有在事后才变得显而易见。
 热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 #  通过第一章，我们已经知道每个神经元的激活值的计算方法，根据上面的公式，我们可以得出$a_j^l$的表达方式：
$$ a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}&#43;b_{j}^{l}\right) $$
举个例子：$a_3^2$表示第二层的第三个神经元的激活值，那么该输出值怎么同上一层的输出值以及权重关联起来的呢，根据激活值的计算公式，我们可以得出：
$$ \begin{aligned} a_3^2 &amp;= \sigma(w_{3 1}^{2} a_1^1 &#43; w_{3 2}^{2} a_2^1&#43; w_{3 3}^{2} a_3^1 &#43; b_3^2 ) \ &amp;= \sigma\left(\sum_{k=1}^3 w_{3 k}^2 a_{k}^1 &#43; b_3^2 \right) \end{aligned} $$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.howie6879.cn/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/" /><meta property="article:section" content="docs" />



<title>02.反向传播算法如何工作 | 机器学习之路</title>
<link rel="manifest" href="/ml_book/manifest.json">
<link rel="icon" href="/ml_book/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/ml_book/book.min.e935e20bd0d469378cb482f0958edf258c731a4f895dccd55799c6fbc8043f23.css" integrity="sha256-6TXiC9DUaTeMtILwlY7fJYxzGk&#43;JXczVV5nG&#43;8gEPyM=">
<script defer src="/ml_book/en.search.min.a8a5fa00b257a439744f5e2ded1501ccf866eb4708141aeb82d9953b30346981.js" integrity="sha256-qKX6ALJXpDl0T14t7RUBzPhm60cIFBrrgtmVOzA0aYE="></script>
<style>

img[src$='#center']
{
    display: block;
    margin: 0.7rem auto;  
     
}

img[src$='#floatleft']
{
    float:left;
    margin: 0.7rem;       
     
}

img[src$='#floatright']
{
    float:right;
    margin: 0.7rem;       
     
}
</style>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a href="/ml_book"><span>机器学习之路</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>






  
<ul>
  
  <li>
    <a href="https://www.howie6879.cn/" target="_blank" rel="noopener">
        老胡的储物柜
      </a>
  </li>
  
  <li>
    <a href="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/wechat_howie.png" target="_blank" rel="noopener">
        微信公众号
      </a>
  </li>
  
  <li>
    <a href="https://github.com/howie6879" target="_blank" rel="noopener">
        Github
      </a>
  </li>
  
</ul>







  



  
  <ul>
    
      
        <li>
          
  
  

  
    <span>第一部分：基石</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/01_basic/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="">01.神经网络基础</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>第二部分：探索 NNDL</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/01.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/" class="">01.识别手写字</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/" class=" active">02.反向传播算法如何工作</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/03.%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%B3%95/" class="">03.改进神经 络的学习 法</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/04.%E7%A5%9E%E7%BB%8F%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/" class="">04.神经 络可以计算任何函数的可视化证明</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/05.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83/" class="">05.深度神经 络为何很难训练</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/02_nndl/06.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="">06.深度学习</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>第三部分：统计学习方法</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/03_lihang/01.%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/" class="">01.统计学习方法概论</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/03_lihang/02.%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="">02.感知机</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>第四部分：附录</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/04_appendix/00.%E4%B8%80%E7%AB%99%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%91%E7%A0%94%E5%8F%91%E5%B9%B3%E5%8F%B0/" class="">00.一站式机器学习云研发平台</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/04_appendix/01.%E8%AF%91%E5%A6%82%E4%BD%95%E7%94%A8python%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="">01.[译]如何用 Python创建一个简单的神经网络</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://www.howie6879.cn/ml_book/docs/04_appendix/02.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/" class="">02.梯度下降数学推导</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/ml_book/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>02.反向传播算法如何工作</strong>

  <label for="toc-control">
    
    <img src="/ml_book/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#热神经络中使矩阵快速计算输出的法">热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法</a></li>
    <li><a href="#关于代价函数的两个假设">关于代价函数的两个假设</a></li>
    <li><a href="#反向传播的四个基本方程">反向传播的四个基本方程</a>
      <ul>
        <li><a href="#输出层误差的程">输出层误差的⽅程</a></li>
        <li><a href="#使用下一层的误差表示当前层的误差">使用下一层的误差表示当前层的误差</a></li>
        <li><a href="#代价函数关于络中任意偏置的改变率">代价函数关于⽹络中任意偏置的改变率</a></li>
        <li><a href="#代价函数关于任何个权重的改变率">代价函数关于任何⼀个权重的改变率</a></li>
      </ul>
    </li>
    <li><a href="#反向传播算法">反向传播算法</a></li>
    <li><a href="#反向传播全局观">反向传播：全局观</a></li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="反向传播算法如何工作">
  反向传播算法如何工作
  <a class="anchor" href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e5%a6%82%e4%bd%95%e5%b7%a5%e4%bd%9c">#</a>
</h1>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/20190524092154.png" alt="" /></p>
<p>前面一章，我们通过了梯度下降算法实现目标函数的最小化，从而学习了该神经网络的权重和偏置，但是有一个问题并没有考虑到，那就是如何计算代价函数的梯度，本章的重点就是介绍计算这些梯度的快速算法——反向传播算法，首先先介绍一下上图中以及下面文章中会出现的一些数学符号：</p>
<ul>
<li>$L$ : 表示网络层数</li>
<li>$b_j^l$ : 表示第$l$层的第$j$个神经元的偏置</li>
<li>$a_j^l$ : 表示第$l$层的第$j$个神经元的激活值</li>
<li>$w_{j k}^{l}$ : 表示从$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接上的权重</li>
<li>$w^l$ : 权重矩阵，其中元素表示$l-1$层连接到$l$层神经元的权重</li>
<li>$b^l$ : 第$l$层神经元的偏置向量</li>
<li>$z^l$ : 第$l$层神经元的带权输入向量</li>
<li>$a^l$ : 第$l$层每个神经元激活值构成的向量</li>
<li>$\delta_{j}^{l}$ : 第$l$层第$j$个神经元的**误差**</li>
</ul>
<p>本篇文章是公式重灾区，但是涉及的知识并不高级，这也说明一个道理：</p>
<blockquote>
<p>很多看似显而易见的想法只有在事后才变得显而易见。</p>
</blockquote>
<h2 id="热神经络中使矩阵快速计算输出的法">
  热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法
  <a class="anchor" href="#%e7%83%ad%e7%a5%9e%e7%bb%8f%e7%bb%9c%e4%b8%ad%e4%bd%bf%e7%9f%a9%e9%98%b5%e5%bf%ab%e9%80%9f%e8%ae%a1%e7%ae%97%e8%be%93%e5%87%ba%e7%9a%84%e6%b3%95">#</a>
</h2>
<p>通过第一章，我们已经知道每个神经元的激活值的计算方法，根据上面的公式，我们可以得出$a_j^l$的表达方式：</p>
<p>$$
a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)
$$</p>
<p>举个例子：$a_3^2$表示第二层的第三个神经元的激活值，那么该输出值怎么同上一层的输出值以及权重关联起来的呢，根据激活值的计算公式，我们可以得出：</p>
<p>$$
\begin{aligned}
a_3^2 &amp;= \sigma(w_{3 1}^{2} a_1^1 + w_{3 2}^{2} a_2^1+ w_{3 3}^{2} a_3^1 + b_3^2 )
\ &amp;= \sigma\left(\sum_{k=1}^3 w_{3 k}^2 a_{k}^1 + b_3^2  \right)
\end{aligned}
$$</p>
<p>可以看到例子中的结果满足表达式，接下来，让我们将表达式改成向量形式：</p>
<p>$$
\begin{aligned}
a^{l} &amp;= \sigma\left(w^{l} a^{l-1}+b^{l}\right)
\ &amp;= \sigma\left(z^l \right)
\end{aligned}
$$</p>
<p>这个式子是正确的么，我们实际根据第一层到第二层的计算来看看：</p>
<p>首先定义第一层的激活函数输出值向量$a^1$:</p>
<p>$$
{a^1} = \left[ \begin{array}{c}{a_1^1} \ {a_2^1} \ {a_3^1} \end{array}\right]
$$</p>
<p>然后是第一层神经元连接到第二层神经元的权重矩阵：</p>
<p>$$
{w^2} = \left[ \begin{array}{c}{w_{1 1}^2,w_{1 2}^2,w_{1 3}^2} \ {w_{2 1}^2,w_{2 2}^2,w_{2 3}^2} \ {w_{3 1}^2,w_{3 2}^2,w_{3 3}^2} \ {w_{4 1}^2,w_{4 2}^2,w_{4 3}^2} \end{array}\right]
$$</p>
<p>同理，第二层神经元的偏置向量：</p>
<p>$$
{b^2} = \left[ \begin{array}{c}{b_1^2} \ {b_2^2} \ {b_3^2} \ {b_4^2} \end{array}\right]
$$</p>
<p>我们的目标是求得第二层神经元激活值构成的向量$a^2$:</p>
<p>$$
{a^2} = \left[ \begin{array}{c}{a_1^2} \ {a_2^2} \ {a_3^2} \ {a_4^2} \end{array}\right]
$$</p>
<p>激活值计算如下：</p>
<p>$$
\begin{aligned}
a^2 &amp;= \sigma(w^2a^1 + b^2)
\&amp;= \sigma\left(\left[ \begin{array}{c}{w_{1 1}^2,w_{1 2}^2,w_{1 3}^2} \ {w_{2 1}^2,w_{2 2}^2,w_{2 3}^2} \ {w_{3 1}^2,w_{3 2}^2,w_{3 3}^2} \ {w_{4 1}^2,w_{4 2}^2,w_{4 3}^2} \end{array}\right] \left[ \begin{array}{c}{a_1^1} \ {a_2^1} \ {a_3^1} \end{array}\right] + \left[ \begin{array}{c}{b_1^2} \ {b_2^2} \ {b_3^2} \ {b_4^2} \end{array}\right]\right)
\end{aligned}
$$</p>
<p>$$
{a^2} = \left[ \begin{array}{c}{a_1^2} \ {a_2^2} \ {a_3^2} \ {a_4^2} \end{array}\right]=\sigma\left(\left[ \begin{array}{c}{w_{1 1}^2 a_1^1+w_{1 2}^2 a_2^1+w_{1 3}^2 a_3^1}+b_1^2 \ {w_{2 1}^2 a_1^1+w_{2 2}^2 a_2^1+w_{2 3}^2 a_3^1}+b_2^2 \ {w_{3 1}^2 a_1^1+w_{3 2}^2 a_2^1+w_{3 3}^2 a_3^1}+b_3^2 \ {w_{4 1}^2 a_1^1+w_{4 2}^2 a_2^1+w_{4 3}^2 a_3^1}+b_4^2 \end{array}\right]\right)
$$</p>
<p>可以看到$a_3^2$的值和前面第一次举例子算出来的值一致。</p>
<h2 id="关于代价函数的两个假设">
  关于代价函数的两个假设
  <a class="anchor" href="#%e5%85%b3%e4%ba%8e%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0%e7%9a%84%e4%b8%a4%e4%b8%aa%e5%81%87%e8%ae%be">#</a>
</h2>
<p>我们以均方误差得出的代价函数如下：</p>
<p>$$
C=\frac{1}{2 n} \sum_{x}\left|y(x)-a^{L}(x)\right|^{2}
$$</p>
<p>公式说明：</p>
<ul>
<li>$y(x)$是目标输出</li>
<li>$a^{L}(x)$是当输入是$x$时候网络输出的激活值向量</li>
</ul>
<p>好了，为了应⽤反向传播，我们需要对代价函数 $C$ 做出什么样的前提假设呢？</p>
<p>第一：代价函数可以被写成⼀个在每个训练样本 $x$ 上的代价函数 $C_x$ 的均值:</p>
<p>$$
C=\frac{1}{n} \sum_{x} C_{x}
$$</p>
<p>第⼆：代价可以写成神经⽹络输出的函数：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190524120302.png" alt="" /></p>
<p>为对于⼀个单独的训练样本 $x$ 其⼆次代价函数可以写作：</p>
<p>$$
C=\frac{1}{2}\left|y-a^{L}\right|^{2}=\frac{1}{2} \sum_{j}\left(y_{j}-a_{j}^{L}\right)^{2}
$$</p>
<h2 id="反向传播的四个基本方程">
  反向传播的四个基本方程
  <a class="anchor" href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e6%96%b9%e7%a8%8b">#</a>
</h2>
<p>反向传播其实是对权重和偏置变化影响代价函数过程的理解，最终极的含义其实就是计算偏导数 $\partial C / \partial w_{j k}^{l}$ 和 $\partial C / \partial b_{j}^{l}$。</p>
<p>为了计算这些值，我们引入一个中间量$\delta_{j}^{l}$ ，其表示的是第$l$层第$j$个神经元的**误差**，其中$\delta^l$表示第$l$层的误差向量，对于这个误差，我们应该怎样表示呢：</p>
<p>$$
\delta_{j}^{l} \equiv \frac{\partial C}{\partial z_{j}^{l}}
$$</p>
<p>接下来要做的就是将这些误差和 $\partial C / \partial w_{j k}^{l}$ 和 $\partial C / \partial b_{j}^{l}$ 联系起来，解决方案就是反向传播基于四个基本⽅程：</p>
<h3 id="输出层误差的程">
  输出层误差的⽅程
  <a class="anchor" href="#%e8%be%93%e5%87%ba%e5%b1%82%e8%af%af%e5%b7%ae%e7%9a%84%e7%a8%8b">#</a>
</h3>
<p><strong>输出层误差的⽅程</strong>，$\delta^L$ ： 每个元素定义如下：</p>
<p>$$
\delta_{j}^{L}=\frac{\partial C}{\partial a_{j}^{L}} \sigma^{\prime}\left(z_{j}^{L}\right)
$$</p>
<p>矩阵形式重写⽅程：</p>
<p>$$
\delta^{L}=\nabla_{a} C \odot \sigma^{\prime}\left(z^{L}\right)
$$</p>
<p>其中$\nabla_{a}$就是梯度向量，其元素就是偏导数$\partial C / \partial a_{j}^{L}$的所有元素，以上述二次代价函数为例：</p>
<p>$$
C_{x}=\frac{1}{2}\left|y-a^{L}\right|^{2}
$$</p>
<p>可以得出：</p>
<p>$$
\nabla_{a} C=\left(a^{L}-y\right)
$$</p>
<p>因此方程的矩阵形式可以改成：</p>
<p>$$
\delta^{L}=\left(a^{L}-y\right) \odot \sigma^{\prime}\left(z^{L}\right)
$$</p>
<p>推导过程如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528115627.png" alt="" /></p>
<h3 id="使用下一层的误差表示当前层的误差">
  使用下一层的误差表示当前层的误差
  <a class="anchor" href="#%e4%bd%bf%e7%94%a8%e4%b8%8b%e4%b8%80%e5%b1%82%e7%9a%84%e8%af%af%e5%b7%ae%e8%a1%a8%e7%a4%ba%e5%bd%93%e5%89%8d%e5%b1%82%e7%9a%84%e8%af%af%e5%b7%ae">#</a>
</h3>
<p>$$
\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)
$$</p>
<p>推导过程如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528120156.png" alt="" /></p>
<h3 id="代价函数关于络中任意偏置的改变率">
  代价函数关于⽹络中任意偏置的改变率
  <a class="anchor" href="#%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0%e5%85%b3%e4%ba%8e%e7%bb%9c%e4%b8%ad%e4%bb%bb%e6%84%8f%e5%81%8f%e7%bd%ae%e7%9a%84%e6%94%b9%e5%8f%98%e7%8e%87">#</a>
</h3>
<p>$$
\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}
$$</p>
<p>推导过程如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528140657.png" alt="" /></p>
<h3 id="代价函数关于任何个权重的改变率">
  代价函数关于任何⼀个权重的改变率
  <a class="anchor" href="#%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0%e5%85%b3%e4%ba%8e%e4%bb%bb%e4%bd%95%e4%b8%aa%e6%9d%83%e9%87%8d%e7%9a%84%e6%94%b9%e5%8f%98%e7%8e%87">#</a>
</h3>
<p>$$
\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}
$$</p>
<p>推导过程如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528142309.png" alt="" /></p>
<p>反向传播的四个基本公式，靠着一个链式法则，就全都推下来了，没有什么难度</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190524150849.png" alt="" /></p>
<h2 id="反向传播算法">
  反向传播算法
  <a class="anchor" href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95">#</a>
</h2>
<p>反向传播算法给出了一种计算代价函数梯度的方法，算法描述如下：</p>
<ul>
<li>输入特征x：为输⼊层设置对应的激活值$a^1$</li>
<li>前向传播：对每个$$l=2,3,&hellip;,L$$计算相应的$$z^l$$和$$a^l$$
<ul>
<li>$$z^l=w^{l} a^{l-1}+b^{l}$$</li>
<li>$$a^l=\sigma(z^l)$$</li>
</ul>
</li>
<li>输出层误差：$$\delta^{L}=\nabla_{a} C \odot \sigma^{\prime}\left(z^{L}\right)$$</li>
<li>反向误差传播：对每个$$l=L-1,L-2,&hellip;,2$$，计算$$\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)$$</li>
<li>输出：代价函数的梯度由$$\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}$$和$$\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}$$得出</li>
</ul>
<h2 id="反向传播全局观">
  反向传播：全局观
  <a class="anchor" href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%85%a8%e5%b1%80%e8%a7%82">#</a>
</h2>
<p>假设我们已经对⼀些⽹络中的 $w_{j k}^l$ 做⼀点⼩⼩的变动 $$\Delta w_{j k}^{l}$$</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528150559.png" alt="" /></p>
<p>显然，这样会造成输出激活值的改变：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528150804.png" alt="" /></p>
<p>然后，会让下一层所有的激活值产生改变：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528150841.png" alt="" /></p>
<p>接着，这些改变都将影响到⼀个个下⼀层，到达输出层，最终影响代价函数：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528150904.png" alt="" /></p>
<p>根据求导的思想，我们可以得出下面公式：</p>
<p>$$
\Delta C \approx \frac{\partial C}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}
$$</p>
<p>我们知道，$$\Delta w_{j k}^{l}$$造成了第$$l$$层的第$$j$$神经元的激活值的变化$$\Delta a_{j}^{l}$$，这个变化由下⾯的公式给出：</p>
<p>$$
\Delta a_{j}^{l} \approx \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}
$$</p>
<p>$$\Delta a_{j}^{l}$$的变化会造成下一层所有神经元激活值的变化，我们聚焦到其中⼀个激活值上看看影响的情况，不防设$$a_q^{l+1}$$：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/images/20190528151606.png" alt="" /></p>
<p>实际上，这会导致下⾯的变化：</p>
<p>$$
\Delta a_{q}^{l+1} \approx \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \Delta a_{j}^{l}
$$</p>
<p>我们已经知道$$\Delta a_{j}^{l} \approx \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}$$，我们可以得到：</p>
<p>$$
\Delta a_{q}^{l+1} \approx \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}
$$</p>
<p>就这样一直传播下去，最终将所有的影响汇聚到输出层代价的变化，假设$$a_{j}^{l}, a_{q}^{l+1}, \ldots, a_{n}^{L-1}, a_{m}^{L}$$，那么结果的表达式就是：</p>
<p>$$
\Delta C \approx \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}} \ldots \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}
$$</p>
<p>影响输出层代价的权重值有很多，所以我们需要进行求和：</p>
<p>$$
\Delta C \approx \sum_{m n p_{\ldots q}} \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}} \ldots \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}
$$</p>
<p>因为：</p>
<p>$$
\Delta C \approx \frac{\partial C}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}
$$</p>
<p>带入上面式子，得出：</p>
<p>$$
\begin{aligned}
\frac{\partial C}{\partial w_{j k}^{l}}&amp;=\sum_{m n p \ldots q} \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}} \cdots \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}}
\&amp;=a_{k}^{l-1} \delta_{j}^{l}
\end{aligned}
$$</p>
<p>想起一句歌词，又回到最初的起点，我们竟然就是在做反向传播，神奇。</p>
<h2 id="参考">
  参考
  <a class="anchor" href="#%e5%8f%82%e8%80%83">#</a>
</h2>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a></li>
<li><a href="https://github.com/zhanggyb/nndl">Neural Networks and Deep Learning 中文版</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26765585">知乎上另外一篇笔记</a></li>
</ul>
<p>搞定收工，有兴趣欢迎关注我的公众号：</p>
<p><img src="https://cdn.jsdelivr.net/gh/howie6879/oss/uPic/wechat_howie.png" alt="wechat_howie" /></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {equationNumbers: { autoNumber: "AMS" }}
  });
</script>


 
        
<div class="guide-links">
    
    
    </div>
      </footer>

      
  
  <div class="book-comments">

<footer>
    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://www.howie6879.cn/css/gitalk.css?v=0.0.0">
    <script src="https://www.howie6879.cn/js/gitalk.min.js?v=0.0.0"></script>
    <script>
        var gitalk = new Gitalk({
            clientID: '2c38ed8e7f85da0f1510',
            clientSecret: '1984f14456cb1a999dde013ec6a3e6123a92d59a',
            repo: 'howie6879.github.io',
            owner: 'howie6879',
            admin: ['howie6879'],
            id: location.pathname.substr(0, 48), 
            distractionFreeMode: false 
        })

        gitalk.render('gitalk-container')
    </script>
</footer></div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#热神经络中使矩阵快速计算输出的法">热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法</a></li>
    <li><a href="#关于代价函数的两个假设">关于代价函数的两个假设</a></li>
    <li><a href="#反向传播的四个基本方程">反向传播的四个基本方程</a>
      <ul>
        <li><a href="#输出层误差的程">输出层误差的⽅程</a></li>
        <li><a href="#使用下一层的误差表示当前层的误差">使用下一层的误差表示当前层的误差</a></li>
        <li><a href="#代价函数关于络中任意偏置的改变率">代价函数关于⽹络中任意偏置的改变率</a></li>
        <li><a href="#代价函数关于任何个权重的改变率">代价函数关于任何⼀个权重的改变率</a></li>
      </ul>
    </li>
    <li><a href="#反向传播算法">反向传播算法</a></li>
    <li><a href="#反向传播全局观">反向传播：全局观</a></li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>

</html>












