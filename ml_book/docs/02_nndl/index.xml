<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>第二部分：探索 NNDL on 机器学习之路</title>
    <link>https://www.howie6879.cn/ml_book/docs/02_nndl/</link>
    <description>Recent content in 第二部分：探索 NNDL on 机器学习之路</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://www.howie6879.cn/ml_book/docs/02_nndl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/01.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/01.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/</guid>
      <description>识别手写字 #  Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。
神经网络是一门重要的机器学习技术，它通过模拟人脑的神经网络来实现人工智能的目的，所以其也是深度学习的基础，了解它之后自然会受益颇多，本章主要是以识别手写字这个问题来贯穿整篇，那么，人类的视觉系统和神经网络到底在识别一个目标的时候，主要区别在哪？
 人类视觉系统：通过数十亿年不断地进化与学习，最终能够极好地适应理解视觉世界的任务，从而无意识地就可以对目标进行判断识别 神经网络：通过提供的样本来推断出识别某种目标的规则，作为判断标准  本章的主要内容是介绍神经网络的基本概念以及引入一个识别手写数字的例子来加深我们的理解，你将了解到：
 两个重要的人工神经元：感知器和S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 标准的神经网络学习算法：随机梯度下降算法  感知器 #  1943年，心理学家McCulloch和数学家Pitts发表了《A logical calculus of the ideas immanent in nervous activity》，其中提出了抽象的神经元模型MP，但是在这个模型中权重都是要求提前设置好才可以输出目标值，所以很遗憾，它不可以学习，但这不影响此模型给后来者带来的影响，比如感知器：
 感知器是Frank Rosenblatt提出的一个由两层神经元组成的人工神经网络，它的出现在当时可是引起了轰动，因为感知器是首个可以学习的神经网络
 感知器的工作方式如下所示：
$x_{1},x_{2},x_{3}$ 分别表示三个不同的二进制输入，output则是一个二进制输出，对于多种输入，可能有的输入成立有的不成立，在这么多输入的影响下，该如何判断输出output呢？Rosenblatt引入了权重来表示相应输入的重要性。
对于$x_{1},x_{2},&amp;hellip;,x_{j}$个输入，每个输入都有对应的权重$w_{1},w_{2},&amp;hellip;,w_{j}$，最后的输出output由每个输入与其对应的权重相乘之和与阈值之差$\sum _{j} w{_j}x{_j}$来决定，如下：
假设$b=-threshold$且$w$和$x$对应权重和输⼊的向量，即：
 $x=(x_{1},x_{2},&amp;hellip;,x_{j})$ $w=(w_{1},w_{2},&amp;hellip;,w_{j})$  那么感知器的规则可以重写为:
这就是感知器的数学模型，是不是就像一个逻辑回归模型？只要将感知器输出规则替换为($f(x)=x$)，后面我们会知道这称之为激活函数，其实这种感知器叫做线性单元。
它的出现让我们可以设计学习算法，从而实现自动调整人工神经元的权重和偏置，与此同时output也会随之改变，这就是学习！如果你有兴趣可以看我用python写的一个感知器自动学习实现与非门，代码在nndl_chapter01。
说句题外话，由于感知器是单层神经网络，它只能实现简单的线性分类任务，所以它无法对异或问题进行分类，异或的真值表如下：
   $x$ $y$ $output$     0 0 0   0 1 1   1 0 1   1 1 0    可以看出来，异或问题是线性不可分的，不信你画个坐标轴试试看，那么问题来了？怎么解决，大部分都能很快地想出解决方案，既然感知器可以实现线性分类，也就是说实现与非门是没有问题的，逻辑上来说我们可以实现任何逻辑功能（比如四个与非门实现异或），但前提是为感知器加入一个隐藏层，意思就是多了一个隐藏层的神经网络之后，就可以解决异或问题。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/</guid>
      <description>反向传播算法如何工作 #  前面一章，我们通过了梯度下降算法实现目标函数的最小化，从而学习了该神经网络的权重和偏置，但是有一个问题并没有考虑到，那就是如何计算代价函数的梯度，本章的重点就是介绍计算这些梯度的快速算法——反向传播算法，首先先介绍一下上图中以及下面文章中会出现的一些数学符号：
 $L$ : 表示网络层数 $b_j^l$ : 表示第$l$层的第$j$个神经元的偏置 $a_j^l$ : 表示第$l$层的第$j$个神经元的激活值 $w_{j k}^{l}$ : 表示从$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接上的权重 $w^l$ : 权重矩阵，其中元素表示$l-1$层连接到$l$层神经元的权重 $b^l$ : 第$l$层神经元的偏置向量 $z^l$ : 第$l$层神经元的带权输入向量 $a^l$ : 第$l$层每个神经元激活值构成的向量 $\delta_{j}^{l}$ : 第$l$层第$j$个神经元的**误差**  本篇文章是公式重灾区，但是涉及的知识并不高级，这也说明一个道理：
 很多看似显而易见的想法只有在事后才变得显而易见。
 热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 #  通过第一章，我们已经知道每个神经元的激活值的计算方法，根据上面的公式，我们可以得出$a_j^l$的表达方式：
$$ a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right) $$
举个例子：$a_3^2$表示第二层的第三个神经元的激活值，那么该输出值怎么同上一层的输出值以及权重关联起来的呢，根据激活值的计算公式，我们可以得出：
$$ \begin{aligned} a_3^2 &amp;amp;= \sigma(w_{3 1}^{2} a_1^1 + w_{3 2}^{2} a_2^1+ w_{3 3}^{2} a_3^1 + b_3^2 ) \ &amp;amp;= \sigma\left(\sum_{k=1}^3 w_{3 k}^2 a_{k}^1 + b_3^2 \right) \end{aligned} $$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/03.%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/03.%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%B3%95/</guid>
      <description>改进神经⽹络的学习⽅法 #   万丈高楼平地起，反向传播是深度学习这栋大厦的基石，所以在这块花多少时间都是值得的
 前面一章，我们深入理解了反向传播算法如何工作，本章的主要目的是改进神经网络的学习方法，本章涉及的技术包括：
 交叉熵代价函数 四种称为规范化的⽅法（L1 和 L2 规范化，弃权和训练数据的⼈为扩展） 更好的权重初始化⽅法 如何选择神经网络的超参数  交叉熵代价函数 #  通过前面的学习，我们知道，神经网络一直在努力地让代价函数变小，现在我们抛出一个问题，代价函数从初始值到我们理想状态的值，这个过程所消耗的时间是怎样的情况呢？
对于人类来说，我们犯比较明显的错误的时候，改正的效果也很明显，神经网络也是这样的么？让我们通过下面的一个小例子来详细看看：
假设有这样一个神经元，我们希望当输入为1的时候，输出为0，让我们通过梯度下降的方式来训练权重和偏置，⾸先将权重和偏置初始化为0.6 和 0.9，前面我们说过，S型神经元的输出计算方式为：
输⼊$x_{1},x_{2},&amp;hellip;,x_{j}$，权重为$w_{1},w_{2},&amp;hellip;,w_{j}$，和偏置b的S型神经元的输出是：
$$ \frac{1}{1+\exp(-\sum_j w_j x_j-b)} $$
将0.6 和 0.9带入，即可得到输出为：0.82，很显然和目标0相差甚远，那么就只能继续寻找最优解了，随着迭代期的增加，神经元的输出、权重、偏置和代价的变化如下⾯⼀系列图形所⽰：
可以看到，200次迭代后，输出已经满足我们的需求了，毕竟只要小于0.5我们都可以归为0，接下来再将初始权重和偏置都设置为2.0，此时初始输出为0.98，这是和⽬标值的差距相当⼤的，现在看看神经元学习的过程：
这次可以明显地看到迭代150次左右没有很明显的变化，随后速度就快了起来。
这个例子引出了一个问题，包括在其他一般的神经网络中也会出现，为何会学习缓慢呢？我们在训练中应该怎么避免这个情况？思考一下，问题的原因其实很简单
 权重和偏置是根据梯度下降的形式来更新其对应的值，学习缓慢就意味着代价函数的偏导数很小
 让我们从数学公式的角度来理解这句话和上面的图，对于目前提供的单个样例的输入和输出：
 $x=1$ $y=0$  再结合前面第一章讲的代价函数的定义：
$$ C(w,b) \equiv \frac{1}{2n} \sum_x | y(x) - a|^2 $$
 w 表⽰所有的⽹络中权重的集合 b 是所有的偏置 n 是训练输⼊数据的个数 a 是表⽰当输⼊为x时输出的向量，可以理解为output  类比一下，此时的代价函数为：
$$ C=\frac{(y-a)^{2}}{2} $$
接下来就是求权重和偏置的偏导数，推导之前说明一下前面的公式：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/04.%E7%A5%9E%E7%BB%8F%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/04.%E7%A5%9E%E7%BB%8F%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/</guid>
      <description>神经⽹络可以计算任何函数的可视化证明 #  本章其实和前面章节的关联性不大，所以大可将本章作为小短文来阅读，当然基本的深度学习基础还是要有的。
主要介绍了神经⽹络拥有的⼀种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入$x$，其值$f(x)$（或者说近似值）是网络的输出，哪怕是多输入和多输出也是如此，我们大可直接得出一个结论：
 不论我们想要计算什么样的函数，我们都确信存在⼀个神经⽹络（多层）可以计算它
 试想以下这种普遍性代表着什么，我觉得代表着做更多可能的事情（将其看做计算一种函数）：
 比如将中文翻译成英文 比如根据⼀个mp4视频⽂件⽣成⼀个描述电影情节并讨论表演质量的问题 &amp;hellip;  现实往往是残酷的，我们知道有这个网络存在，比如中文翻译成英文的网络，通常情况下想得往往不可得，网络在那里，但更可能我们得不到，怎么办？
前面我们知道，我们通过学习算法来拟合函数，学习算法和普遍性的结合是⼀种有趣的混合，直到现在，本书⼀直是着重谈学习算法，到了本章，我们来看看普遍性，看看它究竟意味着什么。
两个预先声明 #  在解释为何普遍性定理成⽴前，关于神经⽹络可以计算任何函数有两个预先声明需要注意一下：
 这句话不是说⼀个⽹络可以被⽤来准确地计算任何函数，而是说，我们可以获得尽可能好的⼀个近似，通过增加隐藏元的数量，我们可以提升近似的精度，同时对于目标精度，我们需要确定精度范围：$|g(x)-f(x)|&amp;lt;\epsilon$，其中$\epsilon&amp;gt;0$ 按照上⾯的⽅式近似的函数类其实是连续函数，如果函数不是连续的，也就是会有突然、极陡的跳跃，那么⼀般来说⽆法使⽤⼀个神经⽹络进⾏近似，这并不意外，因为神经⽹络计算的就是输⼊的连续函数   普遍性定理的表述：包含⼀个隐藏层的神经⽹络可以被⽤来按照任意给定的精度来近似任何连续函数
 接下来的内容会使⽤有两个隐藏层的⽹络来证明这个结果的弱化版本，在问题中会简要介绍如何通过⼀些微调把这个解释适应于只使⽤⼀个隐藏层的⽹络并给出证明。
一个输入和一个输出的普遍性 #  先从一个简单的函数$f(x)$（即只有一个输入和一个输出）开始，我们将利用神经网络来近似这个连续函数：
第一章我们就探讨过多层感知机实现异或，这次同样的，我们加入一个隐藏层就可以让函数舞动起来，比如下面这个有一个隐藏层、两个隐藏神经元的网络：
第一步，暂时只考虑顶层的神经元，第一章也讲过S型神经元，所以输出范围类似上图右上角，重点看看这个S型函数，前面已经说过：
$$ \sigma(z) \equiv 1 /\left(1+e^{-z}\right) $$
其中：$z=wx+b$，参见右上角的图，让我们考虑一下几个情况：
 当$x$不变，$b$逐渐增加的情况下，输出会在原来的基础上变大，图像会相对向左边运动，因为$w$没变，所以图像形状不会变   上述情况让$b$键减小，图像会右移，同样图像形状不变 当$b$不变，$w$减小，很显然，图像的陡峭程度会下降，反之亦然  下图是书中给出的图示：
其实我们完全可以自己绘制这个过程，利用Python的matplotlib可以很好地完成这个事情：
import matplotlib.pyplot as plt import numpy as np def sigmoid(w, b, x): return 1.0 / (1.0 + np.exp(-(w * x + b))) def plot_sigmoid(w, b): x = np.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/05.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/05.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83/</guid>
      <description>深度神经⽹络为何很难训练 #  上一章提到了神经网络的一种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入得到一个近似的输出。
 普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许多现实世界问题的函数
 而且理论上我们只要一个隐藏层就可以计算任何函数，第一章我们就用如下的网络结构完成了一个手写字识别的模型：
这时候，大家心中可能都会有这样一个想法，如果加大网络的深度，模型的识别准确率是否会提高？
随即我们会基于反向传播的随机梯度下降来训练神经网络，但实际上这会产生一些问题，因为我们的深度神经网络并没有比浅层网络好很多。那么此处就引出了一个问题，为什么深度神经网络相对训练困难？
仔细研究会发现：
  如果网络后面层学习状况好的时候，前面的层次经常会在训练时停滞不前
  反之情况也会发生，前面层训练良好，后面停滞不前
  实际上，我们发现在深度神经⽹络中使⽤基于梯度下降的学习⽅法本⾝存在着内在不稳定性，这种不稳定性使得先前或者后⾯神经网络层的学习过程阻滞。
消失的梯度问题 #  我们在第一章识别手写字曾经以MNIST数字分类问题做过示例，接下来我们同样通过这个样例来看看我们的神经网络在训练过程中究竟哪里出了问题。
简单回顾一下，之前我们的训练样本路径为/pylab/datasets/mnist.pkl.gz ，相关案例代码在pylab都可以找到（我稍做了改动以支持Python3）。
import mnist_loader import network2 training_data, validation_data, test_data = mnist_loader.load_data_wrapper() # 输入层 784 # 隐藏层 30 # 输出层 10 sizes = [784, 30, 10] net = network2.Network(sizes=sizes) # 随机梯度下降开始训练 net.SGD( training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True, ) &amp;#34;&amp;#34;&amp;#34; Epoch 0 training complete Accuracy on evaluation data: 9280 / 10000 Epoch 1 training complete Accuracy on evaluation data: 9391 / 10000 .</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.cn/ml_book/docs/02_nndl/06.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.cn/ml_book/docs/02_nndl/06.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</guid>
      <description>深度学习 #  基于第一章节的学习，我们知道单层神经网络无法处理异或问题；如果需要计算多层神经网络，当时的计算机又缺少足够的计算能力，因此60年代末神经网络第一次进入了寒冬。
随后80年代，方向传播算法的出现解决了多层神经网络所需要的复杂计算问题（这也意味着异或问题的解决），但是由于数据集量级的影响，神经网络的能力被过拟合问题限制住了脚本，同时当时的SVM如日中天并且拥有极强的解释性，于是神经有一次陷入风雪。
摩尔定律一直在发挥着作用，终于到了2006年，Hinton在Science上提出了深度置信网络的神经网络模型，这是神经网络重回巅峰的代表性事件。随后发生的事情大家都有所耳闻：
 2012年ImageNet竞赛夺冠 2017年AlphaGo和柯洁的围棋对决 各种深度神经网络的商业化实践&amp;hellip;  现在，我们正处于第三次神经网络的复兴潮流中央。经过前面几章的探索，我们已经了解了深度神经⽹络通常⽐浅层神经⽹络更加难以训练，但是为了获得比浅层网络更加强大的能力，我们需要训练深度网络，本章主要讨论可以⽤来训练深度神经⽹络的技术，并在实战中应⽤它们。
本章是一个大章节，主要围绕的是从简单的浅层网络到卷积神经网络来慢慢构建强大的网络，从而解决MNIST手写字识别的问题。
在这个过程中，我们需要研究的相关技术有：卷积、池化、使用GPU来更好地训练、训练数据的算法性拓展、Dropout、网络的综合使用等。
介绍卷积网络 #  在第一章识别手写字中，我们就利用全连接神经网络基于MNIST训练集构建了一个可以识别手写字的深度学习模型。
让我们基于上面的网络结构思考下面几个问题：
 参数过多：对于手写字，第一层的输入图像大小是$28*28=784$个，因此第一个隐藏层的每个神经元到输入层都有784个权重参数，大家可以基于此计算一下，五层的全连接神经网络，参数将极其庞大会导致训练效率不高并且容易出现过拟合； 局表示不变性特征：目前的网络并没有考虑图像的空间结构，它在完全相同的基础上去对待相距很远和彼此接近的输⼊像素；自然图像中的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息．而全连接前馈网络很难提取这些 局部不变性特征，一般需要进行数据增强来提高性能。  卷积神经网络（Convolutional Neural Network，CNN 或 ConvNet）是一类特殊的人工神经网络，也是第一章提到的多层感知器（MLP）的变种。
我们可以从生物学角度出发，来看看卷积神经网络有哪些基本特征。我们知道视觉皮层的细胞存在 一个复杂的构造，这些细胞对视觉输入空间的子区域非常敏感，我们称之为感受野。在全连接网络中我们会将所有的输入神经元都连接到下一个隐藏的神经元，但这次我们不这么干，我们选择一个$5*5$的区域进行局部区域的连接：
这个输⼊图像的区域被称为隐藏神经元的局部感受野，然后在整个输⼊图像上交叉移动局部感受野，此时我们的$2828=784$个输入，用$55$的局部感受野，此时第一个隐藏层就会有$24*24$个神经元：
参考 #   Neural Networks and Deep Learning Neural Networks and Deep Learning 中文版 神经网络与深度学习-邱锡鹏 解析卷积神经网络  </description>
    </item>
    
  </channel>
</rss>
