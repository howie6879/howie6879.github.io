<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>第三部分：统计学习方法 on 机器学习之路</title>
    <link>https://www.howie6879.com/ml_book/docs/03_lihang/</link>
    <description>Recent content in 第三部分：统计学习方法 on 机器学习之路</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://www.howie6879.com/ml_book/docs/03_lihang/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://www.howie6879.com/ml_book/docs/03_lihang/01.%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.com/ml_book/docs/03_lihang/01.%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/</guid>
      <description>统计学习方法概述 # 第一章主要对全书内容做了一个内容的概括：
统计学习：定义、研究对象和方法 监督学习 统计学习三要素：模型、策略、算法 模型评估与选择：包括正则化、交叉验证与学习的泛化能力 生成模型与判别模型 分类问题、标注问题与回归问题 统计学习 # 什么是统计学习
统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，统计学习也称为统计机器学习
统计学习的特点
统计学习以计算机及网络为平台，是建立在计算机及 网络之上的 统计学习以数据为研究对象，是数据驱动的学科 统计学习的目的是对数据进行预测与分析 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论 什么是机器学习
如果一个程序可以在任务T上，随着经验E的增加，效果P也可以随之增加，则称这个程序可以从经验中学习。 &amp;mdash; 卡内基美隆大学的Tom Michael Mitchell教授
如果以垃圾邮件为例，一个程序指的是用到的机器学习算法，比如：朴素贝叶斯、逻辑回归；任务T指的是区分垃圾邮件的任务；经验E为已经区分过是否为垃圾邮件的历史邮件；效果P为机器学习算法在区分是否为垃圾邮件任务上的准确率
统计学习的目的
统计学习用于对数据进行预测与分析，特别是对未知新数据进行预测与分析
统计学习的方法
监督学习（supervised learning）：KNN、决策树、贝叶斯、逻辑回归 非监督学习（unsupervised learning）：聚类、降维 半监督学习（semisupervised learning）：self-training（自训练算法）、Graph-based Semi-supervised Learning（基于图的半监督算法）、Semi-supervised supported vector machine（半监督支持向量机，S3VM） 强化学习（reinforcement learning）：蒙特卡洛方法 监督学习 # 基本概念 # 输入空间（input space）：输入所有可能取值的集合 输出空间（output space）：输出所有可能取值的集合 特征空间（feature space）：每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。这时，所有特征向量存在的空间称为特征空间 联合概率分布：统计学习假设数据存在一定的统计规律，X和Y具有联合概率分布的假设就是监督学习关于数据的基本假设 - 机器学习-联合概率分布笔记 假设空间：学习的目的在于找到最好的模型，模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space） 人们根据输入、输出变量的不同类型，对预测任务给予不同的名称：
分类问题：输出变量为有限个离散变量的预测问题 回归问题：输入变量与输出变量均为连续变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题 问题的形式化 # 监督学习利用训练数据集学习一个模型，再用模型对测试样本集进行预测 （prediction）：
输入训练集 -&amp;gt; 生成模型 -&amp;gt; 预测，由于训练集是人工给出的，所以称之为监督学习</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.howie6879.com/ml_book/docs/03_lihang/02.%E6%84%9F%E7%9F%A5%E6%9C%BA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.howie6879.com/ml_book/docs/03_lihang/02.%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid>
      <description>感知机 # 本章主要介绍了二类分类的线性分类模型：感知机：
感知机模型 感知机学习策略 感知机学习算法 说明：个人感觉这本书偏理论化，讲究的是一招定天下，好处是内功深厚自然无敌，一通百通，但难处是语言有点晦涩，这章可以考虑结合我之前的一篇关于感知器的笔记，或许能加深理解，见这里
感知机模型 # 感知机（perceptron）：是一个二类分类的线性判断模型，其输入为实例的特征向量，输出为实例的类别，取+1和–1值，属于判别模型
注：+1 -1 分别代表正负类，有的可能用 1 0 表示
在介绍感知机定义之前，下面几个概念需要说明一下：
输入空间：输入所有可能取值的集合 输出空间：输出所有可能取值的集合 特征空间：每个具体的输入是一个实例，由特征向量表示 所以对于一个感知机模型，可以这样表示：
输入空间（特征空间）：$\chi \subseteq \mathbb{R} ^n$ 输出空间：$\gamma = \{+1,-1 \}$ 那么感知机就是由输入空间到输出空间的函数：
$$\displaystyle f( x) \ =\ sign( w\cdot x+b)$$
其中：
$sign$: 符号函数 $w$: 权值（weight）或权值向量（weight vector） $b$: 偏置（bias） 感知机的几何解释如下：线性方程
$$w\cdot x + b =0$$
如果是二维空间，感知机就是一个线性函数，将正负样本一分为二，如何是三维空间，那么感知机就是一个平面将类别一切为二，上升到n维空间的话，其对应的是特征空间$\mathbb{R} ^n$的一个超平面$S$：
$w$: 超平面的法向量 $b$: 超平面的截距 感知机学习策略 # 数据集的线性可分性 # 什么是数据集的线性可分性，很简单，对于一个数据集：
$$T = {(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_n,y_n)}$$
如果存在上面一节说的超平面$S$：$w\cdot x + b =0$，能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则称数据集T为线性可分数据集（linearly separable data set），否则，称数据集T线性不可分</description>
    </item>
    
  </channel>
</rss>
