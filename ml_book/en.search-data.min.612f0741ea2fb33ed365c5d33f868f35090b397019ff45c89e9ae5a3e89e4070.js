'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/ml_book/docs/01_basic/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/','title':"01.神经网络基础",'section':"第一部分：基石",'content':"神经网络基础 #  要想入门以及往下理解深度学习，其中一些概念可能是无法避免地需要你理解一番，比如：\n 什么是感知器 什么是神经网络 张量以及运算 微分 梯度下降  带着问题出发 #  在开始之前希望你有一点机器学习方面的知识，解决问题的前提是提出问题，我们提出这样一个问题，对MNIST数据集进行分析，然后在解决问题的过程中一步一步地来捋清楚其中涉及到的概念\nMNIST数据集是一份手写字训练集，出自MNIST，相信你对它不会陌生，它是机器学习领域的一个经典数据集，感觉任意一个教程都拿它来说事，不过这也侧面证明了这个数据集的经典，这里简单介绍一下：\n 拥有60,000个示例的训练集，以及10,000个示例的测试集 图片都由一个28 ×28 的矩阵表示，每张图片都由一个784 维的向量表示 图片分为10类， 分别对应从0～9，共10个阿拉伯数字  压缩包内容如下：\n train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)  上图：\n图片生成代码如下：\n%matplotlib inline import matplotlib import matplotlib.pyplot as plt import numpy as np from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = instances n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = matplotlib.cm.binary, **options) plt.axis(\u0026#34;off\u0026#34;) plt.figure(figsize=(9,9)) plot_digits(train_images[:100], images_per_row=10) plt.show() 不过你不用急着尝试，接下来我们可以一步一步慢慢来分析手写字训练集\n看这一行代码：\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data() MNIST数据集通过keras.datasets加载，其中train_images和train_labels构成了训练集，另外两个则是测试集：\n train_images.shape: (60000, 28, 28) train_labels.shape: (60000,)  我们要做的事情很简单，将训练集丢到神经网络里面去，训练后生成了我们期望的神经网络模型，然后模型再对测试集进行预测，我们只需要判断预测的数字是不是正确的即可\n在用代码构建一个神经网络之前，我先简单介绍一下到底什么是神经网络，让我们从感知器开始\n感知器 #   感知器是Frank Rosenblatt提出的一个由两层神经元组成的人工神经网络，它的出现在当时可是引起了轰动，因为感知器是首个可以学习的神经网络\n 感知器的工作方式如下所示：\n左侧三个变量分别表示三个不同的二进制输入，output则是一个二进制输出，对于多种输入，可能有的输入成立有的不成立，在这么多输入的影响下，该如何判断输出output呢？Rosenblatt引入了权重来表示相应输入的重要性\n此时，output可以表示为：\n上面右侧的式子是一个阶跃函数，就是和Sigmoid、Relu一样作用的激活函数，然后我们就可以自己实现一个感知器：\nimport numpy as np class Perceptron: \u0026#34;\u0026#34;\u0026#34; 代码实现 Frank Rosenblatt 提出的感知器的与非门，加深对感知器的理解 blog: https://www.howie6879.cn/post/33/ \u0026#34;\u0026#34;\u0026#34; def __init__(self, act_func, input_nums=2): \u0026#34;\u0026#34;\u0026#34; 实例化一些基本参数 :param act_func: 激活函数 \u0026#34;\u0026#34;\u0026#34; # 激活函数 self.act_func = act_func # 权重 已经确定只会有两个二进制输入 self.w = np.zeros(input_nums) # 偏置项 self.b = 0.0 def fit(self, input_vectors, labels, learn_nums=10, rate=0.1): \u0026#34;\u0026#34;\u0026#34; 训练出合适的 w 和 b :param input_vectors: 样本训练数据集 :param labels: 标记值 :param learn_nums: 学习多少次 :param rate: 学习率 \u0026#34;\u0026#34;\u0026#34; for i in range(learn_nums): for index, input_vector in enumerate(input_vectors): label = labels[index] output = self.predict(input_vector) delta = label - output self.w += input_vector * rate * delta self.b += rate * delta print(\u0026#34;此时感知器权重为{0}，偏置项为{1}\u0026#34;.format(self.w, self.b)) return self def predict(self, input_vector): if isinstance(input_vector, list): input_vector = np.array(input_vector) return self.act_func(sum(self.w * input_vector) + self.b) def f(z): \u0026#34;\u0026#34;\u0026#34; 激活函数 :param z: (w1*x1+w2*x2+...+wj*xj) + b :return: 1 or 0 \u0026#34;\u0026#34;\u0026#34; return 1 if z \u0026gt; 0 else 0 def get_and_gate_training_data(): \u0026#39;\u0026#39;\u0026#39; AND 训练数据集 \u0026#39;\u0026#39;\u0026#39; input_vectors = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]) labels = np.array([1, 0, 0, 0]) return input_vectors, labels if __name__ == \u0026#39;__main__\u0026#39;: \u0026#34;\u0026#34;\u0026#34; 输出如下： 此时感知器权重为[ 0.1 0.2]，偏置项为-0.2 与门 1 and 1 = 1 1 and 0 = 0 0 and 1 = 0 0 and 0 = 0 \u0026#34;\u0026#34;\u0026#34; # 获取样本数据 and_input_vectors, and_labels = get_and_gate_training_data() # 实例化感知器模型 p = Perceptron(f) # 开始学习 AND p_and = p.fit(and_input_vectors, and_labels) # 开始预测 AND print(\u0026#39;1 and 1 = %d\u0026#39; % p_and.predict([1, 1])) print(\u0026#39;1 and 0 = %d\u0026#39; % p_and.predict([1, 0])) print(\u0026#39;0 and 1 = %d\u0026#39; % p_and.predict([0, 1])) print(\u0026#39;0 and 0 = %d\u0026#39; % p_and.predict([0, 0])) S型神经元 #  神经元和感知器本质上是一样的，他们的区别在于激活函数不同，比如跃迁函数改为Sigmoid函数\n神经网络可以通过样本的学习来调整人工神经元的权重和偏置，从而使输出的结果更加准确，那么怎样给⼀个神经⽹络设计这样的算法呢？\n以数字识别为例，假设⽹络错误地把⼀个9的图像分类为8，我们可以让权重和偏置做些⼩的改动，从而达到我们需要的结果9，这就是学习。对于感知器，我们知道，其返还的结果不是0就是1，很可能出现这样一个情况，我们好不容易将一个目标，比如把9的图像分类为8调整回原来正确的分类，可此时的阈值和偏置会造成其他样本的判断失误，这样的调整不是一个好的方案\n所以，我们需要S型神经元，因为S型神经元返回的是[0,1]之间的任何实数，这样的话权重和偏置的微⼩改动只会引起输出的微⼩变化，此时的output可以表示为σ(w⋅x+b)，而σ就是S型函数，S型函数中S指的是Sigmoid函数，定义如下：\n神经网络 #  神经网络其实就是按照一定规则连接起来的多个神经元，一个神经网络由以下组件构成：\n 输入层：接受传递数据，这里应该是 784 个神经元 隐藏层：发掘出特征 各层之间的权重：自动学习出来 每个隐藏层都会有一个精心设计的激活函数，比如Sigmoid、Relu激活函数 输出层，10个输出 上⼀层的输出作为下⼀层的输⼊，信息总是向前传播，从不反向回馈：前馈神经网络 有回路，其中反馈环路是可⾏的：递归神经网络  从输入层传入手写字训练集，然后通过隐藏层向前传递训练集数据，最后输出层会输出10个概率值，总和为1。现在，我们可以看看Keras代码:\n第一步，对数据进行预处理，我们知道，原本数据形状是(60000, 28, 28)，取值区间为[0, 255]，现在改为[0, 1]：\ntrain_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(\u0026#39;float32\u0026#39;) / 255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype(\u0026#39;float32\u0026#39;) / 255 然后对标签进行分类编码：\nfrom keras.utils import to_categorical train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) 第二步，编写模型：\nfrom keras import models from keras import layers network = models.Sequential() network.add(layers.Dense(512, activation=\u0026#39;relu\u0026#39;, input_shape=(28 * 28,))) network.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) network.compile(optimizer=\u0026#39;rmsprop\u0026#39;,loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) network.fit(train_images, train_labels, epochs=5, batch_size=128) 一个隐藏层，激活函数选用relu，输出层使用softmax返回一个由10个概率值（总和为 1）组成的数组\n训练过程中显示了两个数字：一个是网络在训练数据上的损失loss，另一个是网络在 训练数据上的精度acc\n很简单，我们构建和训练一个神经网络，就这么几行代码，之所以写的这么剪短，是因为keras接接口封装地比较好用，但是里面的理论知识我们还是需要好好研究下\n神经网络的数据表示 #  TensorFlow里面的Tensor是张量的意思，上面例子里面存储在多维Numpy数组中的数据就是张量：张量是数据容器，矩阵就是二维张量，张量是矩阵向任意维度的推广，张量的维度称为轴\n标量 #  包含一个数字的张量叫做标量（0D张量），如下：\nx = np.array(12) print(x, x.ndim) # 12, 0 张量轴的个数也叫做阶(rank)\n向量 #  数字组成的数组叫做向量（1D张量），如下：\nx = np.array([12, 3, 6, 14, 7]) print(x, x.ndim) # [12 3 6 14 7] 1 矩阵 #  向量组成的数组叫做矩阵（2D张量），如下：\nx = np.array([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]]) print(x, x.ndim) # [[ 5 78 2 34 0] # [ 6 79 3 35 1] # [ 7 80 4 36 2]] 2 3D张量与更高维张量 #  将多个矩阵组合成一个新的数组就是一个3D张量，如下：\nx = np.array([[[5, 78, 2, 34, 0], [6, 79, 3, 35, 1]], [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1]], [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1]]]) print(x, x.ndim) # (array([[[ 5, 78, 2, 34, 0], # [ 6, 79, 3, 35, 1]], #  # [[ 5, 78, 2, 34, 0], # [ 6, 79, 3, 35, 1]], #  # [[ 5, 78, 2, 34, 0], # [ 6, 79, 3, 35, 1]]]), 3) 将多个3D张量组合成一个数组，可以创建一个4D张量\n关键属性 #  张量是由以下三个关键属性来定义：\n 轴的个数：3D张量三个轴，矩阵两个轴 形状：是一个整数元祖，比如前面矩阵为(3, 5)，向量(5,)，3D张量为(3, 2, 5) 数据类型  在Numpy中操作张量 #  以前面加载的train_images为：\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data() 比如进行切片选择10~100个数字：\ntrain_images[10:100].shape # (90, 28, 28) 数据批量的概念 #  深度学习模型会将数据集随机分割成小批量进行处理，比如：\nbatch = train_images[:128] batch.shape # (128, 28, 28) 现实世界的数据张量 #  下面将介绍下现实世界中数据的形状：\n 向量数据：2D张量，(samples, features) 时间序列数据或者序列数据：3D张量，(samples, timesteps, features) 图像：4D张量，(samples, height, width, channels) 或 (samples, channels, height, width) 视频：5D张量，(samples, frames, height, width, channels) 或 (samples, frames, channels, height, width)  张量运算 #  类似于计算机程序的计算可以转化为二进制计算，深度学习计算可以转化为数值数据张量上的一些张量运算(tensor operation)\n上面模型的隐藏层代码如下：\nkeras.layers.Dense(512, activation=\u0026#39;relu\u0026#39;) 这一层可以理解为一个函数，输入一个2D张量，输出一个2D张量，就如同上面感知机那一节最后输出的计算函数：\noutput = relu(dot(W, input) + b) 逐元素计算 #  Relu 和加法运算都是逐元素的运算，比如：\n# 输入示例 input_x = np.array([[2], [3], [1]]) # 权重 W = np.array([[5, 6, 1], [7, 8, 1]]) # 计算输出 z z = np.dot(W, input_x) # 实现激活函数 def naive_relu(x): assert len(x.shape) == 2 x = x.copy() for i in range(x.shape[0]): for j in range(x.shape[1]): x[i, j] = max(x[i, j], 0) return x # 激活函数对应的输出 output = naive_relu(z) output 广播 #  张量运算那节中，有这样一段代码：\noutput = relu(dot(W, input) + b) dot(W, input)是2D张量，b是向量，两个形状不同的张量相加，会发生什么？\n如果没有歧义的话，较小的张量会被广播，用来匹配较大张量的形状：\ninput_x = np.array([[1], [3]]) # 权重 W = np.array([[5, 6], [7, 8]]) b = np.array([1]) # 计算输出 z z = np.dot(W, input_x) + b # array([[24], # [32]]) 张量点积 #  点积运算，也叫张量积，如：\nimport numpy as np # 输入示例 input_x = np.array([[2], [3], [1]]) # 权重 W = np.array([[5, 6, 1], [7, 8, 1]]) np.dot(W, input_x) 两个向量之间的点积是一个标量：\ndef naive_vector_dot(x, y): assert len(x.shape) == 1 assert len(y.shape) == 1 assert x.shape[0] == y.shape[0] z = 0. for i in range(x.shape[0]): z += x[i] * y[i] return z x = np.array([1,2]) y = np.array([1,2]) naive_vector_dot(x, y) # 5.0 矩阵和向量点积后是一个向量：\nnp.dot(W, [1, 2, 3]) # array([20, 26]) 张量变形 #  前面对数据进行预处理的时候：\ntrain_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(\u0026#39;float32\u0026#39;) / 255 上面的例子将输入数据的shape变成了(60000, 784)，张量变形指的就是改变张量的行和列，得到想要的形状，前后数据集个数不变，经常遇到一个特殊的张量变形是转置(transposition)，如下：\nx = np.zeros((300, 20)) x = np.transpose(x) x.shape # (20, 300) 梯度优化 #  针对每个输入，神经网络都会通过下面的函数对输入数据进行变换：\noutput = relu(dot(W, input_x) + b) 其中：\n relu：激活函数 W：是一个张量，表示权重，第一步可以取较小的随机值进行随机初始化 b：是一个张量，表示偏置  现在我们需要一个算法来让我们找到权重和偏置，从而使得y=y(x)可以拟合样本输入的x\n再回到感知器 #  感知器学习的过程就是其中权重和偏置不断调优更新的过程，其中的偏置可以理解成输入为1的权重值，那么权重是怎么更新的呢？\n首先，介绍一个概念，损失函数，引用李航老师统计学习方法书中的一个解释：\n 监督学习问题是在假设空间中选取模型f作为决策函数，对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致，用一个损失函数（loss function）或代价函数（cost function）来度量预测错误的程度，损失函数是f(X)和Y的非负实值函数，记作L(Y,f(X))\n 其中模型f(X)关于训练数据集的平均损失，我们称之为：经验风险（empirical risk），上述的权重调整，就是在不断地让经验风险最小，求出最好的模型f(X)，我们暂时不考虑正则化，此时我们经验风险的最优化的目标函数就是：\n求解出此目标函数最小时对应的权重值，就是我们感知器里面对应的权重值，在推导之前，我们还得明白两个概念：\n 什么是导数 什么是梯度  什么是导数 #  假设有一个连续的光滑函数f(x) = y，什么是函数连续性？指的是x的微小变化只能导致y的微小变化。\n假设f(x)上的两点a,b足够接近，那么a,b可以近似为一个线性函数，此时他们斜率为k，那么可以说斜率k是f在b点的导数\n总之，导数描述了改变x后f(x)会如何变化，如果你希望减小f(x)的值，只需要将x沿着导数的反方向移动一小步即可，反之亦然\n什么是梯度 #  梯度是张量运算的导数，是导数这一概念向多元函数导数的推广，它指向函数值上升最快的方向，函数值下降最快的方向自然就是梯度的反方向\n随机梯度下降 #  推导过程如下：\n感知器代码里面的这段:\nself.w += input_vector * rate * delta 就对应上面式子里面推导出来的规则\n总结 #  再来看看全部的手写字识别模型代码：\nfrom keras import models from keras import layers from keras.utils import to_categorical (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(\u0026#39;float32\u0026#39;) / 255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype(\u0026#39;float32\u0026#39;) / 255 train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) network = models.Sequential() network.add(layers.Dense(512, activation=\u0026#39;relu\u0026#39;, input_shape=(28 * 28,))) network.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) network.compile(optimizer=\u0026#39;rmsprop\u0026#39;,loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) network.fit(train_images, train_labels, epochs=5, batch_size=128) test_loss, test_acc = network.evaluate(test_images, test_labels) print(\u0026#39;test_acc:\u0026#39;, test_acc)  输入数据保存在float32格式的Numpy张量中，形状分别是(60000, 784)和(10000, 784) 神经网络结构为：1个输入层、一个隐藏层、一个输出层 categorical_crossentropy是针对分类模型的损失函数 每批128个样本，共迭代5次，一共更新(469 * 5) = 2345次  说明 #  对本文有影响的书籍文章如下，感谢他们的付出：\n [统计学习方法] 第一章 Neural Networks and Deep Learning 第一章 Deep Learning with Python 第二章 hands_on_Ml_with_Sklearn_and_TF hanbt零基础入门深度学习系列  "});index.add({'id':1,'href':'/ml_book/docs/02_nndl/01.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/','title':"01.识别手写字",'section':"第二部分：探索 NNDL",'content':"识别手写字 #  Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。\n神经网络是一门重要的机器学习技术，它通过模拟人脑的神经网络来实现人工智能的目的，所以其也是深度学习的基础，了解它之后自然会受益颇多，本章主要是以识别手写字这个问题来贯穿整篇，那么，人类的视觉系统和神经网络到底在识别一个目标的时候，主要区别在哪？\n 人类视觉系统：通过数十亿年不断地进化与学习，最终能够极好地适应理解视觉世界的任务，从而无意识地就可以对目标进行判断识别 神经网络：通过提供的样本来推断出识别某种目标的规则，作为判断标准  本章的主要内容是介绍神经网络的基本概念以及引入一个识别手写数字的例子来加深我们的理解，你将了解到：\n 两个重要的人工神经元：感知器和S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 标准的神经网络学习算法：随机梯度下降算法  感知器 #  1943年，心理学家McCulloch和数学家Pitts发表了《A logical calculus of the ideas immanent in nervous activity》，其中提出了抽象的神经元模型MP，但是在这个模型中权重都是要求提前设置好才可以输出目标值，所以很遗憾，它不可以学习，但这不影响此模型给后来者带来的影响，比如感知器：\n 感知器是Frank Rosenblatt提出的一个由两层神经元组成的人工神经网络，它的出现在当时可是引起了轰动，因为感知器是首个可以学习的神经网络\n 感知器的工作方式如下所示：\n$x_{1},x_{2},x_{3}$ 分别表示三个不同的二进制输入，output则是一个二进制输出，对于多种输入，可能有的输入成立有的不成立，在这么多输入的影响下，该如何判断输出output呢？Rosenblatt引入了权重来表示相应输入的重要性。\n对于$x_{1},x_{2},\u0026hellip;,x_{j}$个输入，每个输入都有对应的权重$w_{1},w_{2},\u0026hellip;,w_{j}$，最后的输出output由每个输入与其对应的权重相乘之和与阈值之差$\\sum _{j} w{_j}x{_j}$来决定，如下：\n假设$b=-threshold$且$w$和$x$对应权重和输⼊的向量，即：\n $x=(x_{1},x_{2},\u0026hellip;,x_{j})$ $w=(w_{1},w_{2},\u0026hellip;,w_{j})$  那么感知器的规则可以重写为:\n这就是感知器的数学模型，是不是就像一个逻辑回归模型？只要将感知器输出规则替换为($f(x)=x$)，后面我们会知道这称之为激活函数，其实这种感知器叫做线性单元。\n它的出现让我们可以设计学习算法，从而实现自动调整人工神经元的权重和偏置，与此同时output也会随之改变，这就是学习！如果你有兴趣可以看我用python写的一个感知器自动学习实现与非门，代码在**nndl_chapter01**。\n说句题外话，由于感知器是单层神经网络，它只能实现简单的线性分类任务，所以它无法对异或问题进行分类，异或的真值表如下：\n   $x$ $y$ $output$     0 0 0   0 1 1   1 0 1   1 1 0    可以看出来，异或问题是线性不可分的，不信你画个坐标轴试试看，那么问题来了？怎么解决，大部分都能很快地想出解决方案，既然感知器可以实现线性分类，也就是说实现与非门是没有问题的，逻辑上来说我们可以实现任何逻辑功能（比如四个与非门实现异或），但前提是为感知器加入一个隐藏层，意思就是多了一个隐藏层的神经网络之后，就可以解决异或问题。\n可是在当时是没办法对多层神经网络（包括异或逻辑）进行训练的，因为计算量太大了，Minsky在1969年出版了一本叫《Perceptron》的书详细说明了这个问题。\n遇到问题，就解决问题，两层神经网络的作用，是可以对非线性进行分类的，我们的先人并未止步，直到反向传播（Backpropagation，BP）算法的出现解决了计算量太大的问题。\n感知器，终于可以多层了。\nS型神经元 #  前面提到，神经网络可以通过样本的学习来调整人工神经元的权重和偏置，从而使输出的结果更加准确，那么怎样给⼀个神经⽹络设计这样的算法呢？\n以数字识别为例，假设⽹络错误地把⼀个9的图像分类为8，我们可以让权重和偏置做些⼩的改动，从而达到我们需要的结果9，这就是学习。对于感知器，我们知道，其返还的结果不是0就是1，很可能出现这样一个情况，我们好不容易将一个目标，比如把9的图像分类为8调整回原来正确的分类，可此时的阈值和偏置会造成其他样本的判断失误，这样的调整不是一个好的方案。\n所以，我们需要S型神经元，因为S型神经元返回的是[0,1]之间的任何实数，这样的话权重和偏置的微⼩改动只会引起输出的微⼩变化，此时的output可以表示为$\\sigma(w \\cdot x+b)$，而$\\sigma$就是S型函数，S型函数中S指的是Sigmoid函数，定义如下：\n$$ \\sigma(z) \\equiv \\frac{1}{1+e^{-z}} $$\n其中$z$表达式为：\n$$z=\\sum_{j} w{_j}x{_j}+b$$\n那么⼀个具有输⼊$x_{1},x_{2},\u0026hellip;,x_{j}$，权重为$w_{1},w_{2},\u0026hellip;,w_{j}$，和偏置b的S型神经元的输出是：\n$$ \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)} $$\n上面说过，感知器的激活函数只会输出两个值，分别是0或1，让我们把目光投向S型神经元，假设此时$z$是一个很大的正数，那么此时output将无限接近于1，反之，若$z$是一个很大的负数，那么此时output将无限接近于0，从极端情况来看，是不是很像S型神经元呢？\nS型神经元的输出是[0,1]之间的任何实数，那么这个分类模型该如何判断分类结果呢？其实很简单，我们可以定义一个数值，比如：\n $\\sigma(z) \\leq 0.5$ 输出0 $\\sigma(z)\u0026gt;0.5$ 输出1  神经⽹络的架构 #  看下面一个四层神经网络，这种类型的多层⽹络有时被称为多层感知器或者MLP，如下图：\n需要记住以下几个概念：\n ⽹络中最左边的称为输⼊层，其中的神经元称为输⼊神经元 中间得称之为隐藏层，图中有两个隐藏层 最右边称之为输出层 上⼀层的输出作为下⼀层的输⼊，信息总是向前传播，从不反向回馈：前馈神经网络 有回路，其中反馈环路是可⾏的：递归神经网络  ⼀个简单的分类⼿写数字的⽹络 #  对于这六个数字504192，我们该如何实现一个分类⼿写数字的⽹络呢，可以将其分解为两个小问题：\n  504192是连续在一起的图像，首先可以将其分割成6个小图像，比如5 0 \u0026hellip;等\n  再对分割开的小图像进行分类，比如识别5\n  我们将使⽤⼀个三层神经⽹络来识别单个数字： 由于训练数据由$28*28$的⼿写数字的图像组成，所以:\n 输入层有784个神经元，因为$784 = 28 \\times 28$，输⼊像素是灰度级的，值为0:0表⽰⽩⾊，值为1:0表⽰⿊⾊，中间数值表⽰逐渐暗淡的灰⾊ 隐藏层⽤n来表⽰神经元的数量，我们将给n实验不同的数值。⽰例中⽤⼀个⼩的隐藏层来说明，仅仅包含$n = 15$个神经元 ⽹络的输出层包含有10个神经元，如果第一个输出神经元被激活，那么数字就是0，以此类推，从0到9  一个分类⼿写数字的⽹络，大概就可以这样实现。\n随机梯度下降算法 #  分类方案已经确定了，接下来第一步就是获取样本，我们将使⽤MNIST数据集，其包含有数以万计的连带着正确分类器的⼿写数字的扫描图像，下载好建立这样的目录结构，使用jupyter notebook开始吧，如果没有，请自行检索教程安装，代码目录如下：\npylab └── datasets └── MNIST_data ├── t10k-images-idx3-ubyte.gz ├── t10k-labels-idx1-ubyte.gz ├── train-images-idx3-ubyte.gz └── train-labels-idx1-ubyte.gz 然后使用 TensorFlow来读取数据，看看下载的数据集到底是什么样子，可以在mnist_data.ipynb中添加如下代码：\nfrom matplotlib import pyplot as plt from tensorflow.examples.tutorials.mnist.input_data import read_data_sets mnist = read_data_sets(\u0026#34;../datasets/MNIST_data/\u0026#34;, one_hot=True) 查看一些基本信息：\nprint(\u0026#34;Training data size: %s\u0026#34; % mnist.train.num_examples) print(\u0026#34;Validating data size: %s\u0026#34; % mnist.validation.num_examples) print(\u0026#34;Testing data size: %s\u0026#34; % mnist.test.num_examples) # 每张图片是长度为784的一维数组 print(len(mnist.train.images[0])) def display_digit(image): image = image.reshape([28,28]) plt.imshow(image, cmap=plt.get_cmap(\u0026#39;gray_r\u0026#39;)) plt.show() print(\u0026#34;样本真实数字为：%s\u0026#34; % list(mnist.train.labels[1]).index(1)) # 显示图像 display_digit(mnist.train.images[1]) # Output # Training data size: 55000 # Validating data size: 5000 # Testing data size: 10000 # 784 # 样本真实数字为：3 可以看到显示如下图像： 通过代码，我们知道，每个图像被分割成784个值存于一维数组中，看作⼀个$28 \\times 28 = 784$维的向量，每个向量就代表图像中单个像素的灰度值，假设我们用$y$表示output，也就是当我们输入一组样本数据x，那么y就是我们需要的结果，怎么从x得到y呢，我们需要一个函数$y= y(x)$。\n感知器为什么可以学习，因为它可以调整$b$和$w$来实现output的改变，这里，我们同样需要一个算法来让我们找到权重和偏置，从而使得$y= y(x)$可以拟合样本输入的x，为了量化我们如何实现这个⽬标，我们定义⼀个代价函数（有时被称为损失或⽬标函数）：\n$$ C(w,b) \\equiv \\frac{1}{2n} \\sum_x | y(x) - a|^2 $$\n w 表⽰所有的⽹络中权重的集合 b 是所有的偏置 n 是训练输⼊数据的个数 a 是表⽰当输⼊为x时输出的向量，可以理解为output  训练模型的过程就是优化代价函数的过程，Cost Function(代价函数) 越小，就代表模型拟合的越好，所以我们的目的是找出最⼩化权重和偏置的代价函数$C(w,b)$，其实这个就是我们平常用来评价回归算法的均方误差，也就是MSE。\n现在我们的目标很清晰，为了找出合适的权重和偏置值，我们需要让代价函数的值接近于0，在这个条件下我们就可以找出合适的权重和偏置值，我们将采⽤称为梯度下降的算法来达到这个⽬的。\n下面是我的推导过程：\n实现数字分类模型 #  终于，掌握了基本的概念，我们可以通过实战来实现一个数字分类模型，来看看，前面我们掌握的知识是怎么运用于实际的。\n书籍作者很贴心地实现了模型代码，仓库见neural-networks-and-deep-learning 官方git仓库，接下来我们一起来看看具体的实现代码吧。\n首先是实现一个Network类，\n#!/usr/bin/env python import random import numpy as np class Network(object): def __init__(self, sizes): \u0026#34;\u0026#34;\u0026#34; 初始化 :param sizes: 如果想要初始化一个层数为3，里面神经元数量分别为：2,3,1 的神经网络，那么 sizes = [2，3，1] \u0026#34;\u0026#34;\u0026#34; self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] 前面已经说了S型神经元，那么此时激活函数实现如下，更加具体的代码请参考network.py：\ndef sigmoid(z): \u0026#34;\u0026#34;\u0026#34;The sigmoid function.\u0026#34;\u0026#34;\u0026#34; return 1.0/(1.0+np.exp(-z)) 代码已经准备就绪，下面可以开始进行模型训练：\nimport mnist_loader, network training_data, validation_data, test_data = mnist_loader.load_data_wrapper() net = network.Network([784, 30, 10]) net.SGD(list(training_data), 30, 10, 3.0, test_data=list(test_data)) # Output Epoch 0: 9023 / 10000 Epoch 1: 9222 / 10000 Epoch 2: 9312 / 10000 Epoch 3: 9312 / 10000 Epoch 4: 9383 / 10000 Epoch 5: 9421 / 10000 Epoch 6: 9432 / 10000 Epoch 7: 9421 / 10000 Epoch 8: 9454 / 10000 Epoch 9: 9447 / 10000 看结果，十次迭代后，轻轻松松突破94%\n参考 #   文中涉及代码 Neural Networks and Deep Learning Neural Networks and Deep Learning 中文版 神经网络浅讲：从神经元到深度学习 neural-networks-and-deep-learning 官方git仓库  搞定收工，有兴趣欢迎关注我的公众号：\n"});index.add({'id':2,'href':'/ml_book/docs/02_nndl/02.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/','title':"02.反向传播算法如何工作",'section':"第二部分：探索 NNDL",'content':"反向传播算法如何工作 #  前面一章，我们通过了梯度下降算法实现目标函数的最小化，从而学习了该神经网络的权重和偏置，但是有一个问题并没有考虑到，那就是如何计算代价函数的梯度，本章的重点就是介绍计算这些梯度的快速算法——反向传播算法，首先先介绍一下上图中以及下面文章中会出现的一些数学符号：\n $L$ : 表示网络层数 $b_j^l$ : 表示第$l$层的第$j$个神经元的偏置 $a_j^l$ : 表示第$l$层的第$j$个神经元的激活值 $w_{j k}^{l}$ : 表示从$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接上的权重 $w^l$ : 权重矩阵，其中元素表示$l-1$层连接到$l$层神经元的权重 $b^l$ : 第$l$层神经元的偏置向量 $z^l$ : 第$l$层神经元的带权输入向量 $a^l$ : 第$l$层每个神经元激活值构成的向量 $\\delta_{j}^{l}$ : 第$l$层第$j$个神经元的**误差**  本篇文章是公式重灾区，但是涉及的知识并不高级，这也说明一个道理：\n 很多看似显而易见的想法只有在事后才变得显而易见。\n 热⾝：神经⽹络中使⽤矩阵快速计算输出的⽅法 #  通过第一章，我们已经知道每个神经元的激活值的计算方法，根据上面的公式，我们可以得出$a_j^l$的表达方式：\n$$ a_{j}^{l}=\\sigma\\left(\\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\\right) $$\n举个例子：$a_3^2$表示第二层的第三个神经元的激活值，那么该输出值怎么同上一层的输出值以及权重关联起来的呢，根据激活值的计算公式，我们可以得出：\n$$ \\begin{aligned} a_3^2 \u0026amp;= \\sigma(w_{3 1}^{2} a_1^1 + w_{3 2}^{2} a_2^1+ w_{3 3}^{2} a_3^1 + b_3^2 ) \\\\ \u0026amp;= \\sigma\\left(\\sum_{k=1}^3 w_{3 k}^2 a_{k}^1 + b_3^2 \\right) \\end{aligned} $$\n可以看到例子中的结果满足表达式，接下来，让我们将表达式改成向量形式：\n$$ \\begin{aligned} a^{l} \u0026amp;= \\sigma\\left(w^{l} a^{l-1}+b^{l}\\right) \\\\ \u0026amp;= \\sigma\\left(z^l \\right) \\end{aligned} $$\n这个式子是正确的么，我们实际根据第一层到第二层的计算来看看：\n首先定义第一层的激活函数输出值向量$a^1$:\n$$ {a^1} = \\left[ \\begin{array}{c}{a_1^1} \\ {a_2^1} \\ {a_3^1} \\end{array}\\right] $$\n然后是第一层神经元连接到第二层神经元的权重矩阵：\n$$ {w^2} = \\left[ \\begin{array}{c}{w_{1 1}^2,w_{1 2}^2,w_{1 3}^2} \\ {w_{2 1}^2,w_{2 2}^2,w_{2 3}^2} \\ {w_{3 1}^2,w_{3 2}^2,w_{3 3}^2} \\ {w_{4 1}^2,w_{4 2}^2,w_{4 3}^2} \\end{array}\\right] $$\n同理，第二层神经元的偏置向量：\n$$ {b^2} = \\left[ \\begin{array}{c}{b_1^2} \\ {b_2^2} \\ {b_3^2} \\ {b_4^2} \\end{array}\\right] $$\n我们的目标是求得第二层神经元激活值构成的向量$a^2$:\n$$ {a^2} = \\left[ \\begin{array}{c}{a_1^2} \\ {a_2^2} \\ {a_3^2} \\ {a_4^2} \\end{array}\\right] $$\n激活值计算如下：\n$$ \\begin{aligned} a^2 \u0026amp;= \\sigma(w^2a^1 + b^2) \\\\\u0026amp;= \\sigma\\left(\\left[ \\begin{array}{c}{w_{1 1}^2,w_{1 2}^2,w_{1 3}^2} \\\\ {w_{2 1}^2,w_{2 2}^2,w_{2 3}^2} \\ {w_{3 1}^2,w_{3 2}^2,w_{3 3}^2} \\\\ {w_{4 1}^2,w_{4 2}^2,w_{4 3}^2} \\end{array}\\right] \\left[ \\begin{array}{c}{a_1^1} \\\\ {a_2^1} \\\\ {a_3^1} \\end{array}\\right] + \\left[ \\begin{array}{c}{b_1^2} \\\\ {b_2^2} \\\\ {b_3^2} \\\\ {b_4^2} \\end{array}\\right]\\right) \\end{aligned} $$\n$$ {a^2} = \\left[ \\begin{array}{c}{a_1^2} \\\\ {a_2^2} \\\\ {a_3^2} \\\\ {a_4^2} \\end{array}\\right]=\\sigma\\left(\\left[ \\begin{array}{c}{w_{1 1}^2 a_1^1+w_{1 2}^2 a_2^1+w_{1 3}^2 a_3^1}+b_1^2 \\\\ {w_{2 1}^2 a_1^1+w_{2 2}^2 a_2^1+w_{2 3}^2 a_3^1}+b_2^2 \\\\ {w_{3 1}^2 a_1^1+w_{3 2}^2 a_2^1+w_{3 3}^2 a_3^1}+b_3^2 \\\\ {w_{4 1}^2 a_1^1+w_{4 2}^2 a_2^1+w_{4 3}^2 a_3^1}+b_4^2 \\end{array}\\right]\\right) $$\n可以看到$a_3^2$的值和前面第一次举例子算出来的值一致。\n关于代价函数的两个假设 #  我们以均方误差得出的代价函数如下：\n$$ C=\\frac{1}{2 n} \\sum_{x}\\left|y(x)-a^{L}(x)\\right|^{2} $$\n公式说明：\n $y(x)$是目标输出 $a^{L}(x)$是当输入是$x$时候网络输出的激活值向量  好了，为了应⽤反向传播，我们需要对代价函数 $C$ 做出什么样的前提假设呢？\n第一：代价函数可以被写成⼀个在每个训练样本 $x$ 上的代价函数 $C_x$ 的均值:\n$$ C=\\frac{1}{n} \\sum_{x} C_{x} $$\n第⼆：代价可以写成神经⽹络输出的函数：\n为对于⼀个单独的训练样本 $x$ 其⼆次代价函数可以写作：\n$$ C=\\frac{1}{2}\\left|y-a^{L}\\right|^{2}=\\frac{1}{2} \\sum_{j}\\left(y_{j}-a_{j}^{L}\\right)^{2} $$\n反向传播的四个基本方程 #  反向传播其实是对权重和偏置变化影响代价函数过程的理解，最终极的含义其实就是计算偏导数 $\\partial C / \\partial w_{j k}^{l}$ 和 $\\partial C / \\partial b_{j}^{l}$。\n为了计算这些值，我们引入一个中间量$\\delta_{j}^{l}$ ，其表示的是第$l$层第$j$个神经元的**误差**，其中$\\delta^l$表示第$l$层的误差向量，对于这个误差，我们应该怎样表示呢：\n$$ \\delta_{j}^{l} \\equiv \\frac{\\partial C}{\\partial z_{j}^{l}} $$\n接下来要做的就是将这些误差和 $\\partial C / \\partial w_{j k}^{l}$ 和 $\\partial C / \\partial b_{j}^{l}$ 联系起来，解决方案就是反向传播基于四个基本⽅程：\n输出层误差的⽅程 #  输出层误差的⽅程，$\\delta^L$ ： 每个元素定义如下：\n$$ \\delta_{j}^{L}=\\frac{\\partial C}{\\partial a_{j}^{L}} \\sigma^{\\prime}\\left(z_{j}^{L}\\right) $$\n矩阵形式重写⽅程：\n$$ \\delta^{L}=\\nabla_{a} C \\odot \\sigma^{\\prime}\\left(z^{L}\\right) $$\n其中$\\nabla_{a}$就是梯度向量，其元素就是偏导数$\\partial C / \\partial a_{j}^{L}$的所有元素，以上述二次代价函数为例：\n$$ C_{x}=\\frac{1}{2}\\left|y-a^{L}\\right|^{2} $$\n可以得出：\n$$ \\nabla_{a} C=\\left(a^{L}-y\\right) $$\n因此方程的矩阵形式可以改成：\n$$ \\delta^{L}=\\left(a^{L}-y\\right) \\odot \\sigma^{\\prime}\\left(z^{L}\\right) $$\n推导过程如下：\n使用下一层的误差表示当前层的误差 #  $$ \\delta^{l}=\\left(\\left(w^{l+1}\\right)^{T} \\delta^{l+1}\\right) \\odot \\sigma^{\\prime}\\left(z^{l}\\right) $$\n推导过程如下：\n代价函数关于⽹络中任意偏置的改变率 #  $$ \\frac{\\partial C}{\\partial b_{j}^{l}}=\\delta_{j}^{l} $$\n推导过程如下：\n代价函数关于任何⼀个权重的改变率 #  $$ \\frac{\\partial C}{\\partial w_{j k}^{l}}=a_{k}^{l-1} \\delta_{j}^{l} $$\n推导过程如下：\n反向传播的四个基本公式，靠着一个链式法则，就全都推下来了，没有什么难度\n反向传播算法 #  反向传播算法给出了一种计算代价函数梯度的方法，算法描述如下：\n 输入特征x：为输⼊层设置对应的激活值$a^1$ 前向传播：对每个$$l=2,3,\u0026hellip;,L$$计算相应的$$z^l$$和$$a^l$$  $$z^l=w^{l} a^{l-1}+b^{l}$$ $$a^l=\\sigma(z^l)$$   输出层误差：$$\\delta^{L}=\\nabla_{a} C \\odot \\sigma^{\\prime}\\left(z^{L}\\right)$$ 反向误差传播：对每个$$l=L-1,L-2,\u0026hellip;,2$$，计算$$\\delta^{l}=\\left(\\left(w^{l+1}\\right)^{T} \\delta^{l+1}\\right) \\odot \\sigma^{\\prime}\\left(z^{l}\\right)$$ 输出：代价函数的梯度由$$\\frac{\\partial C}{\\partial w_{j k}^{l}}=a_{k}^{l-1} \\delta_{j}^{l}$$和$$\\frac{\\partial C}{\\partial b_{j}^{l}}=\\delta_{j}^{l}$$得出  反向传播：全局观 #  假设我们已经对⼀些⽹络中的 $w_{j k}^l$ 做⼀点⼩⼩的变动 $$\\Delta w_{j k}^{l}$$\n显然，这样会造成输出激活值的改变：\n然后，会让下一层所有的激活值产生改变：\n接着，这些改变都将影响到⼀个个下⼀层，到达输出层，最终影响代价函数：\n根据求导的思想，我们可以得出下面公式：\n$$ \\Delta C \\approx \\frac{\\partial C}{\\partial w_{j k}^{l}} \\Delta w_{j k}^{l} $$\n我们知道，$$\\Delta w_{j k}^{l}$$造成了第$$l$$层的第$$j$$神经元的激活值的变化$$\\Delta a_{j}^{l}$$，这个变化由下⾯的公式给出：\n$$ \\Delta a_{j}^{l} \\approx \\frac{\\partial a_{j}^{l}}{\\partial w_{j k}^{l}} \\Delta w_{j k}^{l} $$\n$$\\Delta a_{j}^{l}$$的变化会造成下一层所有神经元激活值的变化，我们聚焦到其中⼀个激活值上看看影响的情况，不防设$$a_q^{l+1}$$：\n实际上，这会导致下⾯的变化：\n$$ \\Delta a_{q}^{l+1} \\approx \\frac{\\partial a_{q}^{l+1}}{\\partial a_{j}^{l}} \\Delta a_{j}^{l} $$\n我们已经知道$$\\Delta a_{j}^{l} \\approx \\frac{\\partial a_{j}^{l}}{\\partial w_{j k}^{l}} \\Delta w_{j k}^{l}$$，我们可以得到：\n$$ \\Delta a_{q}^{l+1} \\approx \\frac{\\partial a_{q}^{l+1}}{\\partial a_{j}^{l}} \\frac{\\partial a_{j}^{l}}{\\partial w_{j k}^{l}} \\Delta w_{j k}^{l} $$\n就这样一直传播下去，最终将所有的影响汇聚到输出层代价的变化，假设$$a_{j}^{l}, a_{q}^{l+1}, \\ldots, a_{n}^{L-1}, a_{m}^{L}$$，那么结果的表达式就是：\n$$ \\Delta C \\approx \\frac{\\partial C}{\\partial a_{m}^{L}} \\frac{\\partial a_{m}^{L}}{\\partial a_{n}^{L-1}} \\frac{\\partial a_{n}^{L-1}}{\\partial a_{p}^{L-2}} \\ldots \\frac{\\partial a_{q}^{l+1}}{\\partial a_{j}^{l}} \\frac{\\partial a_{j}^{l}}{\\partial w_{j k}^{l}} \\Delta w_{j k}^{l} $$\n影响输出层代价的权重值有很多，所以我们需要进行求和：\n$$ \\Delta C \\approx \\sum_{m n p_{\\ldots q}} \\frac{\\partial C}{\\partial a_{m}^{L}} \\frac{\\partial a_{m}^{L}}{\\partial a_{n}^{L-1}} \\frac{\\partial a_{n}^{L-1}}{\\partial a_{p}^{L-2}} \\ldots \\frac{\\partial a_{q}^{l+1}}{\\partial a_{j}^{l}} \\frac{\\partial a_{j}^{l}}{\\partial w_{j k}^{l}} \\Delta w_{j k}^{l} $$\n因为：\n$$ \\Delta C \\approx \\frac{\\partial C}{\\partial w_{j k}^{l}} \\Delta w_{j k}^{l} $$\n带入上面式子，得出：\n$$ \\begin{aligned} \\frac{\\partial C}{\\partial w_{j k}^{l}}\u0026amp;=\\sum_{m n p \\ldots q} \\frac{\\partial C}{\\partial a_{m}^{L}} \\frac{\\partial a_{m}^{L}}{\\partial a_{n}^{L-1}} \\frac{\\partial a_{n}^{L-1}}{\\partial a_{p}^{L-2}} \\cdots \\frac{\\partial a_{q}^{l+1}}{\\partial a_{j}^{l}} \\frac{\\partial a_{j}^{l}}{\\partial w_{j k}^{l}} \\\\\u0026amp;=a_{k}^{l-1} \\delta_{j}^{l} \\end{aligned} $$\n想起一句歌词，又回到最初的起点，我们竟然就是在做反向传播，神奇。\n参考 #   Neural Networks and Deep Learning Neural Networks and Deep Learning 中文版 知乎上另外一篇笔记  搞定收工，有兴趣欢迎关注我的公众号：\n"});index.add({'id':3,'href':'/ml_book/docs/02_nndl/03.%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%B3%95/','title':"03.改进神经⽹络的学习⽅法",'section':"第二部分：探索 NNDL",'content':"改进神经⽹络的学习⽅法 #   万丈高楼平地起，反向传播是深度学习这栋大厦的基石，所以在这块花多少时间都是值得的\n 前面一章，我们深入理解了反向传播算法如何工作，本章的主要目的是改进神经网络的学习方法，本章涉及的技术包括：\n 交叉熵代价函数 四种称为规范化的⽅法（L1 和 L2 规范化，弃权和训练数据的⼈为扩展） 更好的权重初始化⽅法 如何选择神经网络的超参数  交叉熵代价函数 #  通过前面的学习，我们知道，神经网络一直在努力地让代价函数变小，现在我们抛出一个问题，代价函数从初始值到我们理想状态的值，这个过程所消耗的时间是怎样的情况呢？\n对于人类来说，我们犯比较明显的错误的时候，改正的效果也很明显，神经网络也是这样的么？让我们通过下面的一个小例子来详细看看：\n假设有这样一个神经元，我们希望当输入为1的时候，输出为0，让我们通过梯度下降的方式来训练权重和偏置，⾸先将权重和偏置初始化为0.6 和 0.9，前面我们说过，S型神经元的输出计算方式为：\n输⼊$x_{1},x_{2},\u0026hellip;,x_{j}$，权重为$w_{1},w_{2},\u0026hellip;,w_{j}$，和偏置b的S型神经元的输出是：\n$$ \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)} $$\n将0.6 和 0.9带入，即可得到输出为：0.82，很显然和目标0相差甚远，那么就只能继续寻找最优解了，随着迭代期的增加，神经元的输出、权重、偏置和代价的变化如下⾯⼀系列图形所⽰：\n可以看到，200次迭代后，输出已经满足我们的需求了，毕竟只要小于0.5我们都可以归为0，接下来再将初始权重和偏置都设置为2.0，此时初始输出为0.98，这是和⽬标值的差距相当⼤的，现在看看神经元学习的过程：\n这次可以明显地看到迭代150次左右没有很明显的变化，随后速度就快了起来。\n这个例子引出了一个问题，包括在其他一般的神经网络中也会出现，为何会学习缓慢呢？我们在训练中应该怎么避免这个情况？思考一下，问题的原因其实很简单\n 权重和偏置是根据梯度下降的形式来更新其对应的值，学习缓慢就意味着代价函数的偏导数很小\n 让我们从数学公式的角度来理解这句话和上面的图，对于目前提供的单个样例的输入和输出：\n $x=1$ $y=0$  再结合前面第一章讲的代价函数的定义：\n$$ C(w,b) \\equiv \\frac{1}{2n} \\sum_x | y(x) - a|^2 $$\n w 表⽰所有的⽹络中权重的集合 b 是所有的偏置 n 是训练输⼊数据的个数 a 是表⽰当输⼊为x时输出的向量，可以理解为output  类比一下，此时的代价函数为：\n$$ C=\\frac{(y-a)^{2}}{2} $$\n接下来就是求权重和偏置的偏导数，推导之前说明一下前面的公式：\n $x=1$ $y=0$ $a=\\sigma(z)$ $z=wx+b$  推导过程如下：\n从推导结果可以看到偏导数的值和$\\sigma^{\\prime}\\left(z\\right)$成正比，回顾下前面讲的S型神经元中激活函数的图像：\n我们可以从这幅图看出，当神经元的输出接近1的时候，曲线变得相当平，所以$\\sigma^{\\prime}\\left(z\\right)$就很小，同时也可以得出代价函数C权重和偏置的偏导数也很小，这就是训练速度缓慢的原因\n再看上面代价函数值和迭代次数的图，中间部分非常陡峭，前后非常平缓，是不是非常契合$\\sigma^{\\prime}\\left(z\\right)$的变化的？类似其他的神经网络，也是这样的原因造成训练缓慢，使用均方误差作为代价函数还有个缺点就是容易陷入局部最优解的沼泽中\n引⼊交叉熵代价函数 #  我们已经抛出了神经网络训练缓慢的原因，接下来就是考虑如何解决这个问题\n 研究表明，我们可以通过使⽤交叉熵代价函数来替换⼆次代价函数\n 为了理解什么是交叉熵，我们稍微改变⼀下之前的简单例⼦；假设，我们现在要训练⼀个包含若⼲输⼊变量的的神经元，输⼊$x_{1},x_{2},\u0026hellip;,x_{j}$，权重为$w_{1},w_{2},\u0026hellip;,w_{j}$，和偏置b：\n相关说明：\n $a=\\sigma(z)$ $z=\\sum_{j} w_{j} x_{j}+b$  我们如下定义这个神经元的交叉熵代价函数：\n$$ C=-\\frac{1}{n} \\sum_{x}[y \\ln a+(1-y) \\ln (1-a)] $$\n其中对数图像如下：\n从上面这个公式以及图像可以看出，交叉熵是有资格作为代价函数的：\n 总是非负的 实际输出值和目标输出值之间的差距越小，最终的交叉熵的值接越低  交叉熵作为代价函数更好的地方在于避免了学习速度下降的问题，那么到底是怎么解决的呢？很显然，解决问题的关键点在于$\\sigma^{\\prime}\\left(z\\right)$，话不多说，先求出代价函数针对$w$和$b$的偏导数再说：\n可以看到，$\\sigma^{\\prime}\\left(z\\right)$对权重学习的速度直接被消掉了，只和$\\sigma(z)-y$成正相关，也就是说输出和实际值的误差越大，学习的速率也就越快\n回过头来用交叉熵作为代价函数，看看此时的损失函数值和迭代次数的关系：\n通过对比可以看到此时的变化曲线有以下特点：\n 误差越大，学习速度越快 迭代周期明显变小  其推⼴到有很多神经元的多层神经⽹络上的交叉熵公式为：\n$$ C=-\\frac{1}{n} \\sum_{x} \\sum_{j}\\left[y_{j} \\ln a_{j}^{L}+\\left(1-y_{j}\\right) \\ln \\left(1-a_{j}^{L}\\right)\\right] $$\n交叉熵的含义？源⾃哪⾥？ #   交叉熵可以帮我们完善二次代价函数的缺点，但交叉熵到底是怎么来的呢？\n 在上一小节，我们有推导二次代价函数对于权重和偏置的偏导数，我们知道造成训练缓慢的原因在于$\\sigma^{\\prime}\\left(z\\right)$：\n$$ \\begin{aligned} \\frac{\\partial C}{\\partial w} \u0026amp;=(a-y) \\sigma^{\\prime}(z) x=a \\sigma^{\\prime}(z) \\\\ \\frac{\\partial C}{\\partial b} \u0026amp;=(a-y) \\sigma^{\\prime}(z)=a \\sigma^{\\prime}(z) \\end{aligned} $$\n如果将偏导数里面的$\\sigma^{\\prime}\\left(z\\right)$去掉，这样做会产生一个怎样的结果呢？\n$$ \\begin{aligned} \\frac{\\partial C}{\\partial w_{j}} \u0026amp;=x_{j}(a-y) \\ \\\\frac{\\partial C}{\\partial b} \u0026amp;=(a-y) \\end{aligned} $$\n对此⽅程关于a进⾏积分，得到：\n$$ C=-[y \\ln a+(1-y) \\ln (1-a)]+\\mathrm{constant} $$\n这是⼀个单独的训练样本$x$对代价函数的贡献，为了得到整个的代价函数，我们需要对所有的训练样本进⾏平均， 得到了：\n$$ C=-\\frac{1}{n} \\sum_{x}[y \\ln a+(1-y) \\ln (1-a)]+\\mathrm{constant} $$\n过度拟合和规范化 #   如何解决模型在训练过程中出现的过拟合问题\n 规范化 #  增加训练样本的数量是⼀种减轻过度拟合的⽅法，但是样本本身就是比较稀缺的资源\n还有其他的⼀下⽅法能够减轻过度拟合的程度吗？⼀种可⾏的⽅式就是降低⽹络的规模，然⽽，⼤的⽹络拥有⼀种⽐⼩⽹络更强的潜⼒，能使用大的网络更好\n我们可以使用规范化的技术来缓解过拟合问题，常见的手段是权重衰减（weight decay）或者L2 规范化，L2规范化的想法是增加⼀个额外的项到代价函数上，这个项叫做规范化项（统计学习方法中引入的结构风险），下⾯是规范化的交叉熵：\n$$ C=-\\frac{1}{n} \\sum_{x j}\\left[y_{j} \\ln a_{j}^{L}+\\left(1-y_{j}\\right) \\ln \\left(1-a_{j}^{L}\\right)\\right]+\\frac{\\lambda}{2 n} \\sum_{w} w^{2} $$\n公式前面一项是交叉熵的表达式，第二项表示的是所有权重平方的和（使用一个因子$\\lambda / 2 n$进行量化调整），其中$\\lambda\u0026gt;0$称为规范化参数。\n直觉地看，规范化的效果是让⽹络倾向于学习⼩⼀点的权重，其他的东西都⼀样的。⼤的权重只有能够给出代价函数第⼀项⾜够的提升时才被允许。换⾔之，规范化可以当做⼀种寻找⼩的权重和最⼩化原始的代价函数之间的折中。\n这两部分之前相对的重要性就由λ的值来控制了： λ越⼩，就偏向于最⼩化原始代价函数，反之，倾向于⼩的权重。\n为何规范化可以帮助减轻过度拟合 #  我们已经看到了规范化在实践中能够减少过度拟合了，这是令⼈振奋的，不过这背后的原因还不得⽽知！通常的说法是：\n ⼩的权重在某种程度上，意味着更低的复杂性，也就对数据给出了⼀种更简单却更强⼤解释，因此应该优先选择。\n 接下来我们利用一个简单的回归任务对上述解释进行研究，如下一个简单的数据集，我们为其建立模型：\n我们先考虑使用多项式来拟合数据：\n$$ y=a_{0} x^{9}+a_{1} x^{8}+\\ldots+a_{9} $$\n根据奥姆剃刀原则以及我们各自的经验，显然会选择下面的模型。以此类推，丛神经网络的角度，规范化得到的较小的权重应该会更好，实验也证明了这一点。\n 我们应当时时记住这⼀点，规范化的神经⽹络常常能够⽐⾮规范化的泛化能⼒更强，这只是⼀种实验事实（empirical fact）\n 规范化的其他技术 #  除了L2外还有很多规范化技术：\n L1规范化 弃权（Dropout） 人为增加训练样本  L1规范化：这个⽅法是在未规范化的代价函数上加上⼀个权重绝对值的和：\n$$ C=C_{0}+\\frac{\\lambda}{n} \\sum_{w}|w| $$\n L1规范化倾向于聚集⽹络的权重在 相对少量的⾼重要度连接上，⽽其他权重就会被驱使向0接近\n 弃权（Dropout）：弃权（Dropout）是⼀种相当激进的技术。和L1、L2规范化不同，弃权技术并不依赖对代价函数的修改。\n假设我们有⼀个训练数据x和对应的⽬标输出y。通常我们会通过在⽹络中前向传播x，然后进⾏反向传播来确定对梯度的贡献。使⽤弃权技术，这个过程就改了,我们会从随机（临时）地删除⽹络中的⼀半的隐藏神经元开始，同时让输⼊层和输出层的神经元保持不变\n⼈为扩展训练数据：数据量大到一定程度可以弥补算法的差距，没啥好说的。\n权重初始化 #  创建了神经⽹络后，我们需要进⾏权重和偏置的初始化：之前的⽅式就是根据独⽴⾼斯随机变量来选择权重和偏置，其被归⼀化为均值为0，标准差1\n 采用均值为0标准差为的⾼斯随机分布初始化这些权重$1 / \\sqrt{n_{\\mathrm{in}}}$\n 如何选择神经⽹络的超参数 #  直到现在，我们还没有解释对诸如学习速率η，正则化参数λ等等超参数选择的⽅法，目前只是给出那些效果很好的值而已。实践中，当你使⽤神经⽹络解决问题时，寻找好的超参数其实是比较困难的一件事情。\n本节会给出⼀些⽤于设定超参数的启发式想法，⽬的是帮你发展出⼀套⼯作流来确保 很好地设置超参数：\n 宽泛策略：  将复杂的问题简单化，比如先减少目标类别，简化网络结构 在问题简化后可以方便地进行对超参数进行调节，快速调试是哪个超参数有问题   学习速率：可以观察在某些学习速率下，迭代周期和训练代价的折线图变化情况，主要观察训练代价开始下降的学习率，学习速率可以变成可变的，在不同的迭代次数用不同的学习率 早停法：这是我个人用得比较多的方法，判断验证集的准确率以及训练代价，比如效果不再提升，就可以考虑停止迭代，输出对应模型 正则化参数：开始时不包含正则化（λ = 0.0），确定 η 的值，使⽤确定出来的 η，我们可以使⽤验证数据来选择好的 λ。从尝试 λ = 1.0 开始，然后根据验证集上的性能按照因⼦ 10 增加或减少其值。⼀旦我已经找到⼀个好的量级，你可以改进 λ 的值。这⾥搞定后，你就可以返回再重新优化 η。 自动技术：网格搜索  参考 #   Neural Networks and Deep Learning Neural Networks and Deep Learning 中文版  搞定收工，有兴趣欢迎关注我的公众号：\n"});index.add({'id':4,'href':'/ml_book/docs/02_nndl/04.%E7%A5%9E%E7%BB%8F%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/','title':"04.神经⽹络可以计算任何函数的可视化证明",'section':"第二部分：探索 NNDL",'content':"神经⽹络可以计算任何函数的可视化证明 #  本章其实和前面章节的关联性不大，所以大可将本章作为小短文来阅读，当然基本的深度学习基础还是要有的。\n主要介绍了神经⽹络拥有的⼀种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入$x$，其值$f(x)$（或者说近似值）是网络的输出，哪怕是多输入和多输出也是如此，我们大可直接得出一个结论：\n 不论我们想要计算什么样的函数，我们都确信存在⼀个神经⽹络（多层）可以计算它\n 试想以下这种普遍性代表着什么，我觉得代表着做更多可能的事情（将其看做计算一种函数）：\n 比如将中文翻译成英文 比如根据⼀个mp4视频⽂件⽣成⼀个描述电影情节并讨论表演质量的问题 \u0026hellip;  现实往往是残酷的，我们知道有这个网络存在，比如中文翻译成英文的网络，通常情况下想得往往不可得，网络在那里，但更可能我们得不到，怎么办？\n前面我们知道，我们通过学习算法来拟合函数，学习算法和普遍性的结合是⼀种有趣的混合，直到现在，本书⼀直是着重谈学习算法，到了本章，我们来看看普遍性，看看它究竟意味着什么。\n两个预先声明 #  在解释为何普遍性定理成⽴前，关于神经⽹络可以计算任何函数有两个预先声明需要注意一下：\n 这句话不是说⼀个⽹络可以被⽤来准确地计算任何函数，而是说，我们可以获得尽可能好的⼀个近似，通过增加隐藏元的数量，我们可以提升近似的精度，同时对于目标精度，我们需要确定精度范围：$|g(x)-f(x)|\u0026lt;\\epsilon$，其中$\\epsilon\u0026gt;0$ 按照上⾯的⽅式近似的函数类其实是连续函数，如果函数不是连续的，也就是会有突然、极陡的跳跃，那么⼀般来说⽆法使⽤⼀个神经⽹络进⾏近似，这并不意外，因为神经⽹络计算的就是输⼊的连续函数   普遍性定理的表述：包含⼀个隐藏层的神经⽹络可以被⽤来按照任意给定的精度来近似任何连续函数\n 接下来的内容会使⽤有两个隐藏层的⽹络来证明这个结果的弱化版本，在问题中会简要介绍如何通过⼀些微调把这个解释适应于只使⽤⼀个隐藏层的⽹络并给出证明。\n一个输入和一个输出的普遍性 #  先从一个简单的函数$f(x)$（即只有一个输入和一个输出）开始，我们将利用神经网络来近似这个连续函数：\n第一章我们就探讨过多层感知机实现异或，这次同样的，我们加入一个隐藏层就可以让函数舞动起来，比如下面这个有一个隐藏层、两个隐藏神经元的网络：\n第一步，暂时只考虑顶层的神经元，第一章也讲过S型神经元，所以输出范围类似上图右上角，重点看看这个S型函数，前面已经说过：\n$$ \\sigma(z) \\equiv 1 /\\left(1+e^{-z}\\right) $$\n其中：$z=wx+b$，参见右上角的图，让我们考虑一下几个情况：\n 当$x$不变，$b$逐渐增加的情况下，输出会在原来的基础上变大，图像会相对向左边运动，因为$w$没变，所以图像形状不会变   上述情况让$b$键减小，图像会右移，同样图像形状不变 当$b$不变，$w$减小，很显然，图像的陡峭程度会下降，反之亦然  下图是书中给出的图示：\n其实我们完全可以自己绘制这个过程，利用Python的matplotlib可以很好地完成这个事情：\nimport matplotlib.pyplot as plt import numpy as np def sigmoid(w, b, x): return 1.0 / (1.0 + np.exp(-(w * x + b))) def plot_sigmoid(w, b): x = np.arange(-2, 2, 0.1) y = sigmoid(w, b, x) plt.plot(x, y) 先看下$b$增减下图像的移动情况：\nplt.figure(12) plt.subplots_adjust(wspace=0.2, hspace=0.5) plt.subplot(221) # 绘制原始图像 plt.title(\u0026#34;w = 8 b = -4\u0026#34;) w, b = 8, -4 plot_sigmoid(w, b) plt.subplot(222) # b增加的图像 plt.title(\u0026#34;w = 8 b = 4\u0026#34;) w, b = 8, 4 plot_sigmoid(w, b) plt.subplot(223) plt.title(\u0026#34;w = 8 b = 4\u0026#34;) w, b = 8, 4 plot_sigmoid(w, b) plt.subplot(224) # b减小的图像 plt.title(\u0026#34;w = 8 b = 1\u0026#34;) w, b = 8, 1 plot_sigmoid(w, b) plt.show() 再看下$w$增减下图像的伸缩情况：\nplt.figure(12) plt.subplots_adjust(wspace=0.2, hspace=0.5) plt.subplot(221) # 绘制原始图像 plt.title(\u0026#34;w = 8 b = 4\u0026#34;) w, b = 8, 4 plot_sigmoid(w, b) plt.subplot(222) # w减小的图像 plt.title(\u0026#34;w = 3 b = 4\u0026#34;) w, b = 3, 4 plot_sigmoid(w, b) plt.subplot(223) plt.title(\u0026#34;w = 3 b = 4\u0026#34;) w, b = 3, 4 plot_sigmoid(w, b) plt.subplot(224) # w增加的图像 plt.title(\u0026#34;w = 105 b = 4\u0026#34;) w, b = 105, 4 plot_sigmoid(w, b) plt.show() 首先通过这两张图验证了上面的三点结论，最后可以看到我们得到的图像就像是一个阶跃函数\n为什么需要千方百计地引出阶跃函数出来，这是因为在输出层我们在将所有隐藏神经元的贡献值叠加在一起的时候，分析阶跃函数比S型函数容易。我们该怎么做？结合前面的经验，只要将$w$设置成一个比较大的值，然后通过修改$b$就可以左右移动来定义阶跃函数的位置\n思考下，阶跃发生的点在哪？让我们令$wx+b=0$，即可得出阶跃发生的点可以用$s=-b/w$进行表示，现在我们就可以使用$s$来极大简化我们描述神经元的方式\n目前为止我们专注于仅仅从顶部隐藏神经元输出，让我们看看整个网络的行为，尤其，我们假设隐藏神经元在计算以阶跃点$s_1$（顶部神经元）和$s_2$（底部神经元参数化的阶跃函数，它们各自有输出权重$w_1$和$w_2$：\n为何隐藏层的加权输出如上图，这理解一下，隐藏层的两个神经元的输出可以想象成阶跃函数，那么:\n x小于0.4，输出肯定是0 x大于0.6，输出就是1.8 x介于两者之间，输出就是0.6  接下来，我们进行这样一个设置：\n说明一下：\n x小于0.4，输出肯定是0 x大于0.6，输出还是0 x介于两者之间，输出就是0.8  这边主要得出的一个结论是，我们可以通过$w_1,w_2$来定义加权输出图像中凸起的位置和高度，为了减少混乱，用一个参数$h$表示高度：\n 现在我们脑中应该有个清晰的概念，那就是对于神经元的加权输出组合$\\sum_{j} w_{j} a_{j}$，我们可以通过对$s$和$h$的调整来控制输出函数，从而让加权输出变成我们心目中的输出\n 好了，说明结束，接下来看看最开始绘制出来而函数：\n这个函数的表达式为：\n$$ f(x)=0.2+0.4 x^{2}+0.3 x \\sin (15 x)+0.05 \\cos (50 x) $$\n现在面临的问题是使用一个神经网络来计算它，前面我们着重分析了隐藏神经元输出的加权组合$\\sum_{j} w_{j} a_{j}$，但是要注意，虽然我们经过参数的调整，得到了我们想要的目标函数，此时这个函数是隐藏神经元输出的加权组合$\\sum_{j} w_{j} a_{j}$，但是实际上网络的输出是：$\\sigma(\\sum_{j} w_{j} a_{j}+b)$，也就是说：\n 第一步：我们将隐藏神经元的加权组合$\\sum_{j} w_{j} a_{j}$近似成了$f(x)$ 第二步：但是神经网络的输出却是$\\sigma(f(x))$  $\\sigma(f(x))$的函数输出和$f(x)$的函数输出那可是大相径庭\n 我们控制住了隐藏神经元输出的加权组合$\\sum_{j} w_{j} a_{j}$，但是没有控制住网络的输出是：$\\sigma(\\sum_{j} w_{j} a_{j}+b)$\n 那么此时我们面临的问题就是怎么让$\\sigma(\\sum_{j} w_{j} a_{j}+b)$近似于$f(x)$\n在上述情况下，可操作性的无非就是在加权输出上面做文章，所以我们可以设计一个神经网络，其隐藏层有个加权输出：\n$$ \\sigma^{-1} \\circ f(x) $$\n其中$\\sigma^{-1}$是$\\sigma$的反函数\n很容易可以得出$\\sigma^{-1} \\circ f(x)$的结果输入到$\\sigma$中输出还是$f(x)$，所以我们控制住了$\\sigma(\\sum_{j} w_{j} a_{j}+b)$\n接下来就是调节参数直到拟合到我们满意的程度（⽬标函数和⽹络实际计算函数的平均偏差来衡量）\n多个输入变量 #  上一节已经说明了一个输入和一个输出的普遍性，接下来可以尝试考虑多个输入变量的情况，假设有两个输入：$x,y$，分别对应权重$w_1,w_2$，以及一个神经元上的偏置$b$，看看他们如何影响神经元的输出：\n 思考：将$w_2$设置为0，再不断调整$w1$，输出会怎样变化？\n 首先，输出不受输入$y$影响，其次，按照上一节的惯性，图像应该是会变陡峭，会接近到类似阶跃函数，不同的是此时图像是三维的：\n和上面一样，我们可以通过修改偏置的位置来设置移动阶跃点：\n$$ s_{x} \\equiv-b / w_{1} $$\n凸起的期望高度用相应的权重$h$表示，可以通过分别设置x或y输入神经元的权重为0来分别控制y或x方向的凹凸函数\n让我们考虑当我们叠加两个凹凸函数时会发⽣什么，⼀个沿$x$⽅向，另⼀个沿$y$⽅向，两者都有⾼度$h$：\n试着改变参数h，正如你能看到，这引起输出权重的变化，以及x和y上凹凸函数的⾼度：\n我们构建的有点像是塔形函数，如果我们能构建这样的塔型函数，那么我们能使⽤它们来近似任意的函数，仅仅通过在不同位置累加许多不同⾼度的塔\n以此类推，三个变量的四位函数也不在话下，在发散一下思维，$m$维也可以用完全相同的思想来实现。\nS型神经元的延伸 #  我们已经证明了由$S$型神经元构成的⽹络可以计算任何函数，若输入为: $x_1, x_2 \u0026hellip;x_j$，输出为：\n$$ \\sigma\\left(\\sum_{j} w_{j} x_{j}+b\\right) $$\n其中$\\sigma$是$S$型函数，函数图像如下：\n如果我们考虑⼀个不同类型的神经元，它使⽤其它激活函数，⽐如如下的$s(z)$，会怎样？\n此时神经元的输出为：\n$$ s\\left(\\sum_{j} w_{j} x_{j}+b\\right) $$\n同样的，我们依然可以利用这个函数来通过对权重和偏置的修改来得到一个阶跃函数的近似：\n这样我们可以和前面一样来推出任何目标函数，当然，这要求$s(z)$具有以下条件：\n $s(z)$ 在 $z \\rightarrow-\\infty$ 和 $z \\rightarrow \\infty$时是定义明确的 这两个界限是在我们的阶跃函数上取的两个值，并且这两个界限彼此不同，否则就只能一马平川了哈哈  修补阶跃函数 #  我们用神经元近似阶跃函数的时候，实际上有一个很窄的故障窗口，如下图说明，在这⾥函数会表现得和阶跃函数⾮常不同：\n其实理论上来说，只要权重够大，情况可以随着变好，刚刚是从函数本身出发，避免拟合的函数有问题，换个角度，我们可以从拟合的函数出发，我们利用$1/M$的神经元来模拟目标函数的$1/M$，其实也是变相地将目标函数的权重变大，从而达到效果\n结论 #   普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许多现实世界问题的函数\n 本章主要解答了是否使⽤⼀个神经⽹络可以计算任意特定函数的问题，这个自然是的。理论上我们只要两个隐藏层就可以计算任何函数，但深度学习又是什么情况呢？\n正如在第⼀章中表明过，深度⽹络有⼀个分级结构，使其尤其适⽤于学习分级的知识，这看上去可⽤于解决现实世界的问题，我个人觉得是单层虽然能拟合各种目标函数，这没错，但是一样的数据，单层网络并不能找到理想中的那个函数，虽然有可能对目前的数据拟合得很好，但是使用深度网络却可以更好地提升泛化能力。\n参考 #   Neural Networks and Deep Learning Neural Networks and Deep Learning 中文版  搞定收工，有兴趣欢迎关注我的公众号：\n"});index.add({'id':5,'href':'/ml_book/docs/02_nndl/05.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83/','title':"05.深度神经⽹络为何很难训练",'section':"第二部分：探索 NNDL",'content':"深度神经⽹络为何很难训练 #  上一章提到了神经网络的一种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入得到一个近似的输出。\n 普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许多现实世界问题的函数\n 而且理论上我们只要一个隐藏层就可以计算任何函数，第一章我们就用如下的网络结构完成了一个手写字识别的模型：\n这时候，大家心中可能都会有这样一个想法，如果加大网络的深度，模型的识别准确率是否会提高？\n随即我们会基于反向传播的随机梯度下降来训练神经网络，但实际上这会产生一些问题，因为我们的深度神经网络并没有比浅层网络好很多。那么此处就引出了一个问题，为什么深度神经网络相对训练困难？\n仔细研究会发现：\n  如果网络后面层学习状况好的时候，前面的层次经常会在训练时停滞不前\n  反之情况也会发生，前面层训练良好，后面停滞不前\n  实际上，我们发现在深度神经⽹络中使⽤基于梯度下降的学习⽅法本⾝存在着内在不稳定性，这种不稳定性使得先前或者后⾯神经网络层的学习过程阻滞。\n消失的梯度问题 #  我们在第一章识别手写字曾经以MNIST数字分类问题做过示例，接下来我们同样通过这个样例来看看我们的神经网络在训练过程中究竟哪里出了问题。\n简单回顾一下，之前我们的训练样本路径为/pylab/datasets/mnist.pkl.gz ，相关案例代码在pylab都可以找到（我稍做了改动以支持Python3）。\nimport mnist_loader import network2 training_data, validation_data, test_data = mnist_loader.load_data_wrapper() # 输入层 784 # 隐藏层 30 # 输出层 10 sizes = [784, 30, 10] net = network2.Network(sizes=sizes) # 随机梯度下降开始训练 net.SGD( training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True, ) \u0026#34;\u0026#34;\u0026#34; Epoch 0 training complete Accuracy on evaluation data: 9280 / 10000 Epoch 1 training complete Accuracy on evaluation data: 9391 / 10000 ...... Epoch 28 training complete Accuracy on evaluation data: 9626 / 10000 Epoch 29 training complete Accuracy on evaluation data: 9647 / 1000 \u0026#34;\u0026#34;\u0026#34; 最终我们得到了分类的准确率为 96.47%，如果加深网络的层数会不会提升准确率呢？我们分别尝试一下几种情况：\n# 准确率 96.8% net = network2.Network([784, 30, 30, 10]) # 准确率 96.42% net = network2.Network([784, 30, 30, 30, 10]) # 准确率 96.28% net = network2.Network([784, 30, 30, 30, 30, 10]) 这说明一种情况，尽管我们加深神经网络的层数以让其学到更复杂的分类函数，但是并没有带来更好的分类表现（但也没有变得更差）。\n那么为什么会造成这种情况，这是我们接下来需要思考的问题。可以考虑先假设额外的隐藏层的确能够在原理上起到作⽤，问题是我们的学习算法没有发现正确地权值和偏置。\n下图（基于[784, 30, 30, 10]网络）表⽰了每个神经元权重和偏置在神经⽹络学习时的变化速率，图中的每个神经元有⼀个条形统计图，表⽰这个神经元在⽹络进⾏学习时改变的速度。更⼤的条意味着更快的速度，而小的条则表⽰变化缓慢。\n可以发现，第⼆个隐藏层上的条基本上都要⽐第⼀个隐藏层上的条要⼤；所以，在第⼆个隐藏层的神经元将学习得更加快速。这并不是巧合，前⾯的层学习速度确实低于后⾯的层。\n我们可以继续观察学习速度的变化，下方分别是2~4个隐藏层的学习速度变化图：\n同样的情况出现了，前⾯的隐藏层的学习速度要低于后⾯的隐藏层。这⾥，第⼀层的学习速度和最后⼀层要差了两个数量级，也就是⽐第四层慢了 100 倍。\n我们可以得出一个结果，在某些深度神经⽹络中，在我们隐藏层反向传播的时候梯度倾向于变小；这意味着在前⾯的隐藏层中的神经元学习速度要慢于后⾯的隐藏层，这个现象也被称作是消失的梯度问题。\n导致梯度消失的原因 #   核心原因在于深度神经网络中的梯度不稳定性，会造成前面层中梯度消失或者梯度爆炸\n 看下面这个既简单的深度神经网络，每⼀层都只有⼀ 个单⼀的神经元：\n首先回顾几个计算公式：\n 第$j$个神经元的输出：$a_j = \\sigma(z_j)$ 其中$\\sigma$就是Sigmoid函数 $z_j = w_j*a_{j-1}+b_j$  让我们通过公式追踪一下$b_1$的变化趋势： $$ \\begin{aligned} \\frac{\\partial C}{\\partial b_{1}} = \\frac{\\partial C}{\\partial a_{4}} \\times \\frac{\\partial a_{4}}{\\partial z_{4}} \\times \\frac{\\partial z_{4}}{\\partial a_{3}} \\times \\frac{\\partial a_{3}}{\\partial z_{3}} \\times \\frac{\\partial z_{3}}{\\partial a_{2}} \\times \\frac{\\partial a_{2}}{\\partial z_{2}} \\times \\frac{\\partial z_{2}}{\\partial a_{1}} \\times \\frac{\\partial a_{1}}{\\partial z_{1}} \\times \\frac{\\partial z_{1}}{\\partial b_{1}} \\end{aligned} $$ 以小见大一下，对于上述式子，可以做一下拆解：\n  $a_4 = \\sigma(z_4) = \\sigma(w_4*a_3 + b_4)$\n  $\\frac{\\partial a_4}{\\partial z_4} = \\frac{\\partial \\sigma(z_4)}{\\partial z_4}=\\sigma^{\\prime}\\left(z_4\\right)$\n  $\\frac{\\partial \\sigma(z_4)}{\\partial a_3} = \\frac{\\partial \\sigma(w_4*a_3 + b_4)}{\\partial a_3} = w_4$\n  $\\frac{\\partial z_1}{\\partial b_1} = \\frac{\\partial \\sigma(w_1*a_0 + b_1)}{\\partial b_1} = 1$\n  故： $$ \\begin{aligned}\\frac{\\partial C}{\\partial b_{1}} = \\frac{\\partial C}{\\partial a_{4}} \\times \\sigma^{\\prime}(z_4) \\times w_4 \\times \\sigma^{\\prime}(z_3) \\times w_3 \\times \\sigma^{\\prime}(z_2) \\times w_2 \\times \\sigma^{\\prime}(z_1) \\times 1 \\end{aligned} $$ 其实可以直观地看出上述表达式是一系列如$w_j\\sigma^\\prime(z_j)$的乘积，其中有sigmoid的导数，我们可以观察一下$\\sigma^\\prime(x)$的函数图像（$\\sigma^\\prime(x) = \\sigma(x)(1 - \\sigma(x))$）:\n绘图代码：\nimport matplotlib.pyplot as plt import numpy as np def sigmoid_d(x): y = 1 / (1 + np.exp(-x)) return y * (1 - y) x = np.arange(-4, 4, 0.1) y = sigmoid_d(x) plt.plot(x, y) plt.show() 结合上述公式和函数图像，我们可以得出以下结论：\n 由于权重一般是基于均值为0方差为1的高斯分布，所以任何权重总是处于$(0, 1)$ $\\sigma^\\prime(x) \\leq 0.25$ $w_j\\sigma^\\prime(z_j)\u0026lt;0.25$  再结合上面的表达式，当层数越多，乘积项就会越多，就会导致$\\frac{\\partial C}{\\partial b_{1}}$更小，于是就导致了梯度消失。\n Sigmoid作为激活函数情况下，由于梯度反向传播中的连乘效应，导致了梯度消失\n 反之，梯度爆炸的问题就是神经网络前面层比后面层梯度变化快，从而引起了梯度爆炸的问题，比如$w$较大导致了$|w_j\\sigma^\\prime(z_j)|\u0026gt;1$，再结合上面提到的连乘效应，就自然地梯度爆炸了。\n在更加复杂⽹络中的不稳定梯度 #  当我们从简单的神经网络上发现了随着网络层次的加深，会造成网络权值更新不稳定的情况后，也很明确地观察到了梯度消失问题。继续扩展一下，对于那些每层包含很多神经元的更加复杂的网络来说会是怎样的情况呢？\n对于第$l$层的梯度： $$ \\delta^{l}=\\Sigma^{\\prime}\\left(z^{l}\\right)\\left(w^{l+1}\\right)^{T} \\Sigma^{\\prime}\\left(z^{l+1}\\right)\\left(w^{l+2}\\right)^{T} \\ldots \\Sigma^{\\prime}\\left(z^{L}\\right) \\nabla_{a} C $$ 根据连乘效应，会导致出现不稳定的梯度，和前面例子一样，会导致前面层的梯度指数级地消失。\n其它深度学习的障碍 #  前面三节主要介绍了一大障碍：不稳定梯度（梯度消失或者梯度爆炸），实际上，不稳定梯度仅仅是深度学习的众多障碍之⼀。\n下面会据一些关于什么让训练深度⽹络⾮常困难相关主题的例子，这是个相当复杂的问题：\n 在 2010 年 Glorot \u0026amp;\u0026amp; Bengio发现证据表明 sigmoid 函数的选择会导致训练⽹络的 问题。特别地，他们发现sigmoid函数会导致最终层上的激活函数在训练中会聚集在 0，这也导 致了学习的缓慢。他们的⼯作中提出了⼀些取代sigmoid函数的激活函数选择，使得不会被这种聚集性影响性能。 在 2013 年Sutskever, Martens, Dahl 和 Hinton研究了深度学习使⽤随机权重初始化和基于momentum的SGD⽅法。两种情形下，好的选择可以获得较⼤的差异的训练效果。  论文地址：\n Understanding the difficulty of training deep feedforward neural networks  论文解读：  解读——01 解读——02   论文翻译：这里   On the importance of initialization and momentum in deep learning  "});index.add({'id':6,'href':'/ml_book/docs/03_lihang/01.%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/','title':"01.统计学习方法概论",'section':"第三部分：统计学习方法",'content':"统计学习方法概述 #  第一章主要对全书内容做了一个内容的概括：\n 统计学习：定义、研究对象和方法 监督学习 统计学习三要素：模型、策略、算法 模型评估与选择：包括正则化、交叉验证与学习的泛化能力 生成模型与判别模型 分类问题、标注问题与回归问题  统计学习 #  什么是统计学习\n统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，统计学习也称为统计机器学习\n统计学习的特点\n 统计学习以计算机及网络为平台，是建立在计算机及 网络之上的 统计学习以数据为研究对象，是数据驱动的学科 统计学习的目的是对数据进行预测与分析 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论  什么是机器学习\n如果一个程序可以在任务T上，随着经验E的增加，效果P也可以随之增加，则称这个程序可以从经验中学习。 \u0026mdash; 卡内基美隆大学的Tom Michael Mitchell教授\n 如果以垃圾邮件为例，一个程序指的是用到的机器学习算法，比如：朴素贝叶斯、逻辑回归；任务T指的是区分垃圾邮件的任务；经验E为已经区分过是否为垃圾邮件的历史邮件；效果P为机器学习算法在区分是否为垃圾邮件任务上的准确率\n 统计学习的目的\n统计学习用于对数据进行预测与分析，特别是对未知新数据进行预测与分析\n统计学习的方法\n 监督学习（supervised learning）：KNN、决策树、贝叶斯、逻辑回归 非监督学习（unsupervised learning）：聚类、降维 半监督学习（semisupervised learning）：self-training（自训练算法）、Graph-based Semi-supervised Learning（基于图的半监督算法）、Semi-supervised supported vector machine（半监督支持向量机，S3VM） 强化学习（reinforcement learning）：蒙特卡洛方法  监督学习 #  基本概念 #   输入空间（input space）：输入所有可能取值的集合 输出空间（output space）：输出所有可能取值的集合 特征空间（feature space）：每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。这时，所有特征向量存在的空间称为特征空间 联合概率分布：统计学习假设数据存在一定的统计规律，X和Y具有联合概率分布的假设就是监督学习关于数据的基本假设 - 机器学习-联合概率分布笔记 假设空间：学习的目的在于找到最好的模型，模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）  人们根据输入、输出变量的不同类型，对预测任务给予不同的名称：\n 分类问题：输出变量为有限个离散变量的预测问题 回归问题：输入变量与输出变量均为连续变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题  问题的形式化 #  监督学习利用训练数据集学习一个模型，再用模型对测试样本集进行预测 （prediction）：\n输入训练集 -\u0026gt; 生成模型 -\u0026gt; 预测，由于训练集是人工给出的，所以称之为监督学习\n统计学习三要素 #  统计学习方法由三要素构成，可以简单地表示为：方法 = 模型 + 策略 + 算法\n模型 #  在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数\n例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合，假设空间中的模型一般有无穷多个\n策略 #  知道模型的交涉空间后，现在的目的就是选择出最优的一个模型出来，而这就是统计学习的目标，这里引入两个概念：\n 损失函数（loss function）：用于度量模型一次预测的好坏 风险函数（risk function）：度量平均意义下模型预测的好坏  损失函数和风险函数\n监督学习问题是在假设空间中选取模型f作为决策函数，对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致，用一个损失函数（loss function）或代价函数（cost function）来度量预测错误的程度，损失函数是f(X)和Y的非负实值函数，记作L(Y,f(X))\n统计学习中常用的损失函数有如下几种：\n 0-1损失函数（0-1 loss function）：  平方损失函数（quadratic loss function）：  绝对损失函数（absolute loss function）：  对数损失函数（logarithmic loss function）或对数似然损失函数（loglikelihood loss function）：   有了上面概念的铺垫，又可以引入下面两个概念：\n 模型f(X)关于联合分布P(X,Y)的平均意义下的损失：风险函数（risk function）或期望损失（expected loss）：  模型f(X)关于训练数据集的平均损失：经验风险（empirical risk）或经验损失（empirical loss）：   当样本数量足够的情况下，经验风险是趋近于期望风险的，但是现实中样本数量是有限的，，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正\n这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化\n经验风险最小化与结构风险最小化\n在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式（1.10）就可以确定，经验风险最小化（empirical risk minimization，ERM）的策略认为，经验风险最小的模型是最优的模型\n当样本数量过小，可能会出现过拟合问题，结构风险最小化（structural risk minimization，SRM）是为了防止过拟合而提出来的策略\n结构风险最小化等价于正则化（regularization）：\n加上结构风险最小化的策略，此时的最优模型就是下面的最优化问题求解：\n这样，监督学习问题就变成了经验风险或结构风险函数的最优化问题（1.11）和（1.13），这时经验或结构风险函数是最优化的目标函数\n算法 #  统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型，这时，统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法：\n模型评估与模型选择 #  训练误差与测试误差 #  对于我们利用学习方法通过训练给出的模型，我们希望这个模型能较好的具有以下两点能力：\n 拟合能力：已知数据的预测比较合理 泛化能力：未知数据的预测比较合理  前面说过，损失函数能够度量模型预测的好坏，所以基于损失函数的模型的训练误差（training error）和模型的测试误差（test error）就自然成为学习方法评估的标准：\n过拟合与模型选择 #  我们认为在假设空间存在一个真的模型，那么我们选择的模型的目标就是尽量接近这个真模型，假设空间很可能含有不同复杂度的模型，所以此时我们的目标就变成了寻找一个复杂度合适的模型，这里引出一个概念，过拟合（over fitting） - 所选模型的复杂度则比真模型更高\n接下来我们的问题就转接成了需找一个模型复杂度适当的模型，前面一节我们介绍了训练误差与测试误差，那么训练误差与测试误差和模型的复杂度有没有什么关系呢，看下图：\n可以看到，训练误差和模型复杂度的关系是：随着模型复杂度的增加，训练误差逐渐递减，趋向于0（此时可能过拟合），但是测试误差会有个最小值（此时才是最合适的复杂度）\n那么我们的学习目的就出来了：选择复杂度适当的模型，以达到使测试误差最小\n现在目的就很明确了，就是需要一个途径比较测试误差，选出最值小的，从而选出我们的模型：\n 正则化 交叉验证  正则化与交叉验证 #  正则化 #  在统计学习三要素那一节讲过结构风险最小化策略，此策略的目的就是防止过拟合，正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项(penalty term)：\n正则化的作用是选择经验风险与模型复杂度同时较小的模型\n交叉验证 #  交叉验证的基本想法是重复地使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择，交叉验证有如下三种方式：\n 简单交叉验证 S折交叉验证 留一交叉验证  泛化能力 #  学习方法的泛化能力（generalization ability）是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质\n泛化误差 #  泛化误差的定义\n对于一个学到的模型，其对未知数的预测就是泛化误差（generalization error），其实就是策略那节讲的风险函数：\n泛化误差反映了学习方法的泛化能力，如果一种方法学习的模型比另一种方法学习的模型具有更小的泛化误差，那么这种方法就更有效\n泛化误差上界 #  学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称为泛化误 差上界（generalization error bound）\n泛化误差上界性质如下：\n 它是样本容量的函数，当样本容量增加时，泛化上界趋于0 它是假设空间容量（capacity）的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大  生成模型与判别模型 #  监督学习方法可以分为：\n 生成方法（generative approach）：生成方法可以还原出联合概率分布P(X,Y)，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用 判别方法（discriminative approach）：判别方法直接学习的是条件概率P(Y|X)或决策函数f(X)，直接面对预测，往往学习的准确率更高；由于直接学习P(Y|X)或f(X)，可以对数据进行各种程度上 的抽象、定义特征并使用特征，因此可以简化学习问题  所学到的模型分别称为：\n 生成模型（generative model）：  朴素贝叶斯法 隐马尔可夫模型   判别模型（discriminative model）：  k近邻法 感知机 决策树 逻辑斯谛回归模型 最大熵模型 支持向量机 提升方法 条件随机场 \u0026hellip;\u0026hellip;    分类问题 #   在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题，监督学习从数据中学习一个分类模型或分类决策函数，称为分类器（classifier）\n 许多统计学习方法可以用于分类：\n k近邻法 感知机 朴素贝叶斯法 决策树 决策列表 逻辑斯谛回归模型 支持向量机 提升方法 贝叶斯网络 神经网络 Winnow \u0026hellip;\u0026hellip;  评价分类器性能的指标一般是分类准确率（accuracy）：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比，也就是损失函数是0-1损失时测试数据集上的准确率，见图1.17\n对于二类分类问题常用的评价指标是精确率（precision）与召回率（recall），因为是二分类，所以一共有四中预测情况：\n TP——将正类预测为正类数 FN——将正类预测为负类数 FP——将负类预测为正类数 TN——将负类预测为负类数  标注问题 #   标注问题的目标在于学习一个模型， 使它能够对观测序列给出标记序列作为预测\n 许多统计学习方法可以用于标注：\n 隐马尔可夫模型 条件随机场  评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率，其定义与分类模型相同\n回归问题 #  回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化，回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归\n说明 #  一些参考链接：\n 致力于将李航博士《统计学习方法》一书中所有算法实现一遍 《统计学习方法》的读书笔记，重现了大部分的课后题，仅供参考 深度学习中的概率知识详解  "});index.add({'id':7,'href':'/ml_book/docs/03_lihang/02.%E6%84%9F%E7%9F%A5%E6%9C%BA/','title':"02.感知机",'section':"第三部分：统计学习方法",'content':"感知机 #  本章主要介绍了二类分类的线性分类模型：感知机：\n 感知机模型 感知机学习策略 感知机学习算法  说明：个人感觉这本书偏理论化，讲究的是一招定天下，好处是内功深厚自然无敌，一通百通，但难处是语言有点晦涩，这章可以考虑结合我之前的一篇关于感知器的笔记，或许能加深理解，见这里\n感知机模型 #  感知机（perceptron）：是一个二类分类的线性判断模型，其输入为实例的特征向量，输出为实例的类别，取+1和–1值，属于判别模型\n注：+1 -1 分别代表正负类，有的可能用 1 0 表示\n在介绍感知机定义之前，下面几个概念需要说明一下：\n 输入空间：输入所有可能取值的集合 输出空间：输出所有可能取值的集合 特征空间：每个具体的输入是一个实例，由特征向量表示  所以对于一个感知机模型，可以这样表示：\n 输入空间（特征空间）：$\\chi \\subseteq \\mathbb{R} ^n$ 输出空间：$\\gamma = \\{+1,-1 \\}$  那么感知机就是由输入空间到输出空间的函数：\n$$\\displaystyle f( x) \\ =\\ sign( w\\cdot x+b)$$\n其中：\n $sign$: 符号函数 $w$: 权值（weight）或权值向量（weight vector） $b$: 偏置（bias）  感知机的几何解释如下：线性方程\n$$w\\cdot x + b =0$$\n如果是二维空间，感知机就是一个线性函数，将正负样本一分为二，如何是三维空间，那么感知机就是一个平面将类别一切为二，上升到n维空间的话，其对应的是特征空间$\\mathbb{R} ^n$的一个超平面$S$：\n $w$: 超平面的法向量 $b$: 超平面的截距  感知机学习策略 #  数据集的线性可分性 #  什么是数据集的线性可分性，很简单，对于一个数据集：\n$$T = {(x_1,y_1),(x_2,y_2),\u0026hellip;,(x_n,y_n)}$$\n如果存在上面一节说的超平面$S$：$w\\cdot x + b =0$，能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则称数据集T为线性可分数据集（linearly separable data set），否则，称数据集T线性不可分\n感知机学习策略 #  找出超平面$S$，其实就是确定感知机模型参数：$w b$，根据统计学习方法三要素，此时我们需要确定一个学习策略，比如前面所说的损失函数（经验函数），并使其最小化（猜也猜得到策略讲完，后面就是说学习算法了哈哈）\n上一章以线性代数为例子，用损失函数来度量预测错误的程度，这里的损失函数可以用误分类点到超平面$S$的总距离，输入空间$\\mathbb{R} ^n$中任一点$x_0$到超平面$S$的距离：\n$$\\frac{1}{||w||}|w\\cdot x_0+b|$$\n其中，$||w||$是$w$的$L_2$范数，假设超平面S的误分类点集合为$M$，那么所有误分类点到超平面$S$的总距离为：\n$$-\\frac{1}{||w||}\\sum_{x_i\\in M} y_i(w\\cdot x_i + b)$$\n最终推导出感知机学习的损失函数：\n$$L(w,b) =-\\sum_{x_i\\in M} y_i(w\\cdot x_i + b)$$\n感知机学习算法 #  上面一节已经确定了学习策略，按照统计学习方法三要素，目前需要一个算法来求解，目前最优化的方法是随机梯度下降法\n感知机学习算法的原始形式 #  现在感知机学习算法就是对下面最优化问题的算法：\n$$ \\min_{w,b} L(w,b) =-\\sum_{x_i\\in M} y_i(w\\cdot x_i + b) $$\n现在的问题就转化成，求出参数$w$和$b$，使得上列损失函数达到极小化，这里我直接贴出书中的算法，后面的例子我会用Python代码实现：\n有了解题方法怎么能没有题目呢？李杭老师自然是考虑到了，请听题：\n借用Linus Torvalds大佬的一句话：Talk less, show me your code，所以直接看代码吧：\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; Created by howie.hu at 2018/9/20. \u0026#34;\u0026#34;\u0026#34; import numpy as np class Perceptron: \u0026#34;\u0026#34;\u0026#34; 李航老师统计学习方法第二章感知机例2.1代码实现 \u0026#34;\u0026#34;\u0026#34; def __init__(self, input_nums=2): # 权重 已经确定只会有两个二进制输入 self.w = np.zeros(input_nums) # 偏置项 self.b = 0.0 def fit(self, input_vectors, labels, learn_nums=10, rate=1): \u0026#34;\u0026#34;\u0026#34; 训练出合适的 w 和 b :param input_vectors: 样本训练数据集 :param labels: 标记值 :param learn_nums: 学习多少次 :param rate: 学习率 \u0026#34;\u0026#34;\u0026#34; for i in range(learn_nums): for index, input_vector in enumerate(input_vectors): label = labels[index] delta = label * (sum(self.w * input_vector) + self.b) if delta \u0026lt;= 0: # 计算方法由梯度下降算法推导出来 self.w += label * input_vector * rate self.b += rate * label break print(\u0026#34;最终结果：此时感知器权重为{0}，偏置项为{1}\u0026#34;.format(self.w, self.b)) return self def predict(self, input_vector): \u0026#34;\u0026#34;\u0026#34; 跃迁函数作为激活函数，感知器 :param input_vector: :return: \u0026#34;\u0026#34;\u0026#34; if isinstance(input_vector, list): input_vector = np.array(input_vector) y = sum(self.w * input_vector) + self.b return 1 if y \u0026gt; 0 else -1 if __name__ == \u0026#39;__main__\u0026#39;: input_vectors = np.array([[3, 3], [4, 3], [1, 1]]) labels = np.array([1, 1, -1]) p = Perceptron() model = p.fit(input_vectors, labels) print(model.predict([3, 3])) print(model.predict([4, 3])) print(model.predict([1, 1])) 输出如下：\n最终结果：此时感知器权重为[ 1. 1.]，偏置项为-3.0 1 1 -1 代码写完了，再看看推导过程：\n算法的收敛性 #  对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型，定理2.1如下：\n假设训练数据集$T = {(x_1,y_1),(x_2,y_2),\u0026hellip;,(x_n,y_n)}$是线性可分的，其中$x_i\\in \\chi =\\mathbb{R} ^n$，$y_i \\in \\gamma =\\{-1, 1\\}$，$i=1,2,\u0026hellip;,N$，则有：\n感知机学习算法的对偶形式 #  为什么要介绍感知机学习算法的对偶形式，主要目的就是减少运算量，这里一个知乎回答得挺好：\n代码实现如下：\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; Created by howie.hu at 2018/9/21. \u0026#34;\u0026#34;\u0026#34; import numpy as np class Perceptron: \u0026#34;\u0026#34;\u0026#34; 李航老师统计学习方法第二章感知机例2.2对偶形式代码实现 \u0026#34;\u0026#34;\u0026#34; def __init__(self, alpha_length=3): self.alpha = np.zeros(alpha_length) # 权重 self.w = np.zeros(2) # 偏置项 self.b = 0.0 def fit(self, input_vectors, labels, learn_nums=7): \u0026#34;\u0026#34;\u0026#34; 训练出合适的 w 和 b :param input_vectors: 样本训练数据集 :param labels: 标记值 :param learn_nums: 学习多少次 \u0026#34;\u0026#34;\u0026#34; gram = np.matmul(input_vectors, input_vectors.T) for i in range(learn_nums): for input_vector_index, input_vector in enumerate(input_vectors): label = labels[input_vector_index] delta = 0.0 for alpha_index, a in enumerate(self.alpha): delta += a * labels[alpha_index] * gram[input_vector_index][alpha_index] delta = label * delta + self.b if delta \u0026lt;= 0: self.alpha[input_vector_index] += 1 self.b += label break self.w = sum([j * input_vectors[i] * labels[i] for i, j in enumerate(self.alpha)]) print(\u0026#34;最终结果：此时感知器权重为{0}，偏置项为{1}\u0026#34;.format(self.w, self.b)) return self def predict(self, input_vector): if isinstance(input_vector, list): input_vector = np.array(input_vector) y = sum(self.w * input_vector) + self.b return 1 if y \u0026gt; 0 else -1 if __name__ == \u0026#39;__main__\u0026#39;: input_vectors = np.array([[3, 3], [4, 3], [1, 1]]) labels = np.array([1, 1, -1]) p = Perceptron() model = p.fit(input_vectors, labels) print(model.predict([3, 3])) print(model.predict([4, 3])) print(model.predict([1, 1])) 最终结果：此时感知器权重为[ 1. 1.]，偏置项为-3.0 1 1 -1 说明 #  一些概念的详细解释：\n 超平面是什么？  搞定收工，有兴趣欢迎关注我的公众号：\n"});index.add({'id':8,'href':'/ml_book/docs/04_appendix/00.%E4%B8%80%E7%AB%99%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%91%E7%A0%94%E5%8F%91%E5%B9%B3%E5%8F%B0/','title':"00.一站式机器学习云研发平台",'section':"第四部分：附录",'content':"一站式机器学习云开发平台 #   本篇是关于自身在机器学习这块工作经验的思考总结\n 我希望构建一个机器学习云开发平台，目标在于解决以下问题：\n 团队协作：项目管理，技术\u0026amp;业务的共享如何体现在实际解决问题的过程中； 资源调度：数据处理、模型训练； 模块共享：低代码甚至无代码； 快速开发：快速试错、实践、测试、部署； 需求-\u0026gt;开发的闭环  背景 #  自17年毕业以来，我从事于游戏行业的风控领域，主要涉及的系统的是风控和画像这两块。尽管我从团队初始建立的时候就定义了各种服务模板以及脚手架，但在数据处理、模型构建、管理这块我并没有着重去管控，仅仅是设定了一些服务的标准（初始阶段需要的是快速响应需求），如今随着业务场景和团队成员的增加，有如下问题需要考虑：\n 脚本模块共享问题，团队成员长期开发积累的技术脚本、模型等是否可以模块化用于给其他成员共用； 不同数据及不同模型快速试验的的效率问题，我理想的解决方案是类似工作流那样自由组合脚本，从数据提取到模型构建再到模型管控以作业流的形式来完成（这也意味着只要公有模块覆盖面足够广，就可以进行组件拖拽式开发）； 模型测试管理问题，每个模型有哪些版本？怎么快速测试？分别被哪些服务使用以及如何快速上线； 资源调度问题，公司的数据存于云商，本地开发涉及的资源问题如何解决。  一站式机器学习云研发平台 #  目标 #  上面提到的问题可能也和大数据计算领域有些交集，毕竟都涉及到了 ETL，这些问题实际上涵盖了一个数据模型从开发到上线完整的生命周期。对于上述的需求，其实我们可以分别拆开来看看会衍生出怎样的目标：\n 第1、2两点在我看来可以归纳为一个问题，模块需要抽象，然后让开发者可以自由地从模块仓库选取自己需要的模块进行组合，最终形成一个从数据提取到模型构建再到模型管控的工作流；一个项目可以有多个工作流，工作流的最小元素是模块，工作流的运行参数可以自由定义，开发者可以通过调整参数来快速调优；模块（函数）即服务，组合完毕就意味着一个需求的实现； 第3点的实现离不开两个基础服务，一是模型自动API服务化，二是模型管控服务； 第4点可以从两个方向入手，引入资源管理系统，如k8s或者云商的资源管理服务。  最终目标呼之欲出了，总结下我的需求，我需要的是一个满足数据计算、训练、管理的一站式机器学习云研发平台；而这个平台具有管理一个模型生命周期且形成闭环的功能，核心功能如：\n 数据的访问与计算是无限制的； 模块即服务且可多用户互相共享； 一个需求的解决方案是模块的自由组合，需求的最终产物是模型（也可以是数据）； 对于模型可配置、易管控，可自动API服务化且可快速测试上线。  对于一站式机器学习云研发平台，我们需要其具有怎样的功能已经描述的差不多，既然问题已经抛了出来，接下来就说说怎么解决。\n一切事情都有很多种解决方案，随着云原生的普及，容器技术的引入，中台在国内企业的覆盖率越来越广，该有的技术和业务的基础设施都开始有了一定的积累，怎么利用这些基础设施来进一步提升开发效率是接下来的一个方向，我期望最终解决方案有如下两个特性：\n  低代码（高层次人员无代码）：基于现有的基础设施（脚手架、模型代码库、模板）可以根据通用模块结合个性化配置形成工作流就可以构建一个机器学习应用，工作流为核心的低代码、甚至无代码工作方式是我的期望，那么代码谁来写？这又是一个值得深入探讨的问题，目前暂不探讨，现在就假设有底层程序员在持续奋斗；\n  云开发：因为涉及到资源的调度（数据计算、模型训练等），所以一个模型从数据输入到模型输出的开发流程都可以在云端完成（特别是数据处理和模型训练），浏览器在手，天下我有。\n  就目前垂直的机器学习数据科学领域，再结合最新的技术方向，我个人很看好这两块，我认为低代码、云开发是构建新一代云原生应用的新式武器！（如果一直往前走，容器技术暂且不谈，单单微服务架构是不适用于这种原生应用的构建了，目前有大佬在研究的云研发架构或许是个方向。）\n流程 #  先说说低代码，这里的低代码表达的是我们实现一个模型需求的方式，也就是说我们用低代码的方式来快速实现需求。\n前面提到的模块即服务，通过连接各个模块最终形成一个工作流就是一个友好的方式，这也是市面上大部分产品的实现方式；每个模块的展现形式应该是友好可定义的，然后根据输入参数的不同来展现不同的行为（比如通过DSL定义，再基于DSL构建交互式的界面进行快速低代码开发）。\n前面我们强调了闭环，那么在满足低代码这种特性下，我们的开发流程是怎样的呢？\n 需求分析 方案确立（文档\u0026amp;会议讨论） 组件选择（通过DSL定义） 在机器学习云平台勾选组件、进行DSL配置，形成工作流（工作流的输出就是解决方案的核心） 验证\u0026amp;验收\u0026amp;部署（业务介入）  其实和传统方式一样，低代码的核心点也要求从需求分析到部署可以形成一个独立的闭环，不过在中间开发、验收、部署的过程都尽量实现低代码。低代码带来的效率提升是肉眼可见的，而实现低代码的关键点在于技术与业务的抽象，这里的表现为模块[函数]即服务；通过组合各个函数，就可以很方便地形成一个端到端的工作流以作为对某个需求的解决方案。\n再来说说云开发，在机器学习领域，计算资源是一个不可忽视的问题，特别是在大数据计算清洗以及模型训练这块，目前基本上绕不过几大云商。我的想法是数据在哪，我们的开发环境就可以在哪，结合k8s，我们可以轻松建立团队的云研发环境，比如：\n  code-server\n  jupyterlab\n  这两个是市面上比较成熟的远程开发工具，在之前的一年开发过程中，我在团队中主要就是引用了这两个工具（一些数据计算需求在本地没法做，因为数据都在云商且是动态数据，只能用这种形式远程调试开发）。\n搭建 #  目前开源社区都有不少机器学习相关的工具，我也对一些工具进行了调研，评估标准就是解决我上面背景部分提出的问题，但是最终比较和我心意的是：Orchest：A new kind of IDE for Data Science.\n虽然Orchest并不能完全解决我的问题，但是这个工具的思想和交付和我的需求与思考都非常契合，它虽然没有实现我期望的从数据提取到模型训练上线这一套完整的闭环，但是在工作流的管控这块做了很好的实现，一定程度上实现了低代码开发，Orchest的主要优缺点如下：\n 项目是最高等级的概念，项目下面是工作流，工作流由各个模块组成，交互很友好，每个工作流可被任务进行调度，快速试验（但是项目过多会增加项目管理难度，我提了命名空间的需求，作者团队表示会开发）； 每个模块可自定义环境（Python、R等）且在项目下面模块是用户共享的（自定义镜像还是比较麻烦，反馈后也表示在优化）； 目前模块共享仅限于项目这个层次，我希望这个共享是无视项目的（我提的Issue见#129） 编码支持Jupyter（我提了Code Server的支持#113）； 基于Docker做资源调度，但是这样在团队里面是玩不转的，需要支持类似K8s这种才可以。  尽管Orchest在功能上并没有完全做到上面我说的几个点，但是在低代码和云开发上深得我意，而且这个开源项目看得出来非常有活力，我很喜欢。因此我决定在这个上面做一些二次开发来完全满足我的需求：\n 在模块共享、工作流、快速试验上，Orchest做得已经达到可以生产使用的程度了，而且上手容易，我可以尝试在这块做到跨项目的模块共享； 工作流产出数据或者模型后，怎们快速将模型服务化自动转化成HTTP服务进行快速测试？这里需要解决的问题是每个模型其实依赖的环境是不一样的，在对模型进行自动接口化的过程中就需要动态的识别模型当前的环境（比如依赖的包），所幸基于Orchest可以依据项目名称来检索对应环境，从而自动生成镜像来进行自动接口化；所以最终我会实现一个将模型自动化API的服务作为插件嵌入到Orchest； 随着业务的增加，模型也会越来越多，每个模型有不同的版本，哪个版本被哪个服务使用？用的是哪个版本？确定版本又怎么快速上线？这一套流程需要被一个服务管控（结合自身的业务），所以这里需要的是一个模型管理服务，同样，我也会将这个作为插件嵌入到Orchest。  上面对应的服务差不多两周时间做了个初版，在实现过程中也发现了一些Orchest的Bug，顺手修了下提了个PR，和作者交流了一下发现作者团队还是挺靠谱的，他邀请我们团队作为他们的Launching Customers，意思是我们反馈问题和场景需求，他们提供技术支持，算是初步达成协作。\n目前使用来看的话基本上从需求理解、基于通用模块构建应用工作流、模型测试\u0026amp;评估、管理上线这一套已经形成了闭环，而且在公有模块充足的情况，是可以做到低代码甚至无代码，模型调优是实打实的无代码，而且所有的开发都是在浏览器（云）上进行的。\n可以考虑设想一个构建机器学习应用的场景，基于当前的一站式机器学习云研发平台流程是这样的：\n  构建应用工作流：\n 选取通用数据提取模块（选择目标特征）：这里的数据提取\u0026amp;清洗极耗费资源，一般这里我会将资源调度到云商或者分布式的资源管理集群上，平台上云基本上是跟着这里的数据走    调用数据分析模块\n 假设是回归任务，从公用模型库选择希望调试的模型（可以选多个）    各个服务模块组合成一个工作流，工作流的结果就是需求的解决方案\n  基于工作流定义不同的参数配置进行调优，比如选取十组不同的参数，基于一套工作流构建十个任务同时进行参数调优，快速产出模型\n  验证\u0026amp;评估模型：一键模型API自动化，通过HTTP接口对模型进行测试；机器自动\u0026amp;人工多方多角度进行模型评估（自动化为主）\n  模型一键热更上线：基于模型管理服务\n  上面的内容就是我个人对一站式机器学习云研发平台的思考，实际上在低代码这块如果针对的是某个垂直领域，比如本文说的机器学习这块，个人觉得实践可操作性还是挺大的，我已经按照自己想的做了一个出来并且开始在团队里面实践，可预见的是对团队项目的管理以及成员开发效率的提升，会有很大的作用。\n这两周时间都放在了一站式机器学习云研发平台的折腾上，有些兴奋；后面我会在功能以及理念上做更深入的优化，然后在不断实践中持续思考，再反馈输出更新个人此类别文章；有同样想法的朋友，非常欢迎一起交流交流。\n PS: Orchest是个不错的云研发工具，如果你工作的领域是机器学习以及大数据相关方向，可以尝试使用一下。\n 说明 #  对本文有影响的资料：\n 万物代码化：从低代码、云开发到云研发 awesome-lowcode Orchest：A new kind of IDE for Data Science.  )\n"});index.add({'id':9,'href':'/ml_book/docs/04_appendix/01.%E8%AF%91%E5%A6%82%E4%BD%95%E7%94%A8python%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/','title':"01.[译]如何用 Python创建一个简单的神经网络",'section':"第四部分：附录",'content':"如何用Python创建一个简单的神经网络 #   原文地址：How to Create a Simple Neural Network in Python 作者：Dr. Michael J. Garbade 翻译：howie6879   理解神经网络如何工作的最好方式是自己动手创建一个，这篇文章将会给你演示怎么做到这一点\n 神经网络(NN)，也称之为人工神经网络(ANN)，它是机器学习领域中学习算法集合中的子集，其核心概念略似生物神经网络的概念。\n拥有五年以上经验的德国机器学习专家Andrey Bulezyuk说过：神经网络正在使机器学习发生革命性的巨变，因为他们能够跨越广泛的学科和行业来高效地建模复杂的抽象。\n基本上，一个ANN由以下组件构成：\n 输入层：接受传递数据 隐藏层 输出层 各层之间的权重 每个隐藏层都会有一个精心设计的激活函数，对于此教程，我们将会使用Sigmoid激活函数  神经网络的类型有很多，在这个项目中，我们准备创建一个前馈神经网络，此类型的ANN将会直接从前到后传递数据\n训练前馈神经元往往需要反向传播，反向传播为神经网络提供了相应的输入和输出集合，输入数据在被传递到神经元的时候被处理，然后产生一个输出\n下面展示了一个简单的神经网络结构图：\n而且，理解神经网络如何工作做好的办法就是去学习从头开始构建一个神经网络(不使用任何第三方库，作者意思应该是不使用任何机器学习库)。\n在本文中，我们将演示如何使用Python编程语言创建一个简单的神经网络。\n问题 #  这里用表格列出了我们需要解决的问题：\n我们将会训练一个特定的神经网络模型，当提供一组新数据的时候，使其能够准确地预测输出值。\n如你在表中所见，输出值总是等于输入数据的第一个值，因此我们期望的表中输出(?)值是1。\n让我们思考看看能不能使用一些Python代码来给出相同的结果(再继续阅读之前，你可以在文章末尾仔细地阅读此项目的代码)\n创建一个神经网络类 #  我们将在Python中创建一个NeuralNetwork类来训练神经元以提供准确的预测，该类还具有一些其他的辅助函数\n尽管我们没有使用任何一个神经网络库用于这个简单的神经网络示例，我们也会导入numpy包来协助计算。\n该库带有以下四个重要方法：\n exp：用于生成自然指数 array：用于生成矩阵 dot：用于乘法矩阵 random：用于生成随机数(注意：我们将对随机数进行播种以确保其有效分布)  应用 Sigmoid 激活函数 #  该神经网络将使用Sigmoid function作为激活函数，其绘制了一个典型的S形曲线：\n此函数可以将任意值映射到区间0~1之间，它将帮助我们规范化输入值的和权重乘积之和。\n随后，我们将创建Sigmoid函数的导数来帮助计算机对权重进行必要的调整。\n一个Sigmoid函数的输出可以用来生成它的导数，例如，如果输出变量是X，那么它的导数将是x * (1-x)。\n推导过程如下：\n训练模型 #  在这个阶段我们将教导神经网络进行准确预测，每个输入都有一个权重 - 正面或负面。\n这意味着，如果输入包含一个大的正面或者负面的权重数值将会更多地影响输出值\n请记住，在最开始阶段我们会对每个权重分配一个随机值\n下面是我们在这个神经网络示例问题中使用的训练过程:\n 从训练集获取输入数据，根据他们的权重进行调整，然后通过计算人工神经网络输出的方法将它们抽取出来 我们计算了反向传播的错误率，在这种情况下，它是神经元预测值和实际期望值之间的差异 根据误差程度，我们利用Error Weighted Derivative formula对权重进行微调 我们对这个过程重复15,000次，在每次迭代中都会同时处理整个训练集  我们使用.T函数将矩阵从水平位置转换到垂直位置，因此数据会被这样排序：\n最终，神经元的权重将会被提供的训练集优化，因此，如果神经元被要求去思考一个新的情况，而这个情况和前面的情况是一样的，那么神经元就可以做出准确的预测，这就是反向传播的发生方式。\n总结 #  最终我们初始化了NeuralNetwork类并且运行代码\n下面就是整体的项目代码，如何在Python项目中创建神经网络：\nimport numpy as np class NeuralNetwork(): def __init__(self): # seeding for random number generation np.random.seed(1) # converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0 self.synaptic_weights = 2 * np.random.random((3, 1)) - 1 def sigmoid(self, x): #applying the sigmoid function return 1 / (1 + np.exp(-x)) def sigmoid_derivative(self, x): #computing derivative to the Sigmoid function return x * (1 - x) def train(self, training_inputs, training_outputs, training_iterations): #training the model to make accurate predictions while adjusting weights continually for iteration in range(training_iterations): #siphon the training data via the neuron output = self.think(training_inputs) #computing error rate for back-propagation error = training_outputs - output #performing weight adjustments adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output)) self.synaptic_weights += adjustments def think(self, inputs): #passing the inputs via the neuron to get output  #converting values to floats inputs = inputs.astype(float) output = self.sigmoid(np.dot(inputs, self.synaptic_weights)) return output if __name__ == \u0026#34;__main__\u0026#34;: #initializing the neuron class neural_network = NeuralNetwork() print(\u0026#34;Beginning Randomly Generated Weights: \u0026#34;) print(neural_network.synaptic_weights) #training data consisting of 4 examples--3 input values and 1 output training_inputs = np.array([[0,0,1], [1,1,1], [1,0,1], [0,1,1]]) training_outputs = np.array([[0,1,1,0]]).T #training taking place neural_network.train(training_inputs, training_outputs, 15000) print(\u0026#34;Ending Weights After Training: \u0026#34;) print(neural_network.synaptic_weights) user_input_one = str(input(\u0026#34;User Input One: \u0026#34;)) user_input_two = str(input(\u0026#34;User Input Two: \u0026#34;)) user_input_three = str(input(\u0026#34;User Input Three: \u0026#34;)) print(\u0026#34;Considering New Situation: \u0026#34;, user_input_one, user_input_two, user_input_three) print(\u0026#34;New Output data: \u0026#34;) print(neural_network.think(np.array([user_input_one, user_input_two, user_input_three]))) print(\u0026#34;Wow, we did it!\u0026#34;) 我们设法创建了一个简单的神经网络。\n这个神经网络开始于自己给自己分配了一些随机的权重，此后，它使用训练数据训练自己。\n因此，如果出现新情况[1,0,0]，则其值为0.9999584。\n你记得我们想要的正确答案是1吗？\n那么，这就非常接近了 —— 思考下S形函数的输出值在0到1之间。\n当然，我们只使用一个神经网络来执行这个简单的任务，如果我们连接数千个人工神经网络起来会怎样？我们能否100% 地模仿人类大脑的工作方式么？\n如果你有任何疑问，请留言。\n"});index.add({'id':10,'href':'/ml_book/docs/04_appendix/02.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/','title':"02.梯度下降数学推导",'section':"第四部分：附录",'content':"梯度下降数学推导 #  以感知器为例，可以梯度下降来学习合适的权重和偏置：\n假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为：\n$$ \\bar{y}^i = w_1x_1^i+w_2x_2^i+\u0026hellip;+w_nx_n^i+b $$\n任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示：\n$$ e^i=\\frac{1}{2}(y^i-\\bar{y}^i)^{2} $$\n那么所有误差的和可以表示为：\n$$ \\begin{aligned} E \u0026amp;= e^1+e^2+\u0026hellip;+e^n \\\\ \u0026amp;= \\sum_{i=1}^ne^i \\\\ \u0026amp;= \\frac{1}{2}\\sum_{i=1}^n(y^i-w^Tx^i)^2 \\end{aligned} $$\n梯度下降 #  想象一下，当你从山顶往下走，只要你沿着最陡峭的位置往下走，那么终将走到最底部（也可能是局部最低）：\n我们学习的目的就是在$E$尽量最小，然后得到此时的$w$和$b$，前面说的最陡峭的位置该怎么定义呢？我们可以引入梯度，这是一个向量，指的是函数值上升最快的方向，那么最陡峭的位置就可以用在最陡峭的方向迈出一步（步长，学习速率），用数学公式表示为：\n 其中：\n $\\nabla$表示梯度算子 $\\nabla f(x)$表示函数的梯度 $\\eta$表示梯度、学习速率，可以理解为找准下山的方向后要迈多大步子  推导 #  现在有了目标函数，也知道怎么找到让目标函数值最小的办法，对于参数$w$： $$ E_{(w)} = \\frac{1}{2}\\sum_{i=1}^n(y^i-w^Tx^i)^2 $$\n那么$W$值的更新公式为：\n$$ w_{n e w}=w_{\\text {old }}-\\eta \\nabla E_{(w)} $$\n关键步骤来了，来看看$E_{(w)}$的推导吧：\n$$ \\begin{aligned} \\nabla E(\\mathrm{w}) \u0026amp;=\\frac{\\partial}{\\partial \\mathrm{w}} E(\\mathrm{w}) \\\\\n\u0026amp;=\\frac{\\partial}{\\partial \\mathrm{w}} \\frac{1}{2} \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2} \\\\\u0026amp;=\\frac{1}{2}\\frac{\\partial}{\\partial \\mathrm{w}} \\sum_{i=1}^{n} \\left(y^{(i)2}-2y^{(i)}\\bar{y}^{(i)}+\\bar{y}^{(i)2}\\right) \\end{aligned} $$\n再引入链式求导法则：\n$$ \\begin{aligned} \\nabla E(\\mathrm{w}) \u0026amp;=\\frac{\\partial}{\\partial \\mathrm{w}} E(\\mathrm{w}) \\\\\n\u0026amp;=\\frac{1}{2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\mathrm{w}}\\left(y^{(i) 2}-2 y^{(i)} \\bar{y}^{(i)}+\\bar{y}^{(i) 2}\\right) \\\\\n\u0026amp;=\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\frac{\\partial}{\\partial \\bar{y}^{(i)}}\\left(y^{(i) 2}-2 y^{(i)} \\bar{y}^{(i)}+\\bar{y}^{(i) 2}\\right) \\frac{\\partial y_{(i)}}{\\partial \\mathrm{w}}\\right) \\\\\n\u0026amp;=\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\left(-2 y^{(i)}+2 \\bar{y}^{(i)}\\right) \\mathbf{x}^{(i)}\\right) \\\\\n\u0026amp;=-\\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right) \\mathrm{x}^{(i)} \\end{aligned} $$\n前面提到$W$值的更新公式为：\n$$ w_{n e w}=w_{\\text {old }}-\\eta \\nabla E_{(w)} $$\n将上面计算结果带入：\n$$ w_{n e w}=w_{\\text {old }}+\\eta \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right) \\mathrm{x}^{(i)} $$\n参数的更新方式就这样计算出来了，其实所谓的学习，就是确定一个目标函数用一定的计算方法让其算出最优的参数。\n"});})();