'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/k8s/docs/01_explore/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/','title':"01.神经网络基础",'section':"第一部分：探索",'content':"神经网络基础 #  要想入门以及往下理解深度学习，其中一些概念可能是无法避免地需要你理解一番，比如：\n 什么是感知器 什么是神经网络 张量以及运算 微分 梯度下降  带着问题出发 #  在开始之前希望你有一点机器学习方面的知识，解决问题的前提是提出问题，我们提出这样一个问题，对MNIST数据集进行分析，然后在解决问题的过程中一步一步地来捋清楚其中涉及到的概念\nMNIST数据集是一份手写字训练集，出自MNIST，相信你对它不会陌生，它是机器学习领域的一个经典数据集，感觉任意一个教程都拿它来说事，不过这也侧面证明了这个数据集的经典，这里简单介绍一下：\n 拥有60,000个示例的训练集，以及10,000个示例的测试集 图片都由一个28 ×28 的矩阵表示，每张图片都由一个784 维的向量表示 图片分为10类， 分别对应从0～9，共10个阿拉伯数字  压缩包内容如下：\n train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)  上图：\n图片生成代码如下：\n%matplotlib inline import matplotlib import matplotlib.pyplot as plt import numpy as np from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = instances n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = matplotlib.cm.binary, **options) plt.axis(\u0026#34;off\u0026#34;) plt.figure(figsize=(9,9)) plot_digits(train_images[:100], images_per_row=10) plt.show() 不过你不用急着尝试，接下来我们可以一步一步慢慢来分析手写字训练集\n看这一行代码：\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data() MNIST数据集通过keras.datasets加载，其中train_images和train_labels构成了训练集，另外两个则是测试集：\n train_images.shape: (60000, 28, 28) train_labels.shape: (60000,)  我们要做的事情很简单，将训练集丢到神经网络里面去，训练后生成了我们期望的神经网络模型，然后模型再对测试集进行预测，我们只需要判断预测的数字是不是正确的即可\n在用代码构建一个神经网络之前，我先简单介绍一下到底什么是神经网络，让我们从感知器开始\n感知器 #   感知器是Frank Rosenblatt提出的一个由两层神经元组成的人工神经网络，它的出现在当时可是引起了轰动，因为感知器是首个可以学习的神经网络\n 感知器的工作方式如下所示：\n左侧三个变量分别表示三个不同的二进制输入，output则是一个二进制输出，对于多种输入，可能有的输入成立有的不成立，在这么多输入的影响下，该如何判断输出output呢？Rosenblatt引入了权重来表示相应输入的重要性\n此时，output可以表示为：\n上面右侧的式子是一个阶跃函数，就是和Sigmoid、Relu一样作用的激活函数，然后我们就可以自己实现一个感知器：\nimport numpy as np class Perceptron: \u0026#34;\u0026#34;\u0026#34; 代码实现 Frank Rosenblatt 提出的感知器的与非门，加深对感知器的理解 blog: https://www.howie6879.cn/post/33/ \u0026#34;\u0026#34;\u0026#34; def __init__(self, act_func, input_nums=2): \u0026#34;\u0026#34;\u0026#34; 实例化一些基本参数 :param act_func: 激活函数 \u0026#34;\u0026#34;\u0026#34; # 激活函数 self.act_func = act_func # 权重 已经确定只会有两个二进制输入 self.w = np.zeros(input_nums) # 偏置项 self.b = 0.0 def fit(self, input_vectors, labels, learn_nums=10, rate=0.1): \u0026#34;\u0026#34;\u0026#34; 训练出合适的 w 和 b :param input_vectors: 样本训练数据集 :param labels: 标记值 :param learn_nums: 学习多少次 :param rate: 学习率 \u0026#34;\u0026#34;\u0026#34; for i in range(learn_nums): for index, input_vector in enumerate(input_vectors): label = labels[index] output = self.predict(input_vector) delta = label - output self.w += input_vector * rate * delta self.b += rate * delta print(\u0026#34;此时感知器权重为{0}，偏置项为{1}\u0026#34;.format(self.w, self.b)) return self def predict(self, input_vector): if isinstance(input_vector, list): input_vector = np.array(input_vector) return self.act_func(sum(self.w * input_vector) + self.b) def f(z): \u0026#34;\u0026#34;\u0026#34; 激活函数 :param z: (w1*x1+w2*x2+...+wj*xj) + b :return: 1 or 0 \u0026#34;\u0026#34;\u0026#34; return 1 if z \u0026gt; 0 else 0 def get_and_gate_training_data(): \u0026#39;\u0026#39;\u0026#39; AND 训练数据集 \u0026#39;\u0026#39;\u0026#39; input_vectors = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]) labels = np.array([1, 0, 0, 0]) return input_vectors, labels if __name__ == \u0026#39;__main__\u0026#39;: \u0026#34;\u0026#34;\u0026#34; 输出如下： 此时感知器权重为[ 0.1 0.2]，偏置项为-0.2 与门 1 and 1 = 1 1 and 0 = 0 0 and 1 = 0 0 and 0 = 0 \u0026#34;\u0026#34;\u0026#34; # 获取样本数据 and_input_vectors, and_labels = get_and_gate_training_data() # 实例化感知器模型 p = Perceptron(f) # 开始学习 AND p_and = p.fit(and_input_vectors, and_labels) # 开始预测 AND print(\u0026#39;1 and 1 = %d\u0026#39; % p_and.predict([1, 1])) print(\u0026#39;1 and 0 = %d\u0026#39; % p_and.predict([1, 0])) print(\u0026#39;0 and 1 = %d\u0026#39; % p_and.predict([0, 1])) print(\u0026#39;0 and 0 = %d\u0026#39; % p_and.predict([0, 0])) S型神经元 #  神经元和感知器本质上是一样的，他们的区别在于激活函数不同，比如跃迁函数改为Sigmoid函数\n神经网络可以通过样本的学习来调整人工神经元的权重和偏置，从而使输出的结果更加准确，那么怎样给⼀个神经⽹络设计这样的算法呢？\n以数字识别为例，假设⽹络错误地把⼀个9的图像分类为8，我们可以让权重和偏置做些⼩的改动，从而达到我们需要的结果9，这就是学习。对于感知器，我们知道，其返还的结果不是0就是1，很可能出现这样一个情况，我们好不容易将一个目标，比如把9的图像分类为8调整回原来正确的分类，可此时的阈值和偏置会造成其他样本的判断失误，这样的调整不是一个好的方案\n所以，我们需要S型神经元，因为S型神经元返回的是[0,1]之间的任何实数，这样的话权重和偏置的微⼩改动只会引起输出的微⼩变化，此时的output可以表示为σ(w⋅x+b)，而σ就是S型函数，S型函数中S指的是Sigmoid函数，定义如下：\n神经网络 #  神经网络其实就是按照一定规则连接起来的多个神经元，一个神经网络由以下组件构成：\n 输入层：接受传递数据，这里应该是 784 个神经元 隐藏层：发掘出特征 各层之间的权重：自动学习出来 每个隐藏层都会有一个精心设计的激活函数，比如Sigmoid、Relu激活函数 输出层，10个输出 上⼀层的输出作为下⼀层的输⼊，信息总是向前传播，从不反向回馈：前馈神经网络 有回路，其中反馈环路是可⾏的：递归神经网络  从输入层传入手写字训练集，然后通过隐藏层向前传递训练集数据，最后输出层会输出10个概率值，总和为1。现在，我们可以看看Keras代码:\n第一步，对数据进行预处理，我们知道，原本数据形状是(60000, 28, 28)，取值区间为[0, 255]，现在改为[0, 1]：\ntrain_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(\u0026#39;float32\u0026#39;) / 255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype(\u0026#39;float32\u0026#39;) / 255 然后对标签进行分类编码：\nfrom keras.utils import to_categorical train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) 第二步，编写模型：\nfrom keras import models from keras import layers network = models.Sequential() network.add(layers.Dense(512, activation=\u0026#39;relu\u0026#39;, input_shape=(28 * 28,))) network.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) network.compile(optimizer=\u0026#39;rmsprop\u0026#39;,loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) network.fit(train_images, train_labels, epochs=5, batch_size=128) 一个隐藏层，激活函数选用relu，输出层使用softmax返回一个由10个概率值（总和为 1）组成的数组\n训练过程中显示了两个数字：一个是网络在训练数据上的损失loss，另一个是网络在 训练数据上的精度acc\n很简单，我们构建和训练一个神经网络，就这么几行代码，之所以写的这么剪短，是因为keras接接口封装地比较好用，但是里面的理论知识我们还是需要好好研究下\n神经网络的数据表示 #  TensorFlow里面的Tensor是张量的意思，上面例子里面存储在多维Numpy数组中的数据就是张量：张量是数据容器，矩阵就是二维张量，张量是矩阵向任意维度的推广，张量的维度称为轴\n标量 #  包含一个数字的张量叫做标量（0D张量），如下：\nx = np.array(12) print(x, x.ndim) # 12, 0 张量轴的个数也叫做阶(rank)\n向量 #  数字组成的数组叫做向量（1D张量），如下：\nx = np.array([12, 3, 6, 14, 7]) print(x, x.ndim) # [12 3 6 14 7] 1 矩阵 #  向量组成的数组叫做矩阵（2D张量），如下：\nx = np.array([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]]) print(x, x.ndim) # [[ 5 78 2 34 0] # [ 6 79 3 35 1] # [ 7 80 4 36 2]] 2 3D张量与更高维张量 #  将多个矩阵组合成一个新的数组就是一个3D张量，如下：\nx = np.array([[[5, 78, 2, 34, 0], [6, 79, 3, 35, 1]], [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1]], [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1]]]) print(x, x.ndim) # (array([[[ 5, 78, 2, 34, 0], # [ 6, 79, 3, 35, 1]], #  # [[ 5, 78, 2, 34, 0], # [ 6, 79, 3, 35, 1]], #  # [[ 5, 78, 2, 34, 0], # [ 6, 79, 3, 35, 1]]]), 3) 将多个3D张量组合成一个数组，可以创建一个4D张量\n关键属性 #  张量是由以下三个关键属性来定义：\n 轴的个数：3D张量三个轴，矩阵两个轴 形状：是一个整数元祖，比如前面矩阵为(3, 5)，向量(5,)，3D张量为(3, 2, 5) 数据类型  在Numpy中操作张量 #  以前面加载的train_images为：\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data() 比如进行切片选择10~100个数字：\ntrain_images[10:100].shape # (90, 28, 28) 数据批量的概念 #  深度学习模型会将数据集随机分割成小批量进行处理，比如：\nbatch = train_images[:128] batch.shape # (128, 28, 28) 现实世界的数据张量 #  下面将介绍下现实世界中数据的形状：\n 向量数据：2D张量，(samples, features) 时间序列数据或者序列数据：3D张量，(samples, timesteps, features) 图像：4D张量，(samples, height, width, channels) 或 (samples, channels, height, width) 视频：5D张量，(samples, frames, height, width, channels) 或 (samples, frames, channels, height, width)  张量运算 #  类似于计算机程序的计算可以转化为二进制计算，深度学习计算可以转化为数值数据张量上的一些张量运算(tensor operation)\n上面模型的隐藏层代码如下：\nkeras.layers.Dense(512, activation=\u0026#39;relu\u0026#39;) 这一层可以理解为一个函数，输入一个2D张量，输出一个2D张量，就如同上面感知机那一节最后输出的计算函数：\noutput = relu(dot(W, input) + b) 逐元素计算 #  Relu 和加法运算都是逐元素的运算，比如：\n# 输入示例 input_x = np.array([[2], [3], [1]]) # 权重 W = np.array([[5, 6, 1], [7, 8, 1]]) # 计算输出 z z = np.dot(W, input_x) # 实现激活函数 def naive_relu(x): assert len(x.shape) == 2 x = x.copy() for i in range(x.shape[0]): for j in range(x.shape[1]): x[i, j] = max(x[i, j], 0) return x # 激活函数对应的输出 output = naive_relu(z) output 广播 #  张量运算那节中，有这样一段代码：\noutput = relu(dot(W, input) + b) dot(W, input)是2D张量，b是向量，两个形状不同的张量相加，会发生什么？\n如果没有歧义的话，较小的张量会被广播，用来匹配较大张量的形状：\ninput_x = np.array([[1], [3]]) # 权重 W = np.array([[5, 6], [7, 8]]) b = np.array([1]) # 计算输出 z z = np.dot(W, input_x) + b # array([[24], # [32]]) 张量点积 #  点积运算，也叫张量积，如：\nimport numpy as np # 输入示例 input_x = np.array([[2], [3], [1]]) # 权重 W = np.array([[5, 6, 1], [7, 8, 1]]) np.dot(W, input_x) 两个向量之间的点积是一个标量：\ndef naive_vector_dot(x, y): assert len(x.shape) == 1 assert len(y.shape) == 1 assert x.shape[0] == y.shape[0] z = 0. for i in range(x.shape[0]): z += x[i] * y[i] return z x = np.array([1,2]) y = np.array([1,2]) naive_vector_dot(x, y) # 5.0 矩阵和向量点积后是一个向量：\nnp.dot(W, [1, 2, 3]) # array([20, 26]) 张量变形 #  前面对数据进行预处理的时候：\ntrain_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(\u0026#39;float32\u0026#39;) / 255 上面的例子将输入数据的shape变成了(60000, 784)，张量变形指的就是改变张量的行和列，得到想要的形状，前后数据集个数不变，经常遇到一个特殊的张量变形是转置(transposition)，如下：\nx = np.zeros((300, 20)) x = np.transpose(x) x.shape # (20, 300) 梯度优化 #  针对每个输入，神经网络都会通过下面的函数对输入数据进行变换：\noutput = relu(dot(W, input_x) + b) 其中：\n relu：激活函数 W：是一个张量，表示权重，第一步可以取较小的随机值进行随机初始化 b：是一个张量，表示偏置  现在我们需要一个算法来让我们找到权重和偏置，从而使得y=y(x)可以拟合样本输入的x\n再回到感知器 #  感知器学习的过程就是其中权重和偏置不断调优更新的过程，其中的偏置可以理解成输入为1的权重值，那么权重是怎么更新的呢？\n首先，介绍一个概念，损失函数，引用李航老师统计学习方法书中的一个解释：\n 监督学习问题是在假设空间中选取模型f作为决策函数，对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致，用一个损失函数（loss function）或代价函数（cost function）来度量预测错误的程度，损失函数是f(X)和Y的非负实值函数，记作L(Y,f(X))\n 其中模型f(X)关于训练数据集的平均损失，我们称之为：经验风险（empirical risk），上述的权重调整，就是在不断地让经验风险最小，求出最好的模型f(X)，我们暂时不考虑正则化，此时我们经验风险的最优化的目标函数就是：\n求解出此目标函数最小时对应的权重值，就是我们感知器里面对应的权重值，在推导之前，我们还得明白两个概念：\n 什么是导数 什么是梯度  什么是导数 #  假设有一个连续的光滑函数f(x) = y，什么是函数连续性？指的是x的微小变化只能导致y的微小变化。\n假设f(x)上的两点a,b足够接近，那么a,b可以近似为一个线性函数，此时他们斜率为k，那么可以说斜率k是f在b点的导数\n总之，导数描述了改变x后f(x)会如何变化，如果你希望减小f(x)的值，只需要将x沿着导数的反方向移动一小步即可，反之亦然\n什么是梯度 #  梯度是张量运算的导数，是导数这一概念向多元函数导数的推广，它指向函数值上升最快的方向，函数值下降最快的方向自然就是梯度的反方向\n随机梯度下降 #  推导过程如下：\n感知器代码里面的这段:\nself.w += input_vector * rate * delta 就对应上面式子里面推导出来的规则\n总结 #  再来看看全部的手写字识别模型代码：\nfrom keras import models from keras import layers from keras.utils import to_categorical (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(\u0026#39;float32\u0026#39;) / 255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype(\u0026#39;float32\u0026#39;) / 255 train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) network = models.Sequential() network.add(layers.Dense(512, activation=\u0026#39;relu\u0026#39;, input_shape=(28 * 28,))) network.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) network.compile(optimizer=\u0026#39;rmsprop\u0026#39;,loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) network.fit(train_images, train_labels, epochs=5, batch_size=128) test_loss, test_acc = network.evaluate(test_images, test_labels) print(\u0026#39;test_acc:\u0026#39;, test_acc)  输入数据保存在float32格式的Numpy张量中，形状分别是(60000, 784)和(10000, 784) 神经网络结构为：1个输入层、一个隐藏层、一个输出层 categorical_crossentropy是针对分类模型的损失函数 每批128个样本，共迭代5次，一共更新(469 * 5) = 2345次  说明 #  对本文有影响的书籍文章如下，感谢他们的付出：\n [统计学习方法] 第一章 Neural Networks and Deep Learning 第一章 Deep Learning with Python 第二章 hands_on_Ml_with_Sklearn_and_TF hanbt零基础入门深度学习系列  "});index.add({'id':1,'href':'/k8s/docs/01_explore/02.%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%AD%97/','title':"02.识别手写字",'section':"第一部分：探索",'content':"识别手写字 #  Neural Networks and Deep Learning 是由 Michael Nielsen 编写的开源书籍，这本书主要讲的是如何掌握神经网络的核心概念，包括现代技术的深度学习，为你将来使⽤神经网络和深度学习打下基础，以下是我的读书笔记。\n神经网络是一门重要的机器学习技术，它通过模拟人脑的神经网络来实现人工智能的目的，所以其也是深度学习的基础，了解它之后自然会受益颇多，本章主要是以识别手写字这个问题来贯穿整篇，那么，人类的视觉系统和神经网络到底在识别一个目标的时候，主要区别在哪？\n 人类视觉系统：通过数十亿年不断地进化与学习，最终能够极好地适应理解视觉世界的任务，从而无意识地就可以对目标进行判断识别 神经网络：通过提供的样本来推断出识别某种目标的规则，作为判断标准  本章的主要内容是介绍神经网络的基本概念以及引入一个识别手写数字的例子来加深我们的理解，你将了解到：\n 两个重要的人工神经元：感知器和S型神经元 神经⽹络的架构 ⼀个简单的分类⼿写数字的⽹络 标准的神经网络学习算法：随机梯度下降算法  感知器 #  1943年，心理学家McCulloch和数学家Pitts发表了《A logical calculus of the ideas immanent in nervous activity》，其中提出了抽象的神经元模型MP，但是在这个模型中权重都是要求提前设置好才可以输出目标值，所以很遗憾，它不可以学习，但这不影响此模型给后来者带来的影响，比如感知器：\n 感知器是Frank Rosenblatt提出的一个由两层神经元组成的人工神经网络，它的出现在当时可是引起了轰动，因为感知器是首个可以学习的神经网络\n 感知器的工作方式如下所示：\n$x_{1},x_{2},x_{3}$ 分别表示三个不同的二进制输入，output则是一个二进制输出，对于多种输入，可能有的输入成立有的不成立，在这么多输入的影响下，该如何判断输出output呢？Rosenblatt引入了权重来表示相应输入的重要性。\n对于$x_{1},x_{2},\u0026hellip;,x_{j}$个输入，每个输入都有对应的权重$w_{1},w_{2},\u0026hellip;,w_{j}$，最后的输出output由每个输入与其对应的权重相乘之和与阈值之差$\\sum _{j} w{_j}x{_j}$来决定，如下：\n假设$b=-threshold$且$w$和$x$对应权重和输⼊的向量，即：\n $x=(x_{1},x_{2},\u0026hellip;,x_{j})$ $w=(w_{1},w_{2},\u0026hellip;,w_{j})$  那么感知器的规则可以重写为:\n这就是感知器的数学模型，是不是就像一个逻辑回归模型？只要将感知器输出规则替换为($f(x)=x$)，后面我们会知道这称之为激活函数，其实这种感知器叫做线性单元。\n它的出现让我们可以设计学习算法，从而实现自动调整人工神经元的权重和偏置，与此同时output也会随之改变，这就是学习！如果你有兴趣可以看我用python写的一个感知器自动学习实现与非门，代码在**nndl_chapter01**。\n说句题外话，由于感知器是单层神经网络，它只能实现简单的线性分类任务，所以它无法对异或问题进行分类，异或的真值表如下：\n   $x$ $y$ $output$     0 0 0   0 1 1   1 0 1   1 1 0    可以看出来，异或问题是线性不可分的，不信你画个坐标轴试试看，那么问题来了？怎么解决，大部分都能很快地想出解决方案，既然感知器可以实现线性分类，也就是说实现与非门是没有问题的，逻辑上来说我们可以实现任何逻辑功能（比如四个与非门实现异或），但前提是为感知器加入一个隐藏层，意思就是多了一个隐藏层的神经网络之后，就可以解决异或问题。\n可是在当时是没办法对多层神经网络（包括异或逻辑）进行训练的，因为计算量太大了，Minsky在1969年出版了一本叫《Perceptron》的书详细说明了这个问题。\n遇到问题，就解决问题，两层神经网络的作用，是可以对非线性进行分类的，我们的先人并未止步，直到反向传播（Backpropagation，BP）算法的出现解决了计算量太大的问题。\n感知器，终于可以多层了。\nS型神经元 #  前面提到，神经网络可以通过样本的学习来调整人工神经元的权重和偏置，从而使输出的结果更加准确，那么怎样给⼀个神经⽹络设计这样的算法呢？\n以数字识别为例，假设⽹络错误地把⼀个9的图像分类为8，我们可以让权重和偏置做些⼩的改动，从而达到我们需要的结果9，这就是学习。对于感知器，我们知道，其返还的结果不是0就是1，很可能出现这样一个情况，我们好不容易将一个目标，比如把9的图像分类为8调整回原来正确的分类，可此时的阈值和偏置会造成其他样本的判断失误，这样的调整不是一个好的方案。\n所以，我们需要S型神经元，因为S型神经元返回的是[0,1]之间的任何实数，这样的话权重和偏置的微⼩改动只会引起输出的微⼩变化，此时的output可以表示为$\\sigma(w \\cdot x+b)$，而$\\sigma$就是S型函数，S型函数中S指的是Sigmoid函数，定义如下：\n$$ \\sigma(z) \\equiv \\frac{1}{1+e^{-z}} $$\n其中$z$表达式为：\n$$z=\\sum_{j} w{_j}x{_j}+b$$\n那么⼀个具有输⼊$x_{1},x_{2},\u0026hellip;,x_{j}$，权重为$w_{1},w_{2},\u0026hellip;,w_{j}$，和偏置b的S型神经元的输出是：\n$$ \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)} $$\n上面说过，感知器的激活函数只会输出两个值，分别是0或1，让我们把目光投向S型神经元，假设此时$z$是一个很大的正数，那么此时output将无限接近于1，反之，若$z$是一个很大的负数，那么此时output将无限接近于0，从极端情况来看，是不是很像S型神经元呢？\nS型神经元的输出是[0,1]之间的任何实数，那么这个分类模型该如何判断分类结果呢？其实很简单，我们可以定义一个数值，比如：\n $\\sigma(z) \\leq 0.5$ 输出0 $\\sigma(z)\u0026gt;0.5$ 输出1  神经⽹络的架构 #  看下面一个四层神经网络，这种类型的多层⽹络有时被称为多层感知器或者MLP，如下图：\n需要记住以下几个概念：\n ⽹络中最左边的称为输⼊层，其中的神经元称为输⼊神经元 中间得称之为隐藏层，图中有两个隐藏层 最右边称之为输出层 上⼀层的输出作为下⼀层的输⼊，信息总是向前传播，从不反向回馈：前馈神经网络 有回路，其中反馈环路是可⾏的：递归神经网络  ⼀个简单的分类⼿写数字的⽹络 #  对于这六个数字504192，我们该如何实现一个分类⼿写数字的⽹络呢，可以将其分解为两个小问题：\n  504192是连续在一起的图像，首先可以将其分割成6个小图像，比如5 0 \u0026hellip;等\n  再对分割开的小图像进行分类，比如识别5\n  我们将使⽤⼀个三层神经⽹络来识别单个数字： 由于训练数据由$28*28$的⼿写数字的图像组成，所以:\n 输入层有784个神经元，因为$784 = 28 \\times 28$，输⼊像素是灰度级的，值为0:0表⽰⽩⾊，值为1:0表⽰⿊⾊，中间数值表⽰逐渐暗淡的灰⾊ 隐藏层⽤n来表⽰神经元的数量，我们将给n实验不同的数值。⽰例中⽤⼀个⼩的隐藏层来说明，仅仅包含$n = 15$个神经元 ⽹络的输出层包含有10个神经元，如果第一个输出神经元被激活，那么数字就是0，以此类推，从0到9  一个分类⼿写数字的⽹络，大概就可以这样实现。\n随机梯度下降算法 #  分类方案已经确定了，接下来第一步就是获取样本，我们将使⽤MNIST数据集，其包含有数以万计的连带着正确分类器的⼿写数字的扫描图像，下载好建立这样的目录结构，使用jupyter notebook开始吧，如果没有，请自行检索教程安装，代码目录如下：\npylab └── datasets └── MNIST_data ├── t10k-images-idx3-ubyte.gz ├── t10k-labels-idx1-ubyte.gz ├── train-images-idx3-ubyte.gz └── train-labels-idx1-ubyte.gz 然后使用 TensorFlow来读取数据，看看下载的数据集到底是什么样子，可以在mnist_data.ipynb中添加如下代码：\nfrom matplotlib import pyplot as plt from tensorflow.examples.tutorials.mnist.input_data import read_data_sets mnist = read_data_sets(\u0026#34;../datasets/MNIST_data/\u0026#34;, one_hot=True) 查看一些基本信息：\nprint(\u0026#34;Training data size: %s\u0026#34; % mnist.train.num_examples) print(\u0026#34;Validating data size: %s\u0026#34; % mnist.validation.num_examples) print(\u0026#34;Testing data size: %s\u0026#34; % mnist.test.num_examples) # 每张图片是长度为784的一维数组 print(len(mnist.train.images[0])) def display_digit(image): image = image.reshape([28,28]) plt.imshow(image, cmap=plt.get_cmap(\u0026#39;gray_r\u0026#39;)) plt.show() print(\u0026#34;样本真实数字为：%s\u0026#34; % list(mnist.train.labels[1]).index(1)) # 显示图像 display_digit(mnist.train.images[1]) # Output # Training data size: 55000 # Validating data size: 5000 # Testing data size: 10000 # 784 # 样本真实数字为：3 可以看到显示如下图像： 通过代码，我们知道，每个图像被分割成784个值存于一维数组中，看作⼀个$28 \\times 28 = 784$维的向量，每个向量就代表图像中单个像素的灰度值，假设我们用$y$表示output，也就是当我们输入一组样本数据x，那么y就是我们需要的结果，怎么从x得到y呢，我们需要一个函数$y= y(x)$。\n感知器为什么可以学习，因为它可以调整$b$和$w$来实现output的改变，这里，我们同样需要一个算法来让我们找到权重和偏置，从而使得$y= y(x)$可以拟合样本输入的x，为了量化我们如何实现这个⽬标，我们定义⼀个代价函数（有时被称为损失或⽬标函数）：\n$$ C(w,b) \\equiv \\frac{1}{2n} \\sum_x | y(x) - a|^2 $$\n w 表⽰所有的⽹络中权重的集合 b 是所有的偏置 n 是训练输⼊数据的个数 a 是表⽰当输⼊为x时输出的向量，可以理解为output  训练模型的过程就是优化代价函数的过程，Cost Function(代价函数) 越小，就代表模型拟合的越好，所以我们的目的是找出最⼩化权重和偏置的代价函数$C(w,b)$，其实这个就是我们平常用来评价回归算法的均方误差，也就是MSE。\n现在我们的目标很清晰，为了找出合适的权重和偏置值，我们需要让代价函数的值接近于0，在这个条件下我们就可以找出合适的权重和偏置值，我们将采⽤称为梯度下降的算法来达到这个⽬的。\n下面是我的推导过程：\n实现数字分类模型 #  终于，掌握了基本的概念，我们可以通过实战来实现一个数字分类模型，来看看，前面我们掌握的知识是怎么运用于实际的。\n书籍作者很贴心地实现了模型代码，仓库见neural-networks-and-deep-learning 官方git仓库，接下来我们一起来看看具体的实现代码吧。\n首先是实现一个Network类，\n#!/usr/bin/env python import random import numpy as np class Network(object): def __init__(self, sizes): \u0026#34;\u0026#34;\u0026#34; 初始化 :param sizes: 如果想要初始化一个层数为3，里面神经元数量分别为：2,3,1 的神经网络，那么 sizes = [2，3，1] \u0026#34;\u0026#34;\u0026#34; self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] 前面已经说了S型神经元，那么此时激活函数实现如下，更加具体的代码请参考network.py：\ndef sigmoid(z): \u0026#34;\u0026#34;\u0026#34;The sigmoid function.\u0026#34;\u0026#34;\u0026#34; return 1.0/(1.0+np.exp(-z)) 代码已经准备就绪，下面可以开始进行模型训练：\nimport mnist_loader, network training_data, validation_data, test_data = mnist_loader.load_data_wrapper() net = network.Network([784, 30, 10]) net.SGD(list(training_data), 30, 10, 3.0, test_data=list(test_data)) # Output Epoch 0: 9023 / 10000 Epoch 1: 9222 / 10000 Epoch 2: 9312 / 10000 Epoch 3: 9312 / 10000 Epoch 4: 9383 / 10000 Epoch 5: 9421 / 10000 Epoch 6: 9432 / 10000 Epoch 7: 9421 / 10000 Epoch 8: 9454 / 10000 Epoch 9: 9447 / 10000 看结果，十次迭代后，轻轻松松突破94%\n参考 #   文中涉及代码 Neural Networks and Deep Learning Neural Networks and Deep Learning 中文版 神经网络浅讲：从神经元到深度学习 neural-networks-and-deep-learning 官方git仓库  搞定收工，有兴趣欢迎关注我的公众号：\n"});index.add({'id':2,'href':'/k8s/docs/01_explore/03.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/','title':"03.梯度下降数学推导",'section':"第一部分：探索",'content':"梯度下降数学推导 #  以感知器为例，可以梯度下降来学习合适的权重和偏置：\n假设有n个样本，第i次的实际输出为y，对于样本的预测输出可以表示为：\n$$ \\bar{y}^i = w_1x_1^i+w_2x_2^i+\u0026hellip;+w_nx_n^i+b $$\n任意一个样本的实际输出和预测输出单个样本的误差，可以使用MES表示：\n$$ e^i=\\frac{1}{2}(y^i-\\bar{y}^i)^{2} $$\n那么所有误差的和可以表示为：\n$$ \\begin{aligned} E \u0026amp;= e^1+e^2+\u0026hellip;+e^n \\ \u0026amp;= \\sum_{i=1}^ne^i \\ \u0026amp;= \\frac{1}{2}\\sum_{i=1}^n(y^i-w^Tx^i)^2 \\end{aligned} $$\n想象一下，当你从山顶往下走，只要你沿着最陡峭的位置往下走，那么终将走到最底部（也可能是局部最低）：\n我们学习的目的就是在$E$尽量最小，然后得到此时的$w$和$b$，前面说的最陡峭的位置该怎么定义呢？我们可以引入梯度，这是一个向量，指的是函数值上升最快的方向，那么最陡峭的位置就可以用在最陡峭的方向迈出一步（步长，学习速率），用数学公式表示为：\n$$ \\mathbf{x}{n e w}=\\mathbf{x}{\\text {old }}-\\eta \\nabla f(x) $$\n其中：\n $\\nabla$表示梯度算子 $\\nabla f(x)$表示函数的梯度 $\\eta$表示梯度、学习速率，可以理解为找准下山的方向后要迈多大步子  现在有了目标函数，也知道怎么找到让目标函数值最小的办法，对于参数$w$：\n$$ E_{(w)} = \\frac{1}{2}\\sum_{i=1}^n(y^i-w^Tx^i)^2 $$\n那么$W$值的更新公式为：\n$$ w_{n e w}=w_{\\text {old }}-\\eta \\nabla E_{(w)} $$\n关键步骤来了，来看看$E_{(w)}$的推导吧：\n$$ \\begin{aligned} \\nabla E(\\mathrm{w}) \u0026amp;=\\frac{\\partial}{\\partial \\mathrm{w}} E(\\mathrm{w}) \\\n\u0026amp;=\\frac{\\partial}{\\partial \\mathrm{w}} \\frac{1}{2} \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2} \\\u0026amp;=\\frac{1}{2}\\frac{\\partial}{\\partial \\mathrm{w}} \\sum_{i=1}^{n} \\left(y^{(i)2}-2y^{(i)}\\bar{y}^{(i)}+\\bar{y}^{(i)2}\\right) \\end{aligned} $$\n再引入链式求导法则：\n$$ \\begin{aligned} \\nabla E(\\mathrm{w}) \u0026amp;=\\frac{\\partial}{\\partial \\mathrm{w}} E(\\mathrm{w}) \\\n\u0026amp;=\\frac{1}{2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\mathrm{w}}\\left(y^{(i) 2}-2 y^{(i)} \\bar{y}^{(i)}+\\bar{y}^{(i) 2}\\right) \\\n\u0026amp;=\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\frac{\\partial}{\\partial \\bar{y}^{(i)}}\\left(y^{(i) 2}-2 y^{(i)} \\bar{y}^{(i)}+\\bar{y}^{(i) 2}\\right) \\frac{\\partial y_{(i)}}{\\partial \\mathrm{w}}\\right) \\\n\u0026amp;=\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\left(-2 y^{(i)}+2 \\bar{y}^{(i)}\\right) \\mathbf{x}^{(i)}\\right) \\\n\u0026amp;=-\\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right) \\mathrm{x}^{(i)} \\end{aligned} $$\n前面提到$W$值的更新公式为：\n$$ w_{n e w}=w_{\\text {old }}-\\eta \\nabla E_{(w)} $$\n将上面计算结果带入：\n$$ w_{n e w}=w_{\\text {old }}+\\eta \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right) \\mathrm{x}^{(i)} $$\n参数的更新方式就这样计算出来了，其实所谓的学习，就是确定一个目标函数用一定的计算方法让其算出最优的参数。\n"});index.add({'id':3,'href':'/k8s/docs/02_appendix/01.%E8%AF%91%E5%A6%82%E4%BD%95%E7%94%A8Python%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/','title':"01.[译]如何用 Python创建一个简单的神经网络",'section':"第二部分：附录",'content':"如何用Python创建一个简单的神经网络 #   原文地址：How to Create a Simple Neural Network in Python 作者：Dr. Michael J. Garbade 翻译：howie6879   理解神经网络如何工作的最好方式是自己动手创建一个，这篇文章将会给你演示怎么做到这一点\n 神经网络(NN)，也称之为人工神经网络(ANN)，它是机器学习领域中学习算法集合中的子集，其核心概念略似生物神经网络的概念。\n拥有五年以上经验的德国机器学习专家Andrey Bulezyuk说过：神经网络正在使机器学习发生革命性的巨变，因为他们能够跨越广泛的学科和行业来高效地建模复杂的抽象。\n基本上，一个ANN由以下组件构成：\n 输入层：接受传递数据 隐藏层 输出层 各层之间的权重 每个隐藏层都会有一个精心设计的激活函数，对于此教程，我们将会使用Sigmoid激活函数  神经网络的类型有很多，在这个项目中，我们准备创建一个前馈神经网络，此类型的ANN将会直接从前到后传递数据\n训练前馈神经元往往需要反向传播，反向传播为神经网络提供了相应的输入和输出集合，输入数据在被传递到神经元的时候被处理，然后产生一个输出\n下面展示了一个简单的神经网络结构图：\n而且，理解神经网络如何工作做好的办法就是去学习从头开始构建一个神经网络(不使用任何第三方库，作者意思应该是不使用任何机器学习库)。\n在本文中，我们将演示如何使用Python编程语言创建一个简单的神经网络。\n问题 #  这里用表格列出了我们需要解决的问题：\n我们将会训练一个特定的神经网络模型，当提供一组新数据的时候，使其能够准确地预测输出值。\n如你在表中所见，输出值总是等于输入数据的第一个值，因此我们期望的表中输出(?)值是1。\n让我们思考看看能不能使用一些Python代码来给出相同的结果(再继续阅读之前，你可以在文章末尾仔细地阅读此项目的代码)\n创建一个神经网络类 #  我们将在Python中创建一个NeuralNetwork类来训练神经元以提供准确的预测，该类还具有一些其他的辅助函数\n尽管我们没有使用任何一个神经网络库用于这个简单的神经网络示例，我们也会导入numpy包来协助计算。\n该库带有以下四个重要方法：\n exp：用于生成自然指数 array：用于生成矩阵 dot：用于乘法矩阵 random：用于生成随机数(注意：我们将对随机数进行播种以确保其有效分布)  应用 Sigmoid 激活函数 #  该神经网络将使用Sigmoid function作为激活函数，其绘制了一个典型的S形曲线：\n此函数可以将任意值映射到区间0~1之间，它将帮助我们规范化输入值的和权重乘积之和。\n随后，我们将创建Sigmoid函数的导数来帮助计算机对权重进行必要的调整。\n一个Sigmoid函数的输出可以用来生成它的导数，例如，如果输出变量是X，那么它的导数将是x * (1-x)。\n推导过程如下：\n训练模型 #  在这个阶段我们将教导神经网络进行准确预测，每个输入都有一个权重 - 正面或负面。\n这意味着，如果输入包含一个大的正面或者负面的权重数值将会更多地影响输出值\n请记住，在最开始阶段我们会对每个权重分配一个随机值\n下面是我们在这个神经网络示例问题中使用的训练过程:\n 从训练集获取输入数据，根据他们的权重进行调整，然后通过计算人工神经网络输出的方法将它们抽取出来 我们计算了反向传播的错误率，在这种情况下，它是神经元预测值和实际期望值之间的差异 根据误差程度，我们利用Error Weighted Derivative formula对权重进行微调 我们对这个过程重复15,000次，在每次迭代中都会同时处理整个训练集  我们使用.T函数将矩阵从水平位置转换到垂直位置，因此数据会被这样排序：\n最终，神经元的权重将会被提供的训练集优化，因此，如果神经元被要求去思考一个新的情况，而这个情况和前面的情况是一样的，那么神经元就可以做出准确的预测，这就是反向传播的发生方式。\n总结 #  最终我们初始化了NeuralNetwork类并且运行代码\n下面就是整体的项目代码，如何在Python项目中创建神经网络：\nimport numpy as np class NeuralNetwork(): def __init__(self): # seeding for random number generation np.random.seed(1) # converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0 self.synaptic_weights = 2 * np.random.random((3, 1)) - 1 def sigmoid(self, x): #applying the sigmoid function return 1 / (1 + np.exp(-x)) def sigmoid_derivative(self, x): #computing derivative to the Sigmoid function return x * (1 - x) def train(self, training_inputs, training_outputs, training_iterations): #training the model to make accurate predictions while adjusting weights continually for iteration in range(training_iterations): #siphon the training data via the neuron output = self.think(training_inputs) #computing error rate for back-propagation error = training_outputs - output #performing weight adjustments adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output)) self.synaptic_weights += adjustments def think(self, inputs): #passing the inputs via the neuron to get output  #converting values to floats inputs = inputs.astype(float) output = self.sigmoid(np.dot(inputs, self.synaptic_weights)) return output if __name__ == \u0026#34;__main__\u0026#34;: #initializing the neuron class neural_network = NeuralNetwork() print(\u0026#34;Beginning Randomly Generated Weights: \u0026#34;) print(neural_network.synaptic_weights) #training data consisting of 4 examples--3 input values and 1 output training_inputs = np.array([[0,0,1], [1,1,1], [1,0,1], [0,1,1]]) training_outputs = np.array([[0,1,1,0]]).T #training taking place neural_network.train(training_inputs, training_outputs, 15000) print(\u0026#34;Ending Weights After Training: \u0026#34;) print(neural_network.synaptic_weights) user_input_one = str(input(\u0026#34;User Input One: \u0026#34;)) user_input_two = str(input(\u0026#34;User Input Two: \u0026#34;)) user_input_three = str(input(\u0026#34;User Input Three: \u0026#34;)) print(\u0026#34;Considering New Situation: \u0026#34;, user_input_one, user_input_two, user_input_three) print(\u0026#34;New Output data: \u0026#34;) print(neural_network.think(np.array([user_input_one, user_input_two, user_input_three]))) print(\u0026#34;Wow, we did it!\u0026#34;) 我们设法创建了一个简单的神经网络。\n这个神经网络开始于自己给自己分配了一些随机的权重，此后，它使用训练数据训练自己。\n因此，如果出现新情况[1,0,0]，则其值为0.9999584。\n你记得我们想要的正确答案是1吗？\n那么，这就非常接近了 —— 思考下S形函数的输出值在0到1之间。\n当然，我们只使用一个神经网络来执行这个简单的任务，如果我们连接数千个人工神经网络起来会怎样？我们能否100% 地模仿人类大脑的工作方式么？\n如果你有任何疑问，请留言。\n"});})();